from airas.types.research_hypothesis import ExperimentalDesign, ResearchHypothesis

retrieve_hugging_face_subgraph_input_data = {
    "new_method": ResearchHypothesis(
        method="Open Problems: 1) Existing CL methods treat â€œdata memoryâ€ and â€œparameter growthâ€ as two separate issues; none couples them with compute- or energy-budget constraints. 2) Even the most compact replay buffers still keep either raw pixels or floating-point latents; none can store task evidence in *sub-byte* form while remaining differentiable. 3) Fixed encoders eliminate drift but collapse when the input domain drifts (e.g.\nweather, camera, modality). 4) Current PEFT branches are memory-light yet SRAM-hungry at training time because optimizers must maintain full-precision momenta. 5) No prior work reports *on-device* energy or privacy leakage under the same memory budget.\nMethods: We propose UniMem, a hardware-aware, privacy-preserving continual-learning system that couples ultra-compressed episodic storage with zero-growth adaptation.\nA. Sub-byte Hierarchical Latent Replay (SHLR)\n  â€¢ A shallow, frozen vision-speech encoder maps input xâ†’zâˆˆR^128.  Two-stage product quantization (PQ-VQ) converts z to 2 four-bit indices (â‰ˆ1 byte) which are arithmetic-coded offline (â‡¢0.6 byte/img).  \n  â€¢ A TinyBit-VAE decoder (350 k params) is trained once; at replay time it reconstructs differentiable latents via straight-through Gumbel tricks.  \n  â€¢ To keep the 1-byte code informative under domain shift, we introduce â€œcodebook reheatingâ€: whenever validation loss spikes, only the embedding vectors referenced in the last K batches are fine-tuned for one epoch, then re-quantisedâ€”no new bytes are stored.\nB. Budget-neutral Meta-Adapters\n  â€¢ Instead of LoRA, we learn rank-4 Butterfly-structured adapters whose weights are ternary (-1,0,1).  Gradients are accumulated in 8-bit Kahan buffers and flushed every 8 steps, eliminating 32-bit optimiser state.\n  â€¢ After a task, adapters are merged into the backbone using a closed-form Hadamard update; the ternary diff is XOR-compressed and archived (â‰ˆ6 kB per task). During inference the active model is *identical* in size to the original.\nC. Unified neuro-compression scheduler\n  â€¢ Given a target RAM+Flash+Energy budget, an ILP chooses (buffer bytes, decoder width, adapter rank) that maximises expected accuracy under analytic FLOP/energy models calibrated on an ARM Cortex-M55.\nD. Privacy guard\n  â€¢ Each stored code is encrypted with task-specific one-time pads; guessing any raw pixel from a 0.6-byte code has <2â»Â³â° success probability.  We publish a formal DP bound.\n\nExperimental Setup: Datasets & modalities: Split CIFAR-100 (32Ã—32), Tiny-ImageNet-200 (64Ã—64), ImageNet-R (224Ã—224, 10 tasks), SpeechCommands-35 (1-s audio), and Ego4D 32Ã—32 action clips.\nBaselines: ER (5 k), ER-SVHN-PQ, SparCL-DER++, InfLoRA, AQM, and fine-tune.\nBudgets: (RAM,Flash) = {(0.5 MB,2 MB), (2 MB,8 MB)}.  Measure: AvgAccuracy, AvgForgetting, Energy/train-step (Keysight DAQ), DP-Îµ, and code-reconstruction PSNR.\nAblations: (i) raw-pixel buffer of equal bytes, (ii) no codebook reheating, (iii) float adapters, (iv) optimiser state pruning off.\nEdge deployment: run on STM32H7 (Cortex-M7, 600 MHz) with CMSIS-NN kernels; report wall-clock and peak power.\nExpected Result: â€¢ At 0.5 MB RAM / 2 MB Flash UniMem matches 40 MB ER on CIFAR-100 (+1 pp) and beats InfLoRA by 3 pp on ImageNet-R while consuming 4Ã— less energy.\nâ€¢ Average code size 0.62 byte per sample, 50Ã— smaller than AQM and 300Ã— smaller than raw replay; privacy breach prob <2â»Â²â¹.\nâ€¢ After 20 tasks total model parameters grow <1.5 %, archived XOR diffs â‰ˆ120 kB.\nâ€¢ On-device training reaches 6.8 images/s at 120 mW; competing methods either exceed RAM or run 4Ã— slower.\nâ€¢ Without codebook reheating accuracy drops by 7 pp on cross-domain tasks, demonstrating necessity of adaptive quantiser.\nExpected Conclusion: UniMem shows that sub-byte latent replay combined with ternary, merge-and-archive adapters can satisfy stringent memory *and* energy budgets while preserving privacy in continual learning.  By jointly optimising data, parameters and optimiser state under a single budget, UniMem advances the feasibility of lifelong learning on micro-controllers, wearables and privacy-critical IoT cameras.  Future work: extend SHLR to text tokens and integrate diffusion decoders for higher-fidelity reconstruction without extra bytes.",
        experimental_design=ExperimentalDesign(
            experiment_strategy="--------------------------------------------------------------------\nEXPERIMENT 1  â€“  Large-Scale Dynamic Vocabulary & Phase Coupling Study\n--------------------------------------------------------------------\nGoal\nâ€¢ Quantify how well the Dynamic Vocabulary Phasor (M-0) and Phaseâ€“Semantic Coupling Bound (M-3) preserve accuracy, latency and energy under rapid, user-driven vocabulary change.\n\nSetup\nâ€¢ 10-day chat-like corpus (Reddit + WhatsApp) replayed at 10Ã— real-time.\nâ€¢ Every 2 000 messages, inject 5â€“50 unseen tokens (custom emojis, slang, typos).\nâ€¢ Two model variants served from one A100 (fp16):\n  1. Baseline = ASTRA + vanilla LPD retrained nightly.\n  2. ORION = ASTRA + DVP + PSCB ( Îº auto-tuned).\nâ€¢ Simulator streams the new tokens to 5 000 virtual devices; A100#2 logs BLEU/ROUGE, latency, DRAM energy (NVIDIA Nsight + DIMM power rails).\n\nProcedure\n1. Pre-load both models with same weights.\n2. For each vocabulary update:\n   a) Baseline: trigger full LPD rebuild on A100#1.\n   b) ORION: run 32-anchor re-indexer, then resume serving with PSCB-controlled phase budget.\n3. Continue conversation replay and logging.\n4. Repeat until 1 000 new tokens added.\n\nMetrics\nâ€¢ Quality: Î”BLEU against ground-truth replies.\nâ€¢ Time-to-serve: pause period while model adapts.\nâ€¢ Energy: Î¼J per 100 tokens (GPU) + DRAM energy.\nâ€¢ Safety: number of PSCB violations.\n\nExpected Demonstration\nâ€¢ ORION loses â‰¤0.1 BLEU after each injection whereas Baseline drops 0.8â€“1.4 until nightly retrain.\nâ€¢ Mean adaptation latency: 3.9 ms (ORION) vs 2.4 h (Baseline).\nâ€¢ 22 % lower cumulative DRAM energy thanks to PSCBâ€™s phase-update suppression.\nâ‡’ Shows that DVP + PSCB let the model survive continual vocabulary drift with minimal cost.\n\n--------------------------------------------------------------------\nEXPERIMENT 2  â€“  Mesh-Level Collaborative Caching & Privacy Audit\n--------------------------------------------------------------------\nGoal\nâ€¢ Validate that Federated Spectrum Bloom (M-1) lifts hit-rate and lowers latency without leaking sensitive content.\n\nSetup\nâ€¢ 300 physical devices emulated on 4Ã—A100 using Triton-based multi-tenant server; each emulated endpoint reproduces radio, battery and compute limits of phone / watch / earbud.\nâ€¢ Devices divided into 30 clusters (home, office, commute).  Each cluster connected by BLE mesh (software-defined).\nâ€¢ Data: Emoji-Chat benchmark (with 50 k user tokens) + private names/emails inserted as canaries.\nâ€¢ Privacy auditor tries to reconstruct canaries from the Bloom sketches using 128 V100 GPUs (harder adversary than stated).\n\nProcedure\n1. Warm-start none of the devices (cold cache).\n2. Run 20 000 chat sessions with M-1 enabled; log inter-device hit-rate (IH) and latency.\n3. Repeat with M-1 disabled (private caches only).\n4. Feed all published sketches to the auditor; attempt brute-force & ML inversion.\n\nMetrics\nâ€¢ Hit-rate (%) and median first-token latency (ms).\nâ€¢ BLE mesh traffic (kB) and energy per exchange.\nâ€¢ Privacy: precision / recall of recovered canaries; differential-privacy ÎµÌ‚ estimated from audit.\n\nExpected Demonstration\nâ€¢ IH rises from â‰ˆ12 % â†’ 65â€“70 % and median latency drops ~28 % with M-1.\nâ€¢ Network overhead stays <4 kB per device per hour; battery impact <1 %. \nâ€¢ Auditor recovers <0.02 % of canaries (close to random); empirical ÎµÌ‚ â‰¤0.35, matching design target Îµ=0.3.\nâ‡’ Confirms that FSB provides substantial performance benefits while keeping privacy risk negligible.\n\n--------------------------------------------------------------------\nEXPERIMENT 3  â€“  End-to-End Leakage & Compliance Stress-Test\n--------------------------------------------------------------------\nGoal\nâ€¢ Assess Latent Redaction Transformer (M-2), Bus-Aware Sparsity Scheduler (M-4) and Fair-Use Contrastive Filter (M-5) under adversarial conditions.\n\nSetup\nâ€¢ Malware-Sniffer benchmark: host model executes inside Android 14 image on QEMU; a separate process attaches to every transformer layer, capturing hidden states over AHB bus taps.\nâ€¢ Corpus: 3 k prompts crafted to elicit personal data and copyrighted text (Comic-Books set).\nâ€¢ Four configurations, all run on a single A100 to guarantee same compute budget:\n  A. ASTRA only (no defences).\n  B. +BASS.\n  C. +LRT.\n  D. Full ORION (BASS+LRT+FCF).\nâ€¢ Power rails instrumented to separate DRAM from NPU cores.\n\nProcedure\n1. Execute the 3 k prompts once per configuration.\n2. Malware-Sniffer hashes each captured tensor; collisions with forbidden embeddings counted as latent leaks.\n3. Compare generated surface text against copyrighted database to compute FUVR.\n\nMetrics\nâ€¢ LLR: latent-leak rate (% of steps).\nâ€¢ FUVR: fair-use violation rate (% of outputs).\nâ€¢ E_DRAM: energy per 100 tokens (mJ).\nâ€¢ End-to-end latency (ms).\n\nExpected Demonstration\nâ€¢ Config D cuts LLR from â‰¥4 % (A) to â‰¤0.3 %, FUVR from 7 % to â‰¤0.3 %.\nâ€¢ DRAM energy drops 30 % in B and is preserved in D despite extra heads (thanks to BASS).\nâ€¢ Added latency stays <5 ms per sequence.\nâ‡’ Shows that ORION simultaneously improves privacy, legal compliance and efficiency under real leakage threats.\n\n====================================================================\nCombined Take-Away\nAcross the three experiments we validate:\n1. Robustness to dynamic vocabularies with tight phaseâ€“semantic control.\n2. Efficient, privacy-preserving collaborative intelligence across swarms of devices.\n3. Concrete mitigation of latent leaks and fair-use violations without sacrificing energy or latency.\nTogether these outcomes would decisively demonstrate the practical effectiveness of the ORION framework on modern edge hardware.",
            experiment_details="â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nEXPERIMENT 1  Large-Scale Dynamic-Vocabulary & Phase-Coupling Study\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n1. Purpose\n   â€¢ Verify that M-0 (Dynamic Vocabulary Phasor) + M-3 (PSCB) preserve\n     generation quality, latency and energy when the active vocabulary\n     is mutated on-line.\n\n2. Models\n   a) Baseline-LPDâ€ƒASTRA-Base-1.3B + vanilla Low-rank Phasor Dictionary\n   b) ORION-DVP    ASTRA-Base-1.3B + DVP + PSCB\n   (weights identical at t=0; fp16, ZeRO-2 sharded on 8Ã—A100)\n\n3. Datasets\n   â€¢ Reddit-WhatsApp-10Day: 19.4 M message / reply pairs collected 2020-22\n     (public + user-donated), English.  Licence: CC-BY-SA.\n   â€¢ Custom-Token Injection List: 1 000 unseen tokens (ğŸ˜»-style emojis,\n     slang, misspellings) with ground-truth surface forms.\n   All data is stored in line-delimited JSONL (msg, reply).\n\n4. Pre-processing\n   â€¢ Initial tokeniser = SentencePiece-32k trained on full 10-day text.\n   â€¢ Text â†’ NFC normalisation, URLs stripped, lower-cased except Emojis.\n   â€¢ Energy labels added via Nsight CSV sync.\n   â€¢ Injection script adds N_newâˆˆ{5,10,20,50} tokens every 2 000 msgs,\n     updates ground-truth replies accordingly.\n\n5. Data splitting\n   â€¢ Chronological: Day 1-7 = train (fine-tuning only for Baseline),\n     Day 8      = validation (Îº search, learning-rate decay),\n     Day 9-10   = test stream driving the replay.\n   â€¢ No cross-validation (ordering matters).  Each full run repeated on\n     3 random seeds (tokeniser dropout) â‡’ report meanÂ±95 % CI.\n   â€¢ Model selection: best-val BLEU on Day 8 snapshot.\n\n6. Adaptation procedure\n   Baseline-LPD: every 2 000 msgs triggers full LPD retrain\n     â€“ lr = 3e-4, AdamW(Î²=0.9/0.95), 2 k warm-up, 1 epoch â‰ˆ2 h.\n   ORION-DVP: 32-anchor re-indexer (Algorithm 2) runs for â‰¤4 ms on GPU;\n     PSCB Îº re-optimised by grid-search Îºâˆˆ{0.05,0.1,â€¦0.5} on 128-sample\n     window; cheapest Îº satisfying Î”BLEU<0.1 chosen.\n\n7. Metrics\n   Primary   BLEU-4 and ROUGE-L (reply v. ground truth)\n   Secondary Î”BLEU after each injection, Adaptation-Latency (ms),\n             Î¼J /100 tokens (GPU+DRAM), #PSCB violations\n   All energy measured by:\n     E_GPU = âˆ‘(P_board â€“ Idle)Â·Î”t via NVML at 10 Hz\n     E_DRAM = on-board DIMM current sensor sampled @200 kHz.\n\n8. Hyper-parameter analysis\n   â€¢ Anchor Count mâˆˆ{16,32,64} evaluated offline on validation day.\n   â€¢ PSCB Îº and re-indexer rank râˆˆ{8,16,32} swept; ANOVA used to\n     determine interaction significance (p<0.05).\n\n9. Robustness checks\n   â€¢ OOD insertion: tokens whose UTF-8 bytes overlap existing tokens,\n     and right-to-left scripts (Arabic slang).  Quality + energy logged.\n   â€¢ Noise: randomly flip 5 % of phase entries in DVP to test PSCB.\n\n10. Compute & memory logging\n    â€¢ FLOPs counted with torch.profiler.profile(â€¦., profile_memory=True).\n    â€¢ Wall-clock adaptation time measured by CUDA events.\n    â€¢ Peak VRAM recorded by NVML; host RAM by psutil.\n\n11. Example code snippet\n```\nwith torch.cuda.amp.autocast():\n    out = model(input_ids)\nif vocab_update:\n    R = learn_reindexer(anchor_pairs, rank=32)\n    model.phase_proj.weight = R @ model.phase_proj.weight  # DVP\n    kappa = search_kappa(window_batch, target_bleu=0.1)\n    model.pscb_kappa = kappa\n```\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nEXPERIMENT 2  Mesh-Level Collaborative Caching & Privacy Audit\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n1. Purpose\n   Demonstrate that M-1 (Federated Spectrum Bloom) improves cache\n   hit-rate & latency while respecting differential-privacy guarantees.\n\n2. Models (all inference-only)\n   â€¢ ORION-FSB   ASTRA-Base-1.3B + DVP + PSCB + FSB\n   â€¢ ORION-NoFSB same stack but with per-device private cache\n   (weights identical; tuned Îº from Exp-1 reused)\n\n3. Datasets\n   â€¢ Emoji-Chat v1.0 (50 k user-added emoji tokens, 2.1 M dialogues)\n   â€¢ Canary-Set: 10 k unique e-mails & phone numbers injected randomly\n     to measure leakage.\n\n4. Pre-processing\n   â€¢ Same SentencePiece 32k; when canaries detected they are tagged\n     <PII> for auditor scoring.\n   â€¢ Cache warm-up disabled at t=0.\n\n5. Simulation fabric\n   â€¢ Triton Inference Server replica per A100; 300 endpoints scripted\n     with open-source EdgeSim.  Device classes:\n       Phone (2Ã—A55 + A78), Watch (Cortex-M55), Earbud (Cortex-M33)\n   â€¢ BLE Mesh latency model: log-normal Î¼=12 ms, Ïƒ=3 ms.\n\n6. Data splitting & runs\n   â€¢ 20 k chat sessions randomly drawn; same seed for both configs.\n   â€¢ 5-fold repetition with cluster shuffling â‡’ 1500 device-hours.\n\n7. Metrics\n   Primary   Inter-device hit-rate IH (%)\n   Secondary Median first-token latency, BLE traffic (kB), device energy\n             overhead, privacy precision/recall, ÎµÌ‚\n\n8. Baselines & comparisons\n   â€¢ ORION-FSB vs ORION-NoFSB (ablation)\n   â€¢ Literature baseline: Stackedâ€ŠLRU local cache (re-implemented [*])\n     [*]LRU supports fixed-size tokens only; we extended capacity by\n     re-hashing DVP indices â†’ note in paper.\n\n9. Hyper-parameters inspected\n   â€¢ Bloom size bâˆˆ{64,128,256} bits\n   â€¢ K top phasor directions Kâˆˆ{16,32,64}\n   â€¢ DP-noise Ïƒâˆˆ{0.1,0.3,0.5}; Îµ estimated via moments accountant\n\n10. Robustness\n   â€¢ Cluster churn: 20 % devices disconnect every 5 min.\n   â€¢ Bandwidth throttling: drop 10 % BLE advertisements.\n   â€¢ OOD: New emoji burst (5 k) mid-run.\n\n11. Compute/Memory/Cost\n   â€¢ Per-device SRAM usage recorded by MCU debug probes.\n   â€¢ FLOPs negligible (Bloom set ops); network cost = Î£ rx/tx bytes.\n\n12. Example code (FSB publish-merge loop)\n```\nsketch = bloom.add(top_k_directions)\nmesh.broadcast(sketch)\nagg = secure_aggregate(mesh.recv(), epsilon=0.3)\ncache.warm_start(agg)\n```\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nEXPERIMENT 3  End-to-End Leakage & Compliance Stress-Test\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n1. Purpose\n   Evaluate M-2 (LRT), M-4 (BASS) and M-5 (FCF) against an on-device\n   malware adversary and copyright prompts.\n\n2. Configurations\n   A ASTRA-Base-1.3B                     (no defence)\n   B A + BASS                            (bus scheduler only)\n   C A + LRT                             (latent redaction only)\n   D B + LRT + FCF (full ORION runtime)\n   All share DVP & PSCB from Exp-1 to isolate new modules.\n\n3. Datasets\n   â€¢ Malware-Sniffer benchmark (3 k adversarial prompts)\n   â€¢ ComicBooks10k (copyrighted substrings ground-truth)\n\n4. Pre-processing\n   â€¢ Prompts canonicalised; Unicode confusables mapped.\n   â€¢ Hidden-state taps: we instrument every transformer block output,\n     down-sample to 8-bit for hash-matching to speed auditor.\n\n5. Data splitting\n   â€¢ 3 k prompts â†’ 2 k val / 1 k test (fixed list).  Run each config on\n     all 3 k (validation used only for Ï„, Î» selection).\n   â€¢ Repeated on 3 seeds (model dropouts).\n   â€¢ Early-stop when LRT false-positive rate >1 % on val.\n\n6. Metrics\n   Primary   LLR (latent-leak rate %)  & FUVR (%)\n   Secondary E_DRAM (mJ/100 tok), End-to-End latency (p95 ms), BLEU\n\n7. Hyper-parameter sweeps\n   â€¢ LRT scan period sâˆˆ{4,6,8} layers\n   â€¢ Similarity threshold Ï„âˆˆ{0.7,0.8,0.9}\n   â€¢ FCF Î»âˆˆ{0.0,0.3,0.6}\n   Grid-search on val set; Pareto front plotted (LLR vs Latency).\n\n8. Robustness tests\n   â€¢ Adversarial perturbation: FGSM-style gradient attack on input\n     tokens (Îµ=0.05) to trigger leakage.\n   â€¢ Distribution shift: prompts translated â†’French then back.\n   â€¢ DRAM contention: synthetic background read burst 5 GB/s.\n\n9. Compute & cost accounting\n   â€¢ BASS cycle-accurate simulator â†’ compute row-buffer hit-ratio.\n   â€¢ LRT FLOPs â‰ˆ 2Ã— hidden_dim Ã— seq_len Ã— layers (profiled).  Added\n     VRAM measured; energy overhead per scan logged.\n\n10. Example code fragment (LRT hook)\n```\nclass LRT(nn.Module):\n    def forward(self, h):\n        sim = torch.nn.functional.cosine_similarity(h, forbidden_embs)\n        if torch.any(sim > self.tau):\n            h = h - self.alpha * forbidden_embs.mean(0)  # counter-phasor\n        return h\n...\nfor layer in model.transformer.layers:\n    layer.register_forward_hook(lambda _, __, out: lrt(out))\n```\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSHARED IMPLEMENTATION & LOGGING DETAILS\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ€¢ Framework: PyTorch 2.1 + CUDA 12.2, NCCL 2.17, mixed-precision fp16.\nâ€¢ Seed control: torch.manual_seed(s), numpy, random; cuRAND disabled.\nâ€¢ Profiling tools: torch.profiler, Nsight Systems 2024.1, Perfetto for\n  DRAM; FLOPs exported via ptflops.\nâ€¢ Statistical tests: two-sided paired t-test between best baselines &\n  ORION, Î±=0.05; Holm-Bonferroni correction across metrics.\nâ€¢ Reproducibility: all configs in orion_exps/configs/*.yaml; scripts log\n  git-commit hash + CUDA driver.\nâ€¢ Ethical compliance: copyrighted ComicBooks10k used under research\n  exemption; PII canaries synthetically generated per GDPR guidelines.\nâ€¢ NO-FALLBACK: If any dataset fails checksum â‰  published MD5, scripts\n  abort with FileIntegrityError.\n",
            expected_models=[
                "ASTRA-Base-1.3B (vanilla LPD)",
                "ASTRA-Base-1.3B + DVP + PSCB (ORION)",
                "ASTRA-Base-1.3B + DVP + PSCB + FSB",
                "ASTRA-Base-1.3B + BASS",
                "ASTRA-Base-1.3B + LRT",
                "ASTRA-Base-1.3B + FCF",
                "ASTRA-Base-1.3B Full ORION (DVP+PSCB+FSB+BASS+LRT+FCF)",
            ],
            expected_datasets=[
                "Reddit-WhatsApp-10Day",
                "Emoji-Chat v1.0",
                "Canary-Set PII list",
                "Malware-Sniffer benchmark",
                "ComicBooks10k (copyright corpus)",
            ],
        ),
    )
}
