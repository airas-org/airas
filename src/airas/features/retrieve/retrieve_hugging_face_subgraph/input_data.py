from airas.types.research_hypothesis import ExperimentalDesign, ResearchHypothesis

retrieve_hugging_face_subgraph_input_data = {
    "new_method": ResearchHypothesis(
        method="Open Problems: 1) Existing CL methods treat “data memory” and “parameter growth” as two separate issues; none couples them with compute- or energy-budget constraints. 2) Even the most compact replay buffers still keep either raw pixels or floating-point latents; none can store task evidence in *sub-byte* form while remaining differentiable. 3) Fixed encoders eliminate drift but collapse when the input domain drifts (e.g.\nweather, camera, modality). 4) Current PEFT branches are memory-light yet SRAM-hungry at training time because optimizers must maintain full-precision momenta. 5) No prior work reports *on-device* energy or privacy leakage under the same memory budget.\nMethods: We propose UniMem, a hardware-aware, privacy-preserving continual-learning system that couples ultra-compressed episodic storage with zero-growth adaptation.\nA. Sub-byte Hierarchical Latent Replay (SHLR)\n  • A shallow, frozen vision-speech encoder maps input x→z∈R^128.  Two-stage product quantization (PQ-VQ) converts z to 2 four-bit indices (≈1 byte) which are arithmetic-coded offline (⇢0.6 byte/img).  \n  • A TinyBit-VAE decoder (350 k params) is trained once; at replay time it reconstructs differentiable latents via straight-through Gumbel tricks.  \n  • To keep the 1-byte code informative under domain shift, we introduce “codebook reheating”: whenever validation loss spikes, only the embedding vectors referenced in the last K batches are fine-tuned for one epoch, then re-quantised—no new bytes are stored.\nB. Budget-neutral Meta-Adapters\n  • Instead of LoRA, we learn rank-4 Butterfly-structured adapters whose weights are ternary (-1,0,1).  Gradients are accumulated in 8-bit Kahan buffers and flushed every 8 steps, eliminating 32-bit optimiser state.\n  • After a task, adapters are merged into the backbone using a closed-form Hadamard update; the ternary diff is XOR-compressed and archived (≈6 kB per task). During inference the active model is *identical* in size to the original.\nC. Unified neuro-compression scheduler\n  • Given a target RAM+Flash+Energy budget, an ILP chooses (buffer bytes, decoder width, adapter rank) that maximises expected accuracy under analytic FLOP/energy models calibrated on an ARM Cortex-M55.\nD. Privacy guard\n  • Each stored code is encrypted with task-specific one-time pads; guessing any raw pixel from a 0.6-byte code has <2⁻³⁰ success probability.  We publish a formal DP bound.\n\nExperimental Setup: Datasets & modalities: Split CIFAR-100 (32×32), Tiny-ImageNet-200 (64×64), ImageNet-R (224×224, 10 tasks), SpeechCommands-35 (1-s audio), and Ego4D 32×32 action clips.\nBaselines: ER (5 k), ER-SVHN-PQ, SparCL-DER++, InfLoRA, AQM, and fine-tune.\nBudgets: (RAM,Flash) = {(0.5 MB,2 MB), (2 MB,8 MB)}.  Measure: AvgAccuracy, AvgForgetting, Energy/train-step (Keysight DAQ), DP-ε, and code-reconstruction PSNR.\nAblations: (i) raw-pixel buffer of equal bytes, (ii) no codebook reheating, (iii) float adapters, (iv) optimiser state pruning off.\nEdge deployment: run on STM32H7 (Cortex-M7, 600 MHz) with CMSIS-NN kernels; report wall-clock and peak power.\nExpected Result: • At 0.5 MB RAM / 2 MB Flash UniMem matches 40 MB ER on CIFAR-100 (+1 pp) and beats InfLoRA by 3 pp on ImageNet-R while consuming 4× less energy.\n• Average code size 0.62 byte per sample, 50× smaller than AQM and 300× smaller than raw replay; privacy breach prob <2⁻²⁹.\n• After 20 tasks total model parameters grow <1.5 %, archived XOR diffs ≈120 kB.\n• On-device training reaches 6.8 images/s at 120 mW; competing methods either exceed RAM or run 4× slower.\n• Without codebook reheating accuracy drops by 7 pp on cross-domain tasks, demonstrating necessity of adaptive quantiser.\nExpected Conclusion: UniMem shows that sub-byte latent replay combined with ternary, merge-and-archive adapters can satisfy stringent memory *and* energy budgets while preserving privacy in continual learning.  By jointly optimising data, parameters and optimiser state under a single budget, UniMem advances the feasibility of lifelong learning on micro-controllers, wearables and privacy-critical IoT cameras.  Future work: extend SHLR to text tokens and integrate diffusion decoders for higher-fidelity reconstruction without extra bytes.",
        experimental_design=ExperimentalDesign(
            experiment_strategy="============================================================\nEXPERIMENT 1. Budget-Constrained Continual-Learning Benchmark\n------------------------------------------------------------\nGoal: Show that UniMem hits the best accuracy / forgetting numbers while strictly obeying a unified RAM-Flash-Energy budget.\n\nProtocol\n1. Datasets & task order\n   • Vision – Split CIFAR-100 (10×10 classes), Tiny-ImageNet-200 (20×10), ImageNet-R (10×10).\n   • Audio – SpeechCommands-35 (7 sequential keyword groups).\n   Total = 47 tasks, ~2 M samples.\n2. Budgets (identical to paper):\n   (0.5 MB RAM / 2 MB Flash) and (2 MB RAM / 8 MB Flash).\n3. Baselines (all re-implemented with the same CNN/CRNN backbone): ER-5 k, ER-SVHN-PQ, SparCL-DER++, InfLoRA, AQM, full fine-tune.\n4. Metrics per task & averaged at the end:\n   • AvgAccuracy ↑, AvgForgetting ↓\n   • Peak RAM (CUDA-profiler) & Flash footprint\n   • Energy/train-step measured with NVIDIA’s nvidia-smi power draw and Keysight DAQ for STM32 (see Exp-3)\n   • Training wall-clock & FLOPs.\n5. Runs: 3 seeds, report mean±σ.\n\nExpected Outcome\n• Under the 0.5 MB/2 MB budget, UniMem reaches 71 % AvgAcc on CIFAR-100 (+1 pp vs 40 MB ER, +6 pp vs best budget-matched baseline) and 64 % on ImageNet-R (+3 pp vs InfLoRA).\n• Peak RAM stays ≤0.49 MB and Flash ≤1.98 MB for all 47 tasks; competing methods overshoot or early-stop.\n• Energy per train-step is 4× lower than SparCL and 6× lower than ER-SVHN-PQ, validating the coupled optimiser-state and replay compression design.\nDemonstration: UniMem simultaneously satisfies memory and energy caps while improving CL performance, proving the value of jointly optimising data and parameter footprints.\n\n============================================================\nEXPERIMENT 2. Domain-Shift Stress-Test & Codebook Reheating Ablation\n------------------------------------------------------------\nGoal: Verify that Sub-byte Hierarchical Latent Replay (SHLR) and the reheating mechanism preserve information when the input distribution drifts.\n\nProtocol\n1. Source/Target pairs\n   • CIFAR-100 → CIFAR-C (15 corruption types, 5 severities)\n   • Tiny-ImageNet → Tiny-ImageNet-C\n   • Ego4D 32×32 clips → Ego4D-Outdoor (different camera/weather)\n2. Train UniMem for 10 tasks on the source splits under 0.5 MB budget, then continue with 5 tasks drawn from the target domain.\n3. Compare the following variants:\n   A. Full UniMem (with reheating)\n   B. UniMem-NoReheat (embedding fixed)\n   C. Raw-pixel buffer that stores exactly the same number of bytes (≈0.62 bytes/img × N) using PNG-to-bytes anecdotally.\n4. Metrics:\n   • Target-domain AvgAcc & Forgetting\n   • Latent reconstruction PSNR / SSIM\n   • Extra bytes written after drift (should stay 0 for UniMem)\n   • Decoder fine-tune time & energy.\n\nExpected Outcome\n• Variant A loses <2 pp accuracy between source and target, whereas B drops ~9 pp and C drops ~5 pp (too few pixels to be useful).\n• Reheating touches <3 % of the codebook vectors and writes zero new replay bytes, confirming “zero-growth”.\n• PSNR improves by ~4 dB after each reheating event, correlating with regained accuracy.\nDemonstration: SHLR codes remain adaptive and informative without expanding memory, highlighting UniMem’s robustness under real-world distribution shift.\n\n============================================================\nEXPERIMENT 3. Edge-Device Deployment: Energy & Privacy Evaluation\n------------------------------------------------------------\nGoal: Show that UniMem can learn on a micro-controller within tight power budgets while leaking negligible private information.\n\nProtocol\n1. Hardware: STM32H7 (Cortex-M7, 600 MHz, 512 kB SRAM, 2 MB Flash) + Keysight N6705C power analyser.\n2. Port the TinyBit-VAE and ternary Butterfly adapters to CMSIS-NN; quantise weights to int8/ternary, activations to int8.\n3. Train 5 CIFAR-100 tasks (500 steps each) on-device:\n   • UniMem (scheduler chooses: buffer = 50 kB, decoder width = 32, rank = 4)\n   • InfLoRA-int8 (closest state-of-the-art that fits SRAM)\n   • Fine-tune-int8 (no replay, no adapters).\n4. Measure:\n   • Images/second, mJ/train-step, peak current draw\n   • Final accuracy & forgetting\n   • Total bytes written to Flash across tasks\n   • Privacy risk: run a shadow-model membership-inference attack on stored codes vs 8-bit latent baseline; report attack AUC and empirical δ in (ε,δ)-DP.\n\nExpected Outcome\n• UniMem trains at 6.8 img/s, 120 mW average, never exceeds 480 kB SRAM. InfLoRA runs 1.6 img/s and OOMs without optimizer off-loading; Fine-tune matches speed but ends 18 pp lower in accuracy.\n• Cumulative Flash usage after 5 tasks: UniMem = 78 kB (codes + XOR diffs); InfLoRA = 640 kB.\n• Membership-inference AUC is 0.52 (≈random) for UniMem codes vs 0.71 for 8-bit latents; empirical δ≈1e-9 at ε=1.2, supporting formal bound.\nDemonstration: On real hardware, UniMem meets sub-watt power envelopes and materially reduces privacy leakage, confirming the practical value of its hardware-aware and encrypted sub-byte design.\n============================================================",
            experiment_details="==============================\nEXPERIMENT 1 – Budget-Constrained Continual-Learning Benchmark\n==============================\n1. Models\n   • Proposed:  UniMem\n     – Backbone (shared with all baselines):  EfficientNet-V2-S (vision, 3.3 M fp16 parameters) and CRNN-Small (audio, 0.9 M fp16).\n     – Frozen front‐end encoder: first 3 stages of EfficientNet (vision) / first 2 conv blocks of CRNN (audio) → 128-d latent z.\n     – TinyBit-VAE (≈350 k params, 2 residual blocks, latent 128→128) + SHLR (2×4-bit PQ-VQ) + Gumbel-ST.\n     – Budget-neutral Meta-Adapters: rank-4 ternary butterfly modules inserted after MBConv blocks; merged post-task.\n   • Baselines (identical backbone, fp16 unless stated):\n     – ER-5 k (Chaudhry 2019)\n     – ER-SVHN-PQ (Hayes 2021)\n     – SparCL-DER++ (Yoon 2023)\n     – InfLoRA (Park 2024)\n     – AQM (Lin 2024)\n     – Fine-tune (no replay / adapters).\n     All baselines re-implemented to obey the same two budgets; replay size scaled down when necessary.\n\n2. Datasets\n   • CIFAR-100 Split: 10 tasks × 10 classes (50 k train / 10 k test)\n   • Tiny-ImageNet-200 Split: 20 tasks × 10 classes (100 k / 10 k)\n   • ImageNet-R Split: 10 tasks × 10 classes (24 k / 6 k)\n   • SpeechCommands-35: 7 tasks (≈84 k wavs) grouped by phonetic similarity.\n   All datasets downloaded from official mirrors; MD5 verified.\n\n3. Pre-processing\n   Vision –\n     Resize: 224² (Tiny-ImageNet / ImageNet-R) or 32² (CIFAR).\n     Augs: RandAug (N=2,M=9), CutMix 1.0, RandErase 0.25.\n     Normalise to ImageNet mean/std.\n   Audio –\n     16-kHz mono; 25-ms hop, 64-bin log-Mel.  SpecAug (time/freq mask 2/2).\n   Latent quantisation codebook initialised with k-means on 5 % of first task.\n\n4. Splitting & Curriculum\n   For every dataset, tasks presented sequentially (vision first, then audio) → 47 tasks total.  No shuffling.  Standard test sets kept untouched.  Validation = 10 % of current task’s train set, stratified.\n   Continual-learning loop repeats for 3 random seeds {1,10,42}.\n\n5. Training details\n   Optimiser: AdamW (β=(0.9,0.95)), lr=3e-4 (UniMem & baselines); adapters use lr=2× base lr.\n   Schedulers: cosine decay, warm-up 500 steps.  Batch = 256 (CIFAR), 128 (others) on 8×A100 @ fp16 with gradient-accum 2.\n   Selection criterion: best-val accuracy for each task snapshot; at the end we report\n     (i) last-epoch accuracy (AvgAcc_last)\n     (ii) best-val checkpoint accuracy (AvgAcc_best).  Main paper chooses (i).\n   Early-stopping if val loss ≯ min(loss) for 8 epochs.\n\n6. Evaluation metrics\n   Primary:  AvgAccuracy ↑, AvgForgetting ↓\n   Secondary:  Peak RAM, Flash, Energy/train-step, Training wall-clock, FLOPs/train-step (fvcore), code-PSNR, DP-ε (Renyi).  All values mean±std over 3 seeds.\n\n7. Hyper-parameter sweep\n   • Buffer bytes ∈ {0.25,0.5,1.0 MB}\n   • Adapter rank ∈ {2,4,8}\n   • Learning rate ∈ {1e-4,3e-4,1e-3}\n   Grid search on first 3 CIFAR tasks under 2 MB flash budget; best AvgAcc_best continues.\n\n8. Robustness checks (inside Exp-1 run)\n   Add 15 % Gaussian noise to 10 % of minibatches and randomly permute 2 % labels.  Report accuracy drop.\n\n9. Resource measurement\n   • PyTorch profiler: FLOPs, GPU-mem, time.\n   • nvidia-smi –loop-ms=200 ⇒ power draw averaged over 2 k steps → energy.\n   • Flash footprint: bytes(sys.getsizeof(model.state_dict())).\n\n10. Example code (excerpt)\n```\nseed=1\nset_seed(seed)\ndevice=torch.device('cuda')\nmodel=UniMem(backbone='efficientnet_v2_s').to(device)\ncl_loader=TaskStream([...])\nfor task_id,(train_ds,val_ds,test_ds) in enumerate(cl_loader):\n    train_loader=torch.utils.data.DataLoader(train_ds,batch_size=bs,shuffle=True,num_workers=8,pin_memory=True)\n    optim=torch.optim.AdamW(model.parameters(),lr=3e-4,weight_decay=1e-4)\n    best_acc=0\n    for epoch in range(max_epoch):\n        for x,y in train_loader:\n            out=model(x.to(device))\n            loss=ce(out,y.to(device))+model.replay_loss()\n            loss.backward()\n            if (step+1)%accum==0:\n                optim.step(); optim.zero_grad(); model.step_scheduler()\n        val_acc=eval(model,val_ds)\n        if val_acc>best_acc:\n            best_acc,ckpt=val_acc,model.state_dict()\n        if early_stop(val_history): break\n    model.load_state_dict(ckpt)\n    model.finish_task()              # merge adapters, quantise new codes\n```\n\n==============================\nEXPERIMENT 2 – Domain-Shift Stress-Test & Reheating Ablation\n==============================\n1. Variants\n   A. UniMem (default)\n   B. UniMem-NoReheat (freeze codebook after first 10 tasks)\n   C. RawPixel-0.6B (store ⌈0.62 × N⌉ bytes PNG – obviously very few pixels)\n\n2. Procedure\n   • Train on 10 source tasks (CIFAR-100 or Tiny-ImageNet), budget 0.5 MB RAM / 2 MB Flash.\n   • Switch to 5 target tasks (CIFAR-C etc.).  Reheating triggered when val loss > μ+2σ.\n   • No hyper-parameter change during shift.\n\n3. Metrics & logging\n   – Target-domain AvgAcc, Forgetting.\n   – Latent PSNR/SSIM w.r.t. encoder output.\n   – #Codebook vectors fine-tuned, extra bytes written (flash write counter).\n   – Fine-tune compute: time, energy (GPU wattmeter).\n   – Robustness: corruption mCE (mean Corruption Error) per Hendrycks.\n\n4. Example code (reheating)\n```\nif val_loss>mu+2*sigma:\n    touched=collect_recent_codes(K=512)\n    optimiser=Adam(touched_vectors,lr=1e-3)\n    for epoch in range(1):\n        recon_loss=vae(dec(quant(touched)))\n        (recon_loss*lambda).backward(); optimiser.step(); optimiser.zero_grad()\n    requantise(touched_vectors)\n```\n\n==============================\nEXPERIMENT 3 – Edge-Device Deployment, Energy & Privacy\n==============================\n1. Hardware / Tool-chain\n   • STM32H7B3I-DK board (600 MHz Cortex-M7, 512 kB SRAM, 2 MB Q-SPI flash)\n   • CMSIS-NN 5.10 + Arm-clang 6.19 –O3 –mcpu=cortex-m7 –mfpu=fpv5-sp-d16 –mfloat-abi=hard.\n   • On-board ST-LINK energy probe wired to Keysight N6705C.\n\n2. Porting steps\n   • Convert TinyBit-VAE & backbone to int8 weights using ONNX-Runtime-Quant.\n   • Custom ternary GEMM kernel (3-valued weight, int8 act) via pack-2bits → reuse CMSIS-NN depthwise.\n   • Replay buffer stored in Q-SPI; DMA bursts 1 kB into SRAM circular cache.\n\n3. Training loop parameters\n   Batch=16, lr=5e-4 fixed-point (Q1.15), optimiser=8-bit Kahan (acc buf int32).\n   5 CIFAR-100 tasks ×500 updates each.\n\n4. Measurements\n   • Throughput: RTOS timestamp before/after 1 k steps.\n   • Power: Keysight 50 ksps log; energy = ∫V·I dt.\n   • Privacy attack: record 10 k stored codes, train ResNet-18 shadow, compute AUC.\n   • DP bound: compute ε(δ) following analytic moments of Rényi DP with σ=0.3 noise on codes.\n\n5. Example C pseudo-code\n```\nvoid train_step(int8_t *images, uint8_t *labels){\n  encode_latent(images, z_buf);\n  quantise(z_buf, &code);\n  reconstruct(code, recon);\n  fwd_backbone(recon, logits);\n  ce_loss(logits,labels, &loss);\n  kahan_update(loss, params, grads, acc8);\n  if(step%8==7) flush_accumulators();\n}\n```\n\n6. Reporting\n   Tabulate images/s, mJ/step, peak mA, final Acc, Forgetting, Flash bytes written, MI-AUC, ε,δ.\n\n==============================\nShared components\n==============================\nHyper-parameter sensitivity study (across all exps) – vary lr ±2×, buffer bytes ±50 %, adapter rank {2,4,8}, temperature τ∈{0.5,0.9,1.3}.  Plot heat-map of AvgAcc versus memory & energy.\nNoise & adversarial robustness – PGD-l∞(ε=4/255) on CIFAR test for UniMem vs InfLoRA.\nFLOPs / time / memory – automatically exported by PyTorch profiler; edge device numbers obtained via cycle counter (DWT_CYCCNT).\n\n==============================\nFoot-notes / Implementation clarifications\n==============================\n† All baselines retrained with half-precision activations and int8 replay to respect flash; original papers used fp32.\n† InfLoRA required per-layer scale renormalisation to avoid overflow in int8; details in supplementary code.\n† Energy on A100 excludes idle power by subtracting baseline board draw measured with empty loop.\n\n",
            expected_models=[
                "EfficientNet-V2-S",
                "CRNN-Small",
                "TinyBit-VAE",
                "UniMem (SHLR + ternary Butterfly adapters)",
                "ER-5k",
                "ER-SVHN-PQ",
                "SparCL-DER++",
                "InfLoRA",
                "AQM",
                "Fine-tune",
                "UniMem-NoReheat",
                "RawPixel-0.6B",
                "InfLoRA-int8",
                "Fine-tune-int8",
            ],
            expected_datasets=[
                "CIFAR-100",
                "Tiny-ImageNet-200",
                "ImageNet-R",
                "SpeechCommands-35",
                "Ego4D",
                "CIFAR-C",
                "Tiny-ImageNet-C",
                "Ego4D-Outdoor",
            ],
        ),
    )
}
