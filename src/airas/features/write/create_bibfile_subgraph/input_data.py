create_bibfile_subgraph_input_data = {
    "research_study_list": [
        {
            "title": "Deep Learning for Natural Language Processing: A Comprehensive Survey",
            "abstract": "This paper presents a comprehensive survey of deep learning techniques applied to natural language processing tasks, including sentiment analysis, machine translation, and text classification.",
            "authors": ["Smith, John", "Johnson, Alice"],
            "published_year": 2023,
            "meta_data": {
                "authors": ["Smith, John", "Johnson, Alice"],
                "published_year": 2023,
                "journal": "Nature Machine Intelligence",
                "volume": "5",
                "number": "3",
                "pages": "123-145",
                "doi": "10.1038/s42256-023-00567-8",
                "arxiv_url": "https://arxiv.org/abs/2301.12345",
                "github_url": "https://github.com/nlp-survey/deep-learning-nlp",
            },
        },
        {
            "title": "Transformer Networks: Architecture and Applications",
            "abstract": "We explore the transformer architecture and its applications across various domains including computer vision and natural language processing.",
            "authors": ["Brown, Michael", "Davis, Sarah"],
            "published_year": 2024,
            "meta_data": {
                "authors": ["Brown, Michael", "Davis, Sarah"],
                "published_year": 2024,
                "journal": "Journal of Machine Learning Research",
                "volume": "25",
                "pages": "1-28",
                "doi": "10.5555/3648699.3648700",
                "arxiv_url": "https://arxiv.org/abs/2401.56789",
            },
        },
    ],
    "reference_list": [
        {
            "title": "Attention Is All You Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism.",
            "authors": ["Vaswani, Ashish", "Shazeer, Noam", "Parmar, Niki"],
            "meta_data": {
                "authors": ["Vaswani, Ashish", "Shazeer, Noam", "Parmar, Niki"],
                "published_year": 2017,
                "journal": "Advances in Neural Information Processing Systems",
                "volume": "30",
                "doi": "10.48550/arXiv.1706.03762",
                "arxiv_url": "https://arxiv.org/abs/1706.03762",
            },
        },
        {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.",
            "authors": ["Devlin, Jacob", "Chang, Ming-Wei", "Lee, Kenton"],
            "meta_data": {
                "authors": ["Devlin, Jacob", "Chang, Ming-Wei", "Lee, Kenton"],
                "published_year": 2018,
                "journal": "Proceedings of NAACL-HLT",
                "pages": "4171-4186",
                "doi": "10.18653/v1/N19-1423",
                "arxiv_url": "https://arxiv.org/abs/1810.04805",
            },
        },
        {
            "title": "GPT-3: Language Models are Few-Shot Learners",
            "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task.",
            "authors": ["Brown, Tom B.", "Mann, Benjamin", "Ryder, Nick"],
            "meta_data": {
                "authors": ["Brown, Tom B.", "Mann, Benjamin", "Ryder, Nick"],
                "published_year": 2020,
                "journal": "Advances in Neural Information Processing Systems",
                "volume": "33",
                "pages": "1877-1901",
                "arxiv_url": "https://arxiv.org/abs/2005.14165",
            },
        },
        {
            "title": "Computer Vision and Pattern Recognition Methods",
            "abstract": "This paper focuses on traditional computer vision techniques for image classification and object detection without deep learning approaches.",
            "authors": ["Wilson, Robert", "Taylor, Emma"],
            "meta_data": {
                "authors": ["Wilson, Robert", "Taylor, Emma"],
                "published_year": 2019,
                "journal": "Computer Vision Research",
                "volume": "12",
                "number": "4",
                "pages": "88-102",
                "doi": "10.1234/cvr.2019.12345",
            },
        },
        {
            "title": "ResNet: Deep Residual Learning for Image Recognition",
            "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.",
            "authors": ["He, Kaiming", "Zhang, Xiangyu", "Ren, Shaoqing"],
            "meta_data": {
                "authors": ["He, Kaiming", "Zhang, Xiangyu", "Ren, Shaoqing"],
                "published_year": 2016,
                "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pages": "770-778",
                "doi": "10.1109/CVPR.2016.90",
                "arxiv_url": "https://arxiv.org/abs/1512.03385",
            },
        },
    ],
    "research_hypothesis": {
        "hypothesis": "Deep learning models with attention mechanisms significantly improve performance on natural language understanding tasks compared to traditional approaches",
        "background": "Recent advances in transformer architectures have shown remarkable success in various NLP tasks. This research investigates the effectiveness of attention-based models.",
        "methodology": "We compare transformer-based models against traditional RNN and CNN approaches on multiple NLP benchmarks including sentiment analysis, text classification, and machine translation tasks.",
    },
    "github_repository": {
        "github_owner": "auto-res2",
        "repository_name": "create_bibfile_test_matsuzawa",
        "branch_name": "develop",
    },
}
