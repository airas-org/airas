import re

from airas.utils.parse_bibtex_to_dict import parse_bibtex_to_dict


def convert_placeholders_to_citations(
    paper_content: dict[str, str], references_bib: str
) -> dict[str, str]:
    references_bib_dict = parse_bibtex_to_dict(references_bib)
    for key, content in paper_content.items():
        converted_text = re.sub(
            r"\[([^\[\]]+?)\]",
            lambda match: _replace_citation(match, references_bib_dict),
            content,
        )
        paper_content[key] = converted_text
    return paper_content


def _replace_citation(match, references_bib_dict: dict[str, dict[str, str]]):
    key = match.group(1)
    if key in references_bib_dict:
        return f"\\cite{{{key}}}"
    else:
        return match.group(0)


if __name__ == "__main__":
    paper_content = {
        "Title": "Attention-Based Reinforcement Learning for Dynamic Decision-Making",
        "Abstract": "This paper introduces a novel approach that combines transformer-based attention mechanisms with reinforcement learning to enhance decision-making in dynamic environments. By leveraging the multi-head attention framework [ashish_2017_attention], our method effectively captures long-range temporal dependencies, while integrating policy gradient techniques [schulman_2017_ppo] provides robust optimization for action selection. The approach addresses critical challenges in environments with sparse and delayed rewards, such as Atari games and continuous control tasks, where traditional models struggle to incorporate historical context. Our contribution lies in the architectural integration of attention modules with a reinforcement learning framework, which leads to more interpretable decision policies and stable training dynamics. We verify the efficacy of our method through extensive experiments conducted on five distinct environments using three random seeds per environment. Results indicate an approximate 15% increase in average cumulative reward compared to the state-of-the-art baseline Proximal Policy Optimization. Detailed ablation studies confirm the importance of both the attention and policy optimization components, and visualizations of attention weights provide insights into the temporal focus of the agent. Overall, our findings underscore the potential of combining transformer attention with policy gradients to achieve significant performance gains in dynamic decision-making tasks.",
        "Introduction": "Dynamic environments pose substantial challenges for reinforcement learning agents, especially when decisions require an understanding of long-term dependencies and intricate temporal patterns. In this study, we propose a novel integration of transformer-based attention mechanisms with classical reinforcement learning techniques to address these challenges. The core idea is to employ multi-head attention to selectively extract pertinent information from past states, thereby enabling an adaptive policy that is sensitive to temporal context. This is particularly relevant for tasks such as Atari gaming and continuous control, where the agent must manage sparse rewards and delayed feedback. Previous work in attention mechanisms [ashish_2017_attention] has demonstrated the ability to capture global dependencies without the need for recurrent structures, while methods like Proximal Policy Optimization [schulman_2017_ppo] have provided stable and effective policy updates. However, each of these methodologies has its limitations when applied in isolation. In contrast, our approach bridges these gaps by integrating the two methods into a unified model. Our specific contributions are as follows:\n\u2022 We design a model that harnesses multi-head attention to dynamically weigh historical states, providing a richer context for decision-making.\n\u2022 We implement a reinforcement learning framework that is optimized using policy gradient methods, ensuring a direct alignment with long-term reward maximization.\n\u2022 We conduct extensive experiments across five benchmark environments, demonstrating a consistent 15% increase in average cumulative rewards when compared against state-of-the-art baselines.\n\u2022 We perform comprehensive ablation studies and attention visualizations to substantiate the critical role of each model component and highlight avenues for future research.\nThe remainder of this paper is organized into several sections. We first review related work to contextualize our contributions within existing literature. The Background section outlines the foundational concepts and formal problem setting. The Method section describes the integrated architecture and training procedure in detail. This is followed by an in-depth discussion of our Experimental Setup, results, and a conclusion that summarizes our findings and proposes directions for future work.",
        "Related Work": "Recent years have witnessed significant advances in both reinforcement learning and attention-based models. Traditional approaches, such as Deep Q-Networks [mnih_2015_dqn], have successfully applied deep learning to control tasks by learning policies directly from high-dimensional input. However, these methods are often challenged by long-term dependencies and the issues stemming from sparse reward environments. With the advent of attention mechanisms, particularly within transformer architectures [ashish_2017_attention], new possibilities have emerged for modeling global dependencies in a more efficient manner. On the reinforcement learning front, algorithms like Proximal Policy Optimization [schulman_2017_ppo] have gained popularity due to their balance between training stability and performance through conservative policy updates. The current literature reflects parallel development streams: while attention mechanisms have been predominantly used in natural language processing and sequence modeling, their integration into reinforcement learning frameworks remains underexplored. Our work compares and contrasts these methodologies by demonstrating that an attention-augmented reinforcement model can leverage the strengths of both approaches. Unlike traditional RL methods that overlook the structured dynamics of past states, our approach explicitly models temporal dependencies, a feature that has been only modestly explored in prior works. Moreover, although methods like PPO provide robust performance, they do not incorporate a mechanism to focus on relevant historical states. By juxtaposing our integrated method with these baselines, we illustrate that the explicit incorporation of transformer-based attention mechanisms can lead to significant performance improvements in dynamic environments. In this respect, our study not only builds upon but also extends the current state-of-the-art by offering a novel solution to a well-recognized limitation in conventional RL algorithms.",
        "Background": "A solid grasp of both transformer attention mechanisms and reinforcement learning principles is fundamental to understanding our approach. The transformer model, introduced in [ashish_2017_attention], has reshaped sequence transduction by dispensing with recurrence in favor of multi-head attention, which captures the relationships between distant elements in a sequence through parallel attention heads. This mechanism computes weighted sums of input representations, thereby selectively emphasizing relevant features. The reinforcement learning framework, especially techniques based on policy gradients, directly optimizes the expected cumulative reward by iteratively updating the policy based on observed outcomes. Proximal Policy Optimization [schulman_2017_ppo] is emblematic of this class of methods, balancing efficient exploration and stable training through controlled policy updates. The problem setting addressed in this paper is formalized as a Markov Decision Process (MDP), characterized by a state space, action space, transition dynamics, and a reward function. Within this framework, the agent receives a sequence of observations, and our approach enriches each state representation with a context vector derived from the multi-head attention mechanism. The result is a nuanced policy that adapts to temporally extended information. It is assumed that the environment exhibits significant temporal dependencies, such that the relevance of past observations varies over time. This assumption is crucial because it justifies the integration of attention mechanisms within the reinforcement learning paradigm. Moreover, while classical reinforcement learning addresses reward maximization directly, it often struggles to separate significant historical events from noise. By embedding an attention module, our method dynamically prioritizes historical states that are most informative for future decisions. Thus, the background for our study is grounded in two key theoretical pillars: the transformer-based attention mechanism for contextual understanding and policy gradient methods for reward optimization, both of which are essential for effective decision-making in dynamic environments.",
        "Method": "Our proposed methodology unifies transformer-based multi-head attention with a reinforcement learning framework to enhance decision-making in dynamic environments. Initially, the agent observes the state of the environment, which comprises a sequence of observations. This state is processed by an attention module that employs multiple attention heads. Each head computes a weighted representation of past states, thus capturing different perspectives and temporal patterns. The outputs from the attention heads are then concatenated and transformed into a single context vector. This context vector is integrated with the current state representation and subsequently passed to the policy network, which is subsequently optimized using policy gradient methods as described in [schulman_2017_ppo]. The action distribution produced by the policy network is used to sample an action, thereby influencing the subsequent environment state. Training is performed end-to-end, updating both the attention module and the policy network jointly. The optimization is facilitated by the Adam optimizer, with a learning rate of 3e-4 and a batch size of 256. The following pseudocode illustrates the overall training process:\n\ndef train_agent(env, model, num_timesteps=1000000):\n    optimizer = Adam(model.parameters(), lr=3e-4)\n    for step in range(num_timesteps):\n        state = env.reset()\n        episode_reward = 0\n        while not done:\n            action = model.select_action(state)\n            next_state, reward, done, _ = env.step(action)\n            model.update(state, action, reward, next_state)\n            state = next_state\n            episode_reward += reward\n        if step % 10000 == 0:\n            evaluate_model(model, env)\n\nA key novelty of our method is the explicit focus on temporal dynamics. The attention mechanism provides interpretability, allowing for visual analysis of which past states are deemed most relevant, and ablation studies verify that excluding the attention component results in a marked drop in performance. The challenge in our approach lies in the concurrent optimization of the attention parameters and the policy network, necessitating careful balancing of learning rates and periodic evaluations to ensure stable convergence. Our method builds upon established approaches [ashish_2017_attention, schulman_2017_ppo] while innovatively combining them to meet the challenges of dynamic decision-making tasks. The technical design not only maximizes the cumulative reward but also enhances the model's ability to generalize in environments with complex temporal dependencies.",
        "Experimental Setup": "We designed a rigorous experimental setup to evaluate our attention-based reinforcement learning approach under dynamic decision-making conditions. Experiments were conducted across five distinct environments, including benchmarks from the Atari suite and continuous control tasks, with each environment posing unique challenges in terms of sparse rewards and temporal complexities. Each environment was run with three different random seeds to ensure the robustness and reproducibility of results. The agent was trained for 1 million timesteps per environment, with performance evaluations conducted every 10,000 steps. The primary performance metric is the average cumulative reward measured over 100 episodes, which provides an indicator of both short-term and long-term policy effectiveness. Implementation details include the use of the Adam optimizer with a learning rate of 3e-4 and a batch size of 256, ensuring that both the attention mechanism and the policy network are optimized efficiently. The experimental design includes a direct comparison with the Proximal Policy Optimization method [schulman_2017_ppo], serving as the baseline. Each episode begins with a complete reset of the environment, and the agent interacts with the environment until the episode concludes. The training loop, detailed in the provided pseudocode, encapsulates the core interactions, updates, and periodic evaluations. Additionally, specialized analysis was carried out to visualize the attention weights, highlighting how the agent focuses on temporally relevant information during decision-making. The experimental protocol was designed to mirror standard practices in the field, ensuring that results are comparable to prior studies. This setup provides a controlled yet diverse testing ground to validate the efficacy and scalability of our integrated approach.",
        "Results": "Our experimental outcomes unequivocally demonstrate that incorporating an attention mechanism within a reinforcement learning framework significantly enhances decision-making in dynamic environments. When benchmarked against state-of-the-art methods such as Proximal Policy Optimization [schulman_2017_ppo], our method exhibits an approximately 15% greater average cumulative reward. For concrete figures, our attention-based model achieved an average reward of 520 \u00b1 25 on Breakout, 1840 \u00b1 67 on SpaceInvaders, and 95 \u00b1 8 on CartPole. The stability of training was verified through multiple seeds; the consistent performance across runs indicates strong generalization and robust convergence properties. Analysis of training dynamics reveals steady improvement in cumulative rewards, as illustrated in Figure 1. Figure 1: Training Curves over Timesteps (filename: training_curves.pdf) shows the progressive reward improvement during the 1M timestep training period. Further, attention visualizations provided in Figure 2 illustrate that the model is capable of dynamically prioritizing relevant historical states, which is critical in complex decision-making tasks. Figure 2: Attention Visualization Highlighting Temporal Dependencies (filename: attention_visualization.pdf) emphasizes that the internal mechanism effectively captures temporal cues that guide policy decisions. A comparative performance analysis found in Figure 3 confirms the superior performance of our approach relative to baseline algorithms. Figure 3: Comparative Performance Analysis (filename: performance_comparison.pdf) provides a visual summary of the reward improvements achieved across various environments. Our ablation studies reinforce that both attention and policy gradient components are indispensable; removing either results in noticeable performance degradation. Additionally, hyperparameter sensitivity tests underline the importance of the Adam optimizer settings and batch size in maintaining a balance between context extraction and effective policy learning. Despite these advances, our method did exhibit a dependency on extensive hyperparameter tuning, suggesting a potential avenue for further research to automate this process. In sum, the experimental results validate our hypothesis that an attention-driven reinforcement learning architecture not only ensures stable training but also delivers enhanced performance in dynamic scenarios.",
        "Conclusions": "This paper presents a novel integration of transformer-based attention mechanisms with reinforcement learning to tackle the challenges of dynamic decision-making in complex environments. By fusing multi-head attention with policy gradient methods, our approach effectively captures long-range dependencies and focuses on the most relevant historical states, thereby enhancing the agent's decision-making capabilities. Empirical evaluation across multiple challenging environments, including Atari games and continuous control tasks, demonstrates an average reward improvement of approximately 15% over established methods such as Proximal Policy Optimization. The use of detailed ablation studies and visualization of attention weights reinforces the importance of both components in achieving superior performance. While our findings are promising, highlighting stable training and robust generalization, further research is warranted to automate hyperparameter tuning and extend the approach to even higher-dimensional tasks. Overall, this work lays a strong foundation for future explorations into the integration of transformer architectures with reinforcement learning, offering a promising direction for advancing the state-of-the-art in dynamic, temporally complex decision-making scenarios.",
    }
    reference_bib = """
% ===========================================
% REQUIRED CITATIONS
% These papers must be cited in the manuscript
% ===========================================

@article{ashish_2017_attention,
 abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism.},
 arxiv_url = {https://arxiv.org/abs/1706.03762},
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki},
 doi = {10.48550/arXiv.1706.03762},
 journal = {Advances in Neural Information Processing Systems},
 title = {Attention Is All You Need},
 volume = {30},
 year = {2017}
}

@article{schulman_2017_ppo,
 abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a surrogate objective function using stochastic gradient ascent.},
 arxiv_url = {https://arxiv.org/abs/1707.06347},
 author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla},
 journal = {arXiv preprint arXiv:1707.06347},
 title = {Proximal Policy Optimization Algorithms},
 year = {2017}
}

% ===========================================
% REFERENCE CANDIDATES
% Additional reference papers for context
% ===========================================

@article{mnih_2015_dqn,
 abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning.},
 author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David},
 journal = {Nature},
 pages = {529--533},
 title = {Human-level control through deep reinforcement learning},
 volume = {518},
 year = {2015}
}"""

    result = convert_placeholders_to_citations(paper_content, reference_bib)
    print(result)
