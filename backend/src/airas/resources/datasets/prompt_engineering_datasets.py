PROMPT_ENGINEERING_DATASETS = {
    "GSM8K": {
        "description": "Grade School Math 8K is a dataset of 8,500 high-quality, linguistically diverse grade school math word problems. Each problem requires 2 to 8 steps to solve and includes detailed natural language solutions.",
        "num_training_samples": 7473,
        "num_validation_samples": 1319,
        "huggingface_url": "https://huggingface.co/datasets/openai/gsm8k",
        "task_type": ["text-generation"],
        "language_distribution": "English only (en)",
        "dependent_packages": ["datasets"],
        "code": "from datasets import load_dataset\ndataset = load_dataset('openai/gsm8k', 'main')\ntrain_data = dataset['train']\ntest_data = dataset['test']",
        "citation": "@article{cobbe2021gsm8k,\n  title={Training Verifiers to Solve Math Word Problems},\n  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},\n  journal={arXiv preprint arXiv:2110.14168},\n  year={2021}\n}",
    },
    "MATH": {
        "description": "The MATH dataset consists of approximately 12,500 challenging competition mathematics problems from high school competitions. Each problem includes a detailed step-by-step solution and difficulty level ranging from middle school to early university level.",
        "num_training_samples": 7500,
        "num_validation_samples": 5000,
        "huggingface_url": "https://huggingface.co/datasets/hendrycks/competition_math",
        "task_type": ["text-generation"],
        "language_distribution": "English only (en)",
        "dependent_packages": ["datasets"],
        "code": "from datasets import load_dataset\ndataset = load_dataset('hendrycks/competition_math')\ntrain_data = dataset['train']\ntest_data = dataset['test']",
        "citation": "@article{hendrycksmath2021,\n  title={Measuring Mathematical Problem Solving With the MATH Dataset},\n  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},\n  journal={arXiv preprint arXiv:2103.03874},\n  year={2021}\n}",
    },
    "MMLU": {
        "description": "Massive Multitask Language Understanding (MMLU) is a benchmark covering 57 subjects across STEM, humanities, social sciences, and more. It tests models' world knowledge and problem-solving abilities with multiple-choice questions ranging from elementary to advanced professional level.",
        "num_training_samples": 285,
        "num_validation_samples": 14042,
        "huggingface_url": "https://huggingface.co/datasets/cais/mmlu",
        "task_type": ["text-generation"],
        "language_distribution": "English only (en)",
        "dependent_packages": ["datasets"],
        "code": "from datasets import load_dataset\ndataset = load_dataset('cais/mmlu', 'all')\nfor split in ['auxiliary_train', 'dev', 'test', 'validation']:\n    print(f'{split}: {len(dataset[split])}')",
        "citation": "@article{hendrycks2021measuring,\n  title={Measuring Massive Multitask Language Understanding},\n  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},\n  journal={Proceedings of the International Conference on Learning Representations (ICLR)},\n  year={2021}\n}",
    },
    "TruthfulQA": {
        "description": "TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. The dataset consists of 817 questions across 38 categories including health, law, finance, and politics. Questions are designed to test common misconceptions.",
        "num_training_samples": 0,
        "num_validation_samples": 817,
        "huggingface_url": "https://huggingface.co/datasets/truthfulqa/truthful_qa",
        "task_type": ["text-generation"],
        "language_distribution": "English only (en)",
        "dependent_packages": ["datasets"],
        "code": "from datasets import load_dataset\ndataset = load_dataset('truthfulqa/truthful_qa', 'generation')\nvalidation_data = dataset['validation']",
        "citation": "@article{lin2021truthfulqa,\n  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},\n  author={Stephanie Lin and Jacob Hilton and Owain Evans},\n  journal={arXiv preprint arXiv:2109.07958},\n  year={2021}\n}",
    },
    "HumanEval": {
        "description": "HumanEval is an evaluation harness for code generation that measures functional correctness for synthesizing programs from docstrings. It consists of 164 original programming problems designed to assess language comprehension, algorithms, and simple mathematics, with each problem including a function signature, docstring, body, and unit tests.",
        "num_training_samples": 0,
        "num_validation_samples": 164,
        "huggingface_url": "https://huggingface.co/datasets/openai/openai_humaneval",
        "task_type": ["text-generation"],
        "language_distribution": "Natural Language: English (100%), Programming Language: Python (100%)",
        "dependent_packages": ["datasets"],
        "code": "from datasets import load_dataset\ndataset = load_dataset('openai_humaneval')\ntest_data = dataset['test']\nfor sample in test_data:\n    task_id = sample['task_id']\n    prompt = sample['prompt']\n    canonical_solution = sample['canonical_solution']\n    test = sample['test']\n    entry_point = sample['entry_point']",
        "citation": "@article{chen2021evaluating,\n  title={Evaluating Large Language Models Trained on Code},\n  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and others},\n  journal={arXiv preprint arXiv:2107.03374},\n  year={2021}\n}",
    },
    "MBPP": {
        "description": "MBPP (Mostly Basic Python Problems) is a benchmark dataset consisting of around 1,000 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. It covers programming fundamentals, standard library functionality, and common programming patterns, making it ideal for evaluating code generation capabilities.",
        "num_training_samples": 374,
        "num_validation_samples": 500,
        "huggingface_url": "https://huggingface.co/datasets/google-research-datasets/mbpp",
        "task_type": ["text-generation"],
        "language_distribution": "Natural Language: English (100%), Programming Language: Python (100%)",
        "dependent_packages": ["datasets"],
        "code": "from datasets import load_dataset\ndataset_full = load_dataset('mbpp')\ndataset_sanitized = load_dataset('mbpp', 'sanitized')\ntest_data = dataset_full['test']\nfor sample in test_data:\n    task_id = sample['task_id']\n    text = sample['text']\n    code = sample['code']\n    test_list = sample['test_list']",
        "citation": "@article{austin2021program,\n  title={Program Synthesis with Large Language Models},\n  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and others},\n  journal={arXiv preprint arXiv:2108.07732},\n  year={2021}\n}",
    },
    "GPQA": {
        "description": "GPQA (Graduate-Level Google-Proof Q&A Benchmark) is a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. The questions are designed to be difficult for highly skilled non-expert validators yet solvable by experts, making it suitable for testing advanced reasoning capabilities.",
        "num_training_samples": 0,
        "num_validation_samples": 448,
        "huggingface_url": "https://huggingface.co/datasets/Idavidrein/gpqa",
        "task_type": ["text-generation"],
        "language_distribution": "English only (en)",
        "dependent_packages": ["datasets"],
        "code": "from datasets import load_dataset\ndataset = load_dataset('Idavidrein/gpqa', 'gpqa_main')\nfor split in dataset:\n    print(f'{split}: {len(dataset[split])}')",
        "citation": "@article{rein2023gpqa,\n  title={GPQA: A Graduate-Level Google-Proof Q\\&A Benchmark},\n  author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},\n  journal={arXiv preprint arXiv:2311.12022},\n  year={2023}\n}",
    },
}
