hyperparameter_dict = {
    "learning_rate": "Controls the step size used to update model parameters during optimization. A smaller value ensures stable learning but slows convergence; a larger value accelerates training but risks divergence.",
    "weight_decay": "Coefficient for L2 regularization applied to model weights to prevent overfitting by penalizing large parameter values.",
    "batch_size": "Number of training samples processed before the model parameters are updated. Larger batches improve gradient estimation but require more memory.",
    "num_epochs": "Number of complete passes through the entire training dataset during model training. Determines the total duration of training.",
}
