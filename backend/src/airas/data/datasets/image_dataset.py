IMAGES_DATASETS = {
    "MNIST": {
        "description": "The MNIST database contains 70,000 grayscale images of handwritten digits (0-9). It is a subset of a larger set from NIST, normalized to 28x28 pixels. It is one of the most widely used datasets for training and testing machine learning algorithms for image classification.",
        "num_training_samples": 60000,
        "num_validation_samples": 10000,
        "huggingface_url": "https://huggingface.co/datasets/ylecun/mnist",
        "num_classes": 10,
        "image_size": "28x28 (grayscale)",
        "task_type": ["image-classification"],
        "release_year": 1998,
        "dependent_packages": ["datasets", "PIL", "numpy"],
        "code": "from datasets import load_dataset\n\n# Load MNIST dataset\ndataset = load_dataset(\"ylecun/mnist\")\n\n# Access train and test splits\ntrain_data = dataset['train']\ntest_data = dataset['test']\n\n# Example: Get first image and label\nimage = train_data[0]['image']\nlabel = train_data[0]['label']\nprint(f\"Label: {label}\")\n\n# PyTorch alternative\nimport torchvision\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
        "citation": "@article{lecun1998gradient,\n  title={Gradient-based learning applied to document recognition},\n  author={LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},\n  journal={Proceedings of the IEEE},\n  volume={86},\n  number={11},\n  pages={2278--2324},\n  year={1998},\n  publisher={IEEE}\n}",
    },
    "Fashion-MNIST": {
        "description": "Fashion-MNIST is a dataset of Zalando's article images, consisting of 70,000 grayscale images in 10 categories. It was created as a direct drop-in replacement for MNIST with the same image size and structure, but provides a more challenging classification task.",
        "num_training_samples": 60000,
        "num_validation_samples": 10000,
        "huggingface_url": "https://huggingface.co/datasets/zalando-datasets/fashion_mnist",
        "num_classes": 10,
        "image_size": "28x28 (grayscale)",
        "task_type": ["image-classification"],
        "release_year": 2017,
        "dependent_packages": ["datasets", "PIL", "numpy"],
        "code": "from datasets import load_dataset\n\n# Load Fashion-MNIST dataset\ndataset = load_dataset(\"zalando-datasets/fashion_mnist\")\n\n# Access splits\ntrain_data = dataset['train']\ntest_data = dataset['test']\n\n# Class labels: 0=T-shirt/top, 1=Trouser, 2=Pullover, 3=Dress, 4=Coat,\n#               5=Sandal, 6=Shirt, 7=Sneaker, 8=Bag, 9=Ankle boot\n\n# PyTorch alternative\nimport torchvision\ntrain_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True)\n",
        "citation": "@article{xiao2017fashion,\n  title={Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},\n  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},\n  journal={arXiv preprint arXiv:1708.07747},\n  year={2017}\n}",
    },
    "CIFAR-100": {
        "description": "CIFAR-100 is a dataset containing 60,000 32x32 color images in 100 classes, with 600 images per class. The 100 classes are grouped into 20 superclasses. Each image comes with a fine label (the class) and a coarse label (the superclass).",
        "num_training_samples": 50000,
        "num_validation_samples": 10000,
        "huggingface_url": "https://huggingface.co/datasets/uoft-cs/cifar100",
        "num_classes": "100 (+ 20 superclasses)",
        "image_size": "32x32 (RGB)",
        "task_type": ["image-classification"],
        "release_year": 2009,
        "dependent_packages": ["datasets", "PIL", "numpy"],
        "code": "from datasets import load_dataset\n\n# Load CIFAR-100 dataset\ndataset = load_dataset(\"uoft-cs/cifar100\")\n\n# Access splits\ntrain_data = dataset['train']\ntest_data = dataset['test']\n\n# Each sample has 'fine_label' (0-99) and 'coarse_label' (0-19)\nimage = train_data[0]['img']\nfine_label = train_data[0]['fine_label']\ncoarse_label = train_data[0]['coarse_label']\n\n# PyTorch alternative\nimport torchvision\ntrain_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True)\n",
        "citation": "@techreport{krizhevsky2009learning,\n  title={Learning multiple layers of features from tiny images},\n  author={Krizhevsky, Alex and Hinton, Geoffrey},\n  institution={University of Toronto},\n  year={2009}\n}",
    },
    "COCO": {
        "description": "Microsoft COCO (Common Objects in Context) is a large-scale object detection, segmentation, and captioning dataset. It contains 330K images with over 200K labeled images. Images contain complex everyday scenes with common objects in their natural context.",
        "num_training_samples": 118287,
        "num_validation_samples": 5000,
        "huggingface_url": "https://huggingface.co/datasets/detection-datasets/coco",
        "num_classes": 80,
        "image_size": "variable",
        "task_type": ["object-detection", "instance-segmentation", "image-captioning"],
        "release_year": 2014,
        "dependent_packages": ["datasets", "pycocotools", "PIL"],
        "code": "from datasets import load_dataset\n\n# Load COCO 2017 dataset\ndataset = load_dataset(\"detection-datasets/coco\")\n\n# Access splits\ntrain_data = dataset['train']\nval_data = dataset['validation']\n\n# Each sample contains image, objects (bboxes, labels, segmentation masks)\nsample = train_data[0]\nimage = sample['image']\nobjects = sample['objects']\nbboxes = objects['bbox']\ncategory_ids = objects['category_id']\n\n# Alternative: using official COCO API\nfrom pycocotools.coco import COCO\ncoco = COCO('path/to/annotations/instances_train2017.json')\n",
        "citation": "@inproceedings{lin2014microsoft,\n  title={Microsoft COCO: Common objects in context},\n  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C Lawrence},\n  booktitle={European Conference on Computer Vision},\n  pages={740--755},\n  year={2014},\n  organization={Springer}\n}",
    },
    "Pascal-VOC": {
        "description": "The PASCAL Visual Object Classes (VOC) dataset is a well-known benchmark for object detection, semantic segmentation, and classification. It contains 20 object categories with detailed annotations including bounding boxes, class labels, and segmentation masks.",
        "num_training_samples": 16551,
        "num_validation_samples": 4952,
        "huggingface_url": "https://huggingface.co/datasets/merve/pascal-voc",
        "num_classes": 20,
        "image_size": "variable",
        "task_type": [
            "object-detection",
            "semantic-segmentation",
            "image-classification",
        ],
        "release_year": 2012,
        "dependent_packages": ["datasets", "PIL", "xml"],
        "code": "from datasets import load_dataset\n\n# Load Pascal VOC dataset\ndataset = load_dataset(\"merve/pascal-voc\", \"2012\")\n\n# Access data\ntrain_data = dataset['train']\n\n# Each sample contains image and objects with bounding boxes\nsample = train_data[0]\nimage = sample['image']\nobjects = sample['objects']\nbboxes = objects['bbox']\nlabels = objects['label']\n\n# PyTorch alternative\nimport torchvision\ntrain_dataset = torchvision.datasets.VOCDetection(root='./data', year='2012', \n                                                   image_set='train', download=True)\n",
        "citation": "@article{everingham2010pascal,\n  title={The Pascal Visual Object Classes (VOC) challenge},\n  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},\n  journal={International Journal of Computer Vision},\n  volume={88},\n  number={2},\n  pages={303--338},\n  year={2010}\n}",
    },
    "SVHN": {
        "description": "The Street View House Numbers (SVHN) dataset is obtained from house numbers in Google Street View images. It contains over 600,000 digit images and is similar to MNIST but incorporates the challenging problem of recognizing digits in natural scene images.",
        "num_training_samples": 73257,
        "num_validation_samples": 26032,
        "huggingface_url": "https://huggingface.co/datasets/ufldl-stanford/svhn",
        "num_classes": 10,
        "image_size": "32x32 (RGB)",
        "task_type": ["image-classification"],
        "release_year": 2011,
        "dependent_packages": ["datasets", "PIL", "numpy"],
        "code": "from datasets import load_dataset\n\n# Load SVHN dataset (cropped digits format)\ndataset = load_dataset(\"ufldl-stanford/svhn\", \"cropped_digits\")\n\n# Access splits\ntrain_data = dataset['train']\ntest_data = dataset['test']\nextra_data = dataset['extra']  # Additional 531,131 images\n\n# Each sample contains RGB image and digit label (0-9)\nimage = train_data[0]['image']\nlabel = train_data[0]['label']\n\n# PyTorch alternative\nimport torchvision\ntrain_dataset = torchvision.datasets.SVHN(root='./data', split='train', download=True)\n",
        "citation": "@inproceedings{netzer2011reading,\n  title={Reading digits in natural images with unsupervised feature learning},\n  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},\n  booktitle={NIPS Workshop on Deep Learning and Unsupervised Feature Learning},\n  year={2011}\n}",
    },
    "Food-101": {
        "description": "Food-101 is a challenging dataset of 101,000 food images organized into 101 food categories. Each category contains 1,000 images. The training images were not cleaned and thus contain some noise. The test images are manually reviewed and cleaned.",
        "num_training_samples": 75750,
        "num_validation_samples": 25250,
        "huggingface_url": "https://huggingface.co/datasets/ethz/food101",
        "num_classes": 101,
        "image_size": "512px (max side)",
        "task_type": ["image-classification"],
        "release_year": 2014,
        "dependent_packages": ["datasets", "PIL"],
        "code": "from datasets import load_dataset\n\n# Load Food-101 dataset\ndataset = load_dataset(\"ethz/food101\")\n\n# Access splits\ntrain_data = dataset['train']\ntest_data = dataset['validation']  # Note: 'validation' is actually the test set\n\n# Each sample contains image and label (0-100)\nimage = train_data[0]['image']\nlabel = train_data[0]['label']\n\n# TensorFlow alternative\nimport tensorflow_datasets as tfds\ndataset = tfds.load('food101', split='train')\n",
        "citation": "@inproceedings{bossard2014food,\n  title={Food-101 -- Mining Discriminative Components with Random Forests},\n  author={Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},\n  booktitle={European Conference on Computer Vision},\n  year={2014}\n}",
    },
    "Oxford-Flowers-102": {
        "description": "Oxford Flowers 102 is a fine-grained classification dataset consisting of 102 flower categories commonly found in the UK. Each class contains between 40 and 258 images with large scale, pose and light variations.",
        "num_training_samples": 1020,
        "num_validation_samples": 1020,
        "huggingface_url": "https://huggingface.co/datasets/Voxel51/OxfordFlowers102",
        "num_classes": 102,
        "image_size": "variable",
        "task_type": ["image-classification"],
        "release_year": 2008,
        "dependent_packages": ["datasets", "PIL"],
        "code": "from datasets import load_dataset\n\n# Load Oxford Flowers 102 dataset\ndataset = load_dataset(\"Voxel51/OxfordFlowers102\")\n\n# Access splits\ntrain_data = dataset['train']\nval_data = dataset['validation']\ntest_data = dataset['test']\n\n# Each sample contains image and label (0-101)\nimage = train_data[0]['image']\nlabel = train_data[0]['label']\n\n# PyTorch alternative\nimport torchvision\ntrain_dataset = torchvision.datasets.Flowers102(root='./data', split='train', download=True)\n",
        "citation": "@InProceedings{Nilsback08,\n  author = {Nilsback, M-E. and Zisserman, A.},\n  title = {Automated Flower Classification over a Large Number of Classes},\n  booktitle = {Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing},\n  year = {2008}\n}",
    },
    "STL-10": {
        "description": "An image recognition dataset inspired by CIFAR-10 but designed specifically for developing unsupervised feature learning, deep learning, and self-taught learning algorithms. Each class has fewer labeled training examples than CIFAR-10, but includes 100,000 unlabeled images for unsupervised pre-training.",
        "num_training_samples": 5000,
        "num_validation_samples": 8000,
        "huggingface_url": "https://huggingface.co/datasets/tanganke/stl10",
        "num_classes": 10,
        "image_size": "96x96 (RGB)",
        "task_type": ["image-classification"],
        "release_year": 2011,
        "dependent_packages": ["datasets", "PIL", "numpy", "torch"],
        "code": "from datasets import load_dataset\n\n# Load the STL-10 dataset\ndataset = load_dataset('tanganke/stl10')\n\n# Access training and test splits\ntrain_data = dataset['train']\ntest_data = dataset['test']\n\n# Example: Access first training image\nsample = train_data[0]\nimage = sample['image']  # PIL Image\nlabel = sample['label']  # Integer label (0-9)",
        "citation": "@techreport{Coates2011,\n  author = {Adam Coates and Honglak Lee and Andrew Y. Ng},\n  title = {An Analysis of Single Layer Networks in Unsupervised Feature Learning},\n  institution = {Stanford University},\n  year = {2011}\n}",
    },
    "Caltech-101": {
        "description": "Caltech-101 contains pictures of objects belonging to 101 categories plus a background category. Each category contains roughly 40 to 800 images. The dataset was collected to test object recognition methods and computer vision techniques.",
        "num_training_samples": 8677,
        "num_validation_samples": 0,
        "huggingface_url": "https://huggingface.co/datasets/flwrlabs/caltech101",
        "num_classes": 101,
        "image_size": "variable (approx. 300x200)",
        "task_type": ["object-detection"],
        "release_year": 2006,
        "dependent_packages": ["datasets", "PIL"],
        "code": "from datasets import load_dataset\n\n# Load Caltech-101 dataset\ndataset = load_dataset(\"flwrlabs/caltech101\")\n\n# Access data (no official train/test split)\ndata = dataset['train']\n\n# Each sample contains image and label (0-100)\nimage = data[0]['image']\nlabel = data[0]['label']\n\n# Common practice: split manually\nfrom sklearn.model_selection import train_test_split\n# Use 30 images per class for training, rest for testing\n\n# PyTorch alternative\nimport torchvision\ndataset = torchvision.datasets.Caltech101(root='./data', download=True)\n",
        "citation": "@article{fei2006one,\n  title={One-shot learning of object categories},\n  author={Fei-Fei, Li and Fergus, Rob and Perona, Pietro},\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  volume={28},\n  number={4},\n  pages={594--611},\n  year={2006},\n  publisher={IEEE}\n}",
    },
    # "CelebA": {
    #     "description": "Large-scale face attributes dataset with 202,599 celebrity images, each annotated with 40 binary attributes (e.g., hair color, glasses, smiling), 5 landmark locations, and bounding boxes. Contains high diversity in pose, lighting, and background.",
    #     "num_training_samples": 162770,
    #     "num_validation_samples": 19867,
    #     "huggingface_url": "https://huggingface.co/datasets/flwrlabs/celeba",
    #     "num_classes": "40 binary attributes + 10,177 identities",
    #     "image_size": "178x218 (original), commonly resized to 64x64, 128x128, or 224x224",
    #     "task_type": ["face-attribute-recognition", "face-detection", "landmark-localization", "face-identification"],
    #     "release_year": 2015,
    #     "dependent_packages": [
    #         "torch",
    #         "torchvision",
    #         "datasets",
    #         "pillow",
    #         "numpy",
    #     ],
    #     "code": "from datasets import load_dataset\n\n# Load CelebA dataset\ndataset = load_dataset(\"flwrlabs/celeba\")\n\ntrain_data = dataset['train']\nvalid_data = dataset['valid']\ntest_data = dataset['test']\n\n# Access sample\nsample = train_data[0]\nimage = sample['image']\nis_smiling = sample['Smiling']\nis_male = sample['Male']\n\n# PyTorch alternative\nimport torchvision\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize(64),\n    transforms.CenterCrop(64),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\nceleba_train = torchvision.datasets.CelebA(\n    root='./data',\n    split='train',\n    target_type='attr',\n    transform=transform,\n    download=True\n)\n",
    #     "citation": "@inproceedings{liu2015faceattributes,\n  title={Deep Learning Face Attributes in the Wild},\n  author={Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},\n  booktitle={Proceedings of International Conference on Computer Vision (ICCV)},\n  month={December},\n  year={2015}\n}",
    # },
    # "Places365": {
    #     "description": "Large-scale scene recognition dataset with 365 scene categories. Places365-Standard contains ~1.8M training images, Places365-Challenge contains ~8M training images. Designed based on human visual cognition principles.",
    #     "num_training_samples": 1803460,
    #     "num_validation_samples": 36500,
    #     "huggingface_url": "https://huggingface.co/datasets/Andron00e/Places365-custom",
    #     "num_classes": 365,
    #     "image_size": "256x256 (small version), variable (original)",
    #     "task_type": ["scene-recognition", "scene-classification"],
    #     "release_year": 2017,
    #     "dependent_packages": ["torch", "torchvision", "pillow", "numpy"],
    #     "code": "import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.Places365(\n    root='./data',\n    split='train-standard',\n    small=True,\n    download=True,\n    transform=transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n\n# TensorFlow alternative\nimport tensorflow_datasets as tfds\ndataset, info = tfds.load('places365_small', split='train', with_info=True, as_supervised=True)\n",
    #     "citation": "@article{zhou2017places,\n  title={Places: A 10 Million Image Database for Scene Recognition},\n  author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  year={2017},\n  publisher={IEEE}\n}",
    # },
    "ADE20K": {
        "description": "Large-scale dataset for semantic segmentation and scene parsing. Contains 27,574 images with comprehensive pixel-level annotations covering 3,688 object categories. MIT Scene Parsing Benchmark uses 150-class subset.",
        "num_training_samples": 20210,
        "num_validation_samples": 2000,
        "huggingface_url": "https://huggingface.co/datasets/scene_parse_150",
        "num_classes": "150 (benchmark) / 3,688 (full)",
        "image_size": "variable, typically 512x683 pixels",
        "task_type": ["semantic-segmentation", "instance-segmentation"],
        "release_year": 2017,
        "dependent_packages": [
            "datasets",
            "transformers",
            "pillow",
            "numpy",
            "torch",
        ],
        "code": "from datasets import load_dataset\n\n# Load ADE20K (150-class benchmark)\ndataset = load_dataset(\"scene_parse_150\")\n\ntrain_dataset = dataset['train']\nval_dataset = dataset['validation']\n\nsample = train_dataset[0]\nimage = sample['image']\nannotation = sample['annotation']\nscene_category = sample['scene_category']\n\n# Using with transformers for semantic segmentation\nfrom transformers import BeitFeatureExtractor, BeitForSemanticSegmentation\n\nfeature_extractor = BeitFeatureExtractor.from_pretrained(\n    'microsoft/beit-base-finetuned-ade-640-640'\n)\nmodel = BeitForSemanticSegmentation.from_pretrained(\n    'microsoft/beit-base-finetuned-ade-640-640'\n)\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n",
        "citation": "@inproceedings{zhou2017scene,\n  title={Scene Parsing through ADE20K Dataset},\n  author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2017}\n}",
    },
    "Cityscapes": {
        "description": "Large-scale benchmark for urban street scene understanding. Contains diverse stereo video sequences from 50 different cities with high-quality pixel-level annotations for semantic, instance, and panoptic segmentation.",
        "num_training_samples": 2975,
        "num_validation_samples": 500,
        "huggingface_url": "https://huggingface.co/datasets/Chris1/cityscapes",
        "num_classes": "30 classes (19 evaluation classes)",
        "image_size": "1024x2048 pixels",
        "task_type": [
            "semantic-segmentation",
            "instance-segmentation",
            "panoptic-segmentation",
        ],
        "release_year": 2016,
        "dependent_packages": [
            "cityscapesscripts",
            "pillow",
            "numpy",
            "torch",
            "torchvision",
        ],
        "code": "from torchvision.datasets import Cityscapes\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndataset = Cityscapes(\n    root='./data/cityscapes',\n    split='train',\n    mode='fine',\n    target_type='semantic',\n    transform=transform\n)\n\ntrain_loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\n\n# For multiple target types\ndataset_multi = Cityscapes(\n    root='./data/cityscapes',\n    split='train',\n    mode='fine',\n    target_type=['instance', 'semantic', 'color']\n)\n",
        "citation": "@inproceedings{Cordts2016Cityscapes,\n  title={The Cityscapes Dataset for Semantic Urban Scene Understanding},\n  author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},\n  booktitle={Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2016}\n}",
    },
    "Oxford-IIIT-Pet": {
        "description": "Dataset of 37 pet breeds (cats and dogs) with ~7,349 images. Each image has breed annotation, head bounding box, and pixel-level foreground-background segmentation (trimap). Ideal for fine-grained classification.",
        "num_training_samples": 3680,
        "num_validation_samples": 3669,
        "huggingface_url": "https://huggingface.co/datasets/timm/oxford-iiit-pet",
        "num_classes": 37,
        "image_size": "variable, typically resized to 224x224 or 256x256",
        "task_type": ["image-classification", "semantic-segmentation"],
        "release_year": 2012,
        "dependent_packages": [
            "torch",
            "torchvision",
            "pillow",
            "datasets",
            "numpy",
        ],
        "code": "from datasets import load_dataset\n\n# Hugging Face Datasets\ndataset = load_dataset(\"timm/oxford-iiit-pet\")\n\ntrain_data = dataset['train']\ntest_data = dataset['test']\n\nexample = train_data[0]\nimage = example['image']\nlabel = example['label']\n\n# PyTorch alternative\nimport torchvision\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = torchvision.datasets.OxfordIIITPet(\n    root='./data',\n    split='trainval',\n    target_types='category',\n    transform=transform,\n    download=True\n)\n",
        "citation": '@InProceedings{parkhi12a,\n  author    = "Parkhi, O. M. and Vedaldi, A. and Zisserman, A. and Jawahar, C.~V.",\n  title     = "Cats and Dogs",\n  booktitle = "IEEE Conference on Computer Vision and Pattern Recognition",\n  year      = "2012",\n}',
    },
    "Stanford-Dogs": {
        "description": "The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization. Designed for fine-grained visual recognition with minimal inter-class variation between similar dog breeds.",
        "num_training_samples": 12000,
        "num_validation_samples": 8580,
        "huggingface_url": "https://huggingface.co/datasets/Alanox/stanford-dogs",
        "num_classes": 120,
        "image_size": "variable (RGB)",
        "task_type": ["image-classification"],
        "release_year": 2011,
        "dependent_packages": ["datasets", "PIL", "Python"],
        "code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"Alanox/stanford-dogs\", split=\"full\")\n\n# Access a sample\nsample = dataset[0]\nimage = sample['image']\nlabel = sample['target']\nannotations = sample['annotations']  # Bounding boxes",
        "citation": "@inproceedings{KhoslaYaoJayadevaprakashFeiFei_FGVC2011,\n    author = {Aditya Khosla and Nityananda Jayadevaprakash and Bangpeng Yao and Li Fei-Fei},\n    title = {Novel Dataset for Fine-Grained Image Categorization},\n    booktitle = {FGVC Workshop, CVPR},\n    year = {2011}\n}",
    },
    "CUB-200-2011": {
        "description": "Caltech-UCSD Birds-200-2011 dataset with 11,788 images of 200 bird species. Each image has 1 subcategory label, 15 part locations, 312 binary attributes, and 1 bounding box. Fine-grained visual classification benchmark.",
        "num_training_samples": 5994,
        "num_validation_samples": 5794,
        "huggingface_url": "https://huggingface.co/datasets/bentrevett/caltech-ucsd-birds-200-2011",
        "num_classes": 200,
        "image_size": "variable, typically resized to 224x224, 299x299, or 448x448",
        "task_type": ["image-classification"],
        "release_year": 2011,
        "dependent_packages": [
            "torch",
            "torchvision",
            "pillow",
            "pandas",
            "datasets",
        ],
        "code": "from datasets import load_dataset\n\n# Hugging Face Datasets\ndataset = load_dataset(\"bentrevett/caltech-ucsd-birds-200-2011\")\n\ntrain_data = dataset[\"train\"]\ntest_data = dataset[\"test\"]\n\nexample = train_data[0]\nimage = example[\"image\"]\nlabel = example[\"label\"]\nbbox = example[\"bbox\"]\n\n# Custom PyTorch Dataset\nimport os\nimport pandas as pd\nfrom torchvision.datasets import VisionDataset\nfrom torchvision import transforms\n\nclass Cub2011(VisionDataset):\n    def __init__(self, root, train=True, transform=None):\n        super().__init__(root, transform=transform)\n        self.train = train\n        self._load_metadata()\n    \n    def _load_metadata(self):\n        images = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'images.txt'), \n                            sep=' ', names=['img_id', 'filepath'])\n        image_class_labels = pd.read_csv(\n            os.path.join(self.root, 'CUB_200_2011', 'image_class_labels.txt'),\n            sep=' ', names=['img_id', 'target'])\n        train_test_split = pd.read_csv(\n            os.path.join(self.root, 'CUB_200_2011', 'train_test_split.txt'),\n            sep=' ', names=['img_id', 'is_training_img'])\n        \n        data = images.merge(image_class_labels, on='img_id')\n        self.data = data.merge(train_test_split, on='img_id')\n        \n        if self.train:\n            self.data = self.data[self.data.is_training_img == 1]\n        else:\n            self.data = self.data[self.data.is_training_img == 0]\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        sample = self.data.iloc[idx]\n        path = os.path.join(self.root, 'CUB_200_2011/images', sample.filepath)\n        target = sample.target - 1\n        image = Image.open(path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        return image, target\n",
        "citation": "@techreport{WahCUB_200_2011,\n    Title = {{The Caltech-UCSD Birds-200-2011 Dataset}},\n    Author = {Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.},\n    Year = {2011},\n    Institution = {California Institute of Technology},\n    Number = {CNS-TR-2011-001}\n}",
    },
    "Caltech-256": {
        "description": "A challenging set of 256 object categories containing a total of 30,607 images. An improvement over Caltech-101 with more than doubled number of categories, increased minimum images per category, and includes a larger clutter category for testing background rejection.",
        "num_training_samples": 30607,
        "num_validation_samples": 0,
        "huggingface_url": "https://huggingface.co/datasets/ilee0022/Caltech-256",
        "num_classes": 257,
        "image_size": "variable",
        "task_type": ["image-classification"],
        "release_year": 2007,
        "dependent_packages": ["datasets", "PIL", "Python"],
        "code": 'from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset("ilee0022/Caltech-256")\n\n# Access an example\nexample = dataset["train"][0]\nimage = example["image"]\nlabel = example["label"]\nclass_name = example["text"]',
        "citation": "@misc{griffin_holub_perona_2022,\n  title={Caltech 256},\n  DOI={10.22002/D1.20087},\n  publisher={CaltechDATA},\n  author={Griffin, Gregory and Holub, Alex and Perona, Pietro},\n  year={2022}\n}",
    },
    # "KITTI": {
    #     "description": "Comprehensive real-world dataset for autonomous driving research. Captured using high-resolution stereo cameras, Velodyne 3D laser scanner, and high-precision GPS/IMU. Contains multiple benchmark tasks including 3D object detection, tracking, and depth estimation.",
    #     "num_training_samples": 7481,
    #     "num_validation_samples": 7518,
    #     "huggingface_url": "https://huggingface.co/datasets/nateraw/kitti",
    #     "num_classes": "8 (Car, Van, Truck, Pedestrian, Person_sitting, Cyclist, Tram, Misc)",
    #     "image_size": "1242x375 pixels (rectified)",
    #     "task_type": "3D Object Detection, 2D Object Detection, Stereo Vision, Optical Flow, Visual Odometry",
    #     "release_year": 2012,
    #     "dependent_packages": [
    #         "numpy",
    #         "opencv-python",
    #         "pillow",
    #         "torch",
    #         "datasets",
    #         "pykitti",
    #     ],
    #     "code": "from datasets import load_dataset\n\n# Hugging Face Datasets\ndataset = load_dataset(\"nateraw/kitti\")\n\ntrain_data = dataset['train']\n\nsample = train_data[0]\nimage = sample['image']\nlabels = sample['label']\n\n# PyTorch/TorchVision\nfrom torchvision.datasets import Kitti\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nkitti_train = Kitti(\n    root='./data',\n    train=True,\n    transform=transform,\n    download=True\n)\n\n# Using pykitti for raw data\nimport pykitti\n\nbasedir = '/path/to/kitti/dataset'\ndate = '2011_09_26'\ndrive = '0001'\n\ndata = pykitti.raw(basedir, date, drive)\n\nfor img in data.cam2:  # Left color camera\n    # Process image\n    pass\n\nfor velo in data.velo:  # Velodyne point cloud\n    points = velo[:, :3]  # [x, y, z, reflectance]\n",
    #     "citation": "@inproceedings{Geiger2012CVPR,\n  author = {Andreas Geiger and Philip Lenz and Raquel Urtasun},\n  title = {Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},\n  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year = {2012}\n}",
    # },
    "Tiny-ImageNet": {
        "description": "Subset of ImageNet LSVRC created for Stanford CS231n. Contains 200 classes with 64x64 color images. Each class has 500 training images, 50 validation images, and 50 test images. Designed for educational purposes and rapid prototyping.",
        "num_training_samples": 100000,
        "num_validation_samples": 10000,
        "huggingface_url": "https://huggingface.co/datasets/zh-plus/tiny-imagenet",
        "num_classes": 200,
        "image_size": "64x64 pixels (RGB)",
        "task_type": ["image-classification"],
        "release_year": 2015,
        "dependent_packages": [
            "datasets",
            "pillow",
            "torch",
            "torchvision",
            "numpy",
        ],
        "code": "from datasets import load_dataset\n\n# Hugging Face Datasets\ntiny_imagenet = load_dataset('zh-plus/tiny-imagenet', split='train')\n\nprint(tiny_imagenet[0])\n# Output: {'image': <PIL.JpegImagePlugin.JpegImageFile>, 'label': 15}\n\ntrain_data = load_dataset('zh-plus/tiny-imagenet', split='train')\nvalid_data = load_dataset('zh-plus/tiny-imagenet', split='valid')\n\n# PyTorch DataLoader integration\nfrom torch.utils.data import DataLoader\n\ntiny_imagenet.set_format(type='torch', columns=['image', 'label'])\ntrain_loader = DataLoader(tiny_imagenet, batch_size=32, shuffle=True)\n\n# Traditional PyTorch ImageFolder (for local data)\nfrom torchvision import datasets, transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.ImageFolder(\n    root='path/to/tiny-imagenet-200/train',\n    transform=transform\n)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, \n    batch_size=100, \n    shuffle=True, \n    num_workers=4\n)\n",
        "citation": "@article{le2015tiny,\n  title={Tiny ImageNet Visual Recognition Challenge},\n  author={Le, Ya and Yang, Xuan S.},\n  journal={CS 231N},\n  volume={7},\n  number={7},\n  pages={3},\n  year={2015}\n}",
    },
    # "SUN397": {
    #     "description": "Scene UNderstanding dataset for scene recognition. Contains 397 scene categories with 108,754 images covering scenes from abbey to zoo. Each category has at least 100 images. Includes various indoor and outdoor environments.",
    #     "num_training_samples": 76128,
    #     "num_validation_samples": 10875,
    #     "huggingface_url": "https://huggingface.co/datasets/tanganke/sun397",
    #     "num_classes": 397,
    #     "image_size": "variable",
    #     "task_type": "Scene Recognition/Classification",
    #     "release_year": 2010,
    #     "dependent_packages": ["datasets", "torchvision", "torch", "PIL"],
    #     "code": "from datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load SUN397 dataset\ndataset = load_dataset('tanganke/sun397')\n\ntrain_data = dataset['train']\nval_data = dataset['validation']\ntest_data = dataset['test']\n\n# 397 scene categories including:\n# indoor scenes (abbey, airport_terminal, art_gallery, bedroom, etc.)\n# outdoor scenes (beach, bridge, canyon, desert, forest, etc.)\n\n# Visualize samples\nfig, axes = plt.subplots(4, 4, figsize=(16, 16))\nfor i, ax in enumerate(axes.flat):\n    idx = np.random.randint(len(train_data))\n    sample = train_data[idx]\n    ax.imshow(sample['image'])\n    ax.set_title(f\"Scene {sample['label']}\", fontsize=8)\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n\n# PyTorch alternative\nimport torchvision\nsun397 = torchvision.datasets.SUN397(\n    root='./data',\n    download=True\n)\n",
    #     "citation": "@inproceedings{xiao2010sun,\n  title={SUN database: Large-scale scene recognition from abbey to zoo},\n  author={Xiao, Jianxiong and Hays, James and Ehinger, Krista A and Oliva, Aude and Torralba, Antonio},\n  booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n  pages={3485--3492},\n  year={2010}\n}",
    # },
    # "DTD": {
    #     "description": "Describable Textures Dataset. A collection of texture images in the wild, annotated with human-centric attributes inspired by perceptual properties of textures. Contains 5,640 images organized into 47 categories (banded, dotted, woven, bubbly, etc.).",
    #     "num_training_samples": 1880,
    #     "num_validation_samples": 1880,
    #     "huggingface_url": "https://huggingface.co/datasets/tanganke/dtd",
    #     "num_classes": 47,
    #     "image_size": "300x300 to 640x640 (variable)",
    #     "task_type": "Texture Recognition/Classification",
    #     "release_year": 2014,
    #     "dependent_packages": ["datasets", "torch", "torchvision", "PIL"],
    #     "code": "from datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load DTD dataset\ndataset = load_dataset('tanganke/dtd')\n\ntrain_data = dataset['train']\nval_data = dataset['validation']\ntest_data = dataset['test']\n\n# 47 texture categories: banded, blotchy, braided, bubbly,\n# bumpy, chequered, cobwebbed, cracked, crosshatched, crystalline,\n# dotted, fibrous, flecked, freckled, frilly, gauzy, grid,\n# grooved, honeycombed, interlaced, knitted, lacelike, lined,\n# marbled, matted, meshed, paisley, perforated, pitted, pleated,\n# polka-dotted, porous, potholed, scaly, smeared, spiralled,\n# sprinkled, stained, stratified, striped, studded, swirly,\n# veined, waffled, woven, wrinkled, zigzagged\n\n# Visualize\nfig, axes = plt.subplots(3, 3, figsize=(12, 12))\nfor i, ax in enumerate(axes.flat):\n    idx = np.random.randint(len(train_data))\n    sample = train_data[idx]\n    ax.imshow(sample['image'])\n    ax.set_title(f\"Label: {sample['label']}\")\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n",
    #     "citation": "@inproceedings{cimpoi14describing,\n  title={Describing Textures in the Wild},\n  author={M. Cimpoi and S. Maji and I. Kokkinos and S. Mohamed and A. Vedaldi},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2014}\n}",
    # },
    # "Stanford-Cars": {
    #     "description": "Fine-grained classification dataset with 16,185 images of 196 car classes organized by Make, Model, and Year (e.g., '2012 Tesla Model S'). Each image includes bounding box annotations.",
    #     "num_training_samples": 8144,
    #     "num_validation_samples": 8041,
    #     "huggingface_url": "https://huggingface.co/datasets/tanganke/stanford_cars",
    #     "num_classes": 196,
    #     "image_size": "variable (typically around 360x240)",
    #     "task_type": "Fine-grained Vehicle Recognition, Image Classification",
    #     "release_year": 2013,
    #     "dependent_packages": [
    #         "torch",
    #         "torchvision",
    #         "PIL",
    #         "numpy",
    #         "scipy",
    #         "datasets",
    #     ],
    #     "code": "# Using PyTorch/torchvision\nimport torchvision\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndataset = torchvision.datasets.StanfordCars(\n    root='./data',\n    split='train',\n    transform=transform,\n    download=True\n)\n\n# Hugging Face\nfrom datasets import load_dataset\ndataset = load_dataset('tanganke/stanford_cars')\n",
    #     "citation": "@InProceedings{Krause_2013_ICCV_Workshops,\n  author = {Krause, Jonathan and Stark, Michael and Deng, Jia and Fei-Fei, Li},\n  title = {3D Object Representations for Fine-Grained Categorization},\n  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops},\n  year = {2013}\n}",
    # },
    "FGVC-Aircraft": {
        "description": "A benchmark dataset for fine-grained visual categorization of aircraft. Contains images of aircraft with tight bounding boxes and hierarchical labels organized in four levels: Model, Variant (102 classes), Family (70 classes), and Manufacturer (41 classes).",
        "num_training_samples": 3400,
        "num_validation_samples": 6800,
        "huggingface_url": "https://huggingface.co/datasets/HuggingFaceM4/FGVC-Aircraft",
        "num_classes": 102,
        "image_size": "variable",
        "task_type": ["image-classification"],
        "release_year": 2013,
        "dependent_packages": ["datasets", "matplotlib", "PIL", "numpy"],
        "code": 'from datasets import load_dataset\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n# Load dataset\nds = load_dataset("HuggingFaceM4/FGVC-Aircraft")\n\n# Access training data\ntrain_data = ds["train"]\nsample = train_data[0]\n\n# Extract image and bounding box\nimage = sample["image"]\nbbox = sample["bbox"]',
        "citation": "@techreport{maji13fine-grained,\n  title = {Fine-Grained Visual Classification of Aircraft},\n  author = {S. Maji and J. Kannala and E. Rahtu and M. Blaschko and A. Vedaldi},\n  year = {2013},\n  institution = {arXiv},\n  number = {arXiv:1306.5151}\n}",
    },
    # "FFHQ": {
    #     "description": "A high-quality image dataset of human faces, originally created as a benchmark for generative adversarial networks (GAN). The dataset consists of 70,000 high-quality PNG images containing considerable variation in terms of age, ethnicity, and image background.",
    #     "num_training_samples": 60000,
    #     "num_validation_samples": 10000,
    #     "huggingface_url": "https://huggingface.co/datasets/marcosv/ffhq-dataset",
    #     "num_classes": 0,
    #     "image_size": "1024x1024 (PNG)",
    #     "task_type": "Face Generation",
    #     "release_year": 2019,
    #     "dependent_packages": ["datasets", "PIL", "torch", "dlib"],
    #     "code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"marcosv/ffhq-dataset\")\n\n# Access images\nimage = dataset['train'][0]['image']\nlabel = dataset['train'][0]['label']\n\n# Display image\nimage.show()",
    #     "citation": "@article{Karras2019stylegan,\n  title={A Style-Based Generator Architecture for Generative Adversarial Networks},\n  author={Karras, Tero and Laine, Samuli and Aila, Timo},\n  journal={IEEE TPAMI},\n  volume={43},\n  number={12},\n  pages={4217-4228},\n  year={2019}\n}",
    # },
    # "ImageNet-Sketch": {
    #     "description": "Out-of-domain evaluation dataset with 50,889 black and white sketch images (~50 per ImageNet class). Designed to test model robustness and domain transfer from photographic images to sketches.",
    #     "num_training_samples": 0,
    #     "num_validation_samples": 50889,
    #     "huggingface_url": "https://huggingface.co/datasets/songweig/imagenet_sketch",
    #     "num_classes": 1000,
    #     "image_size": "variable",
    #     "task_type": "Out-of-domain Classification, Domain Transfer",
    #     "release_year": 2019,
    #     "dependent_packages": [
    #         "datasets",
    #         "PIL",
    #         "torch",
    #         "torchvision",
    #         "numpy",
    #         "tensorflow-datasets",
    #     ],
    #     "code": "# Using Hugging Face\nfrom datasets import load_dataset\n\ndataset = load_dataset('songweig/imagenet_sketch')\n\nsample = dataset['test'][0]\nimage = sample['image']\nlabel = sample['label']\n\n# TensorFlow Datasets\nimport tensorflow_datasets as tfds\nds = tfds.load('imagenet_sketch', split='test')\n\nfor example in ds.take(1):\n    image = example['image']\n    label = example['label']\n",
    #     "citation": "@inproceedings{wang2019learning,\n  title={Learning Robust Global Representations by Penalizing Local Predictive Power},\n  author={Wang, Haohan and Ge, Songwei and Lipton, Zachary and Xing, Eric P},\n  booktitle={Advances in Neural Information Processing Systems},\n  pages={10506--10518},\n  year={2019}\n}",
    # },
    # "EMNIST": {
    #     "description": "Extended MNIST. A dataset of handwritten characters and digits derived from the NIST Special Database 19. Extends MNIST to include handwritten alphabet characters in addition to digits, including both uppercase and lowercase letters.",
    #     "num_training_samples": 697932,
    #     "num_validation_samples": 116323,
    #     "huggingface_url": "https://huggingface.co/datasets/randall-lab/emnist",
    #     "num_classes": "10-62 (depending on split)",
    #     "image_size": "28x28 (grayscale)",
    #     "task_type": "Handwritten Character Recognition",
    #     "release_year": 2017,
    #     "dependent_packages": ["datasets", "PIL", "numpy", "torch", "torchvision"],
    #     "code": "from datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load EMNIST dataset\n# Available splits: byclass, bymerge, balanced, letters, digits, mnist\ndataset = load_dataset(\"randall-lab/emnist\", name=\"byclass\")\n\ntrain_data = dataset['train']\ntest_data = dataset['test']\n\n# Access sample\nsample = train_data[0]\nimage = sample['image']\nlabel = sample['label']\n\n# Visualize samples\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\nfor i, ax in enumerate(axes.flat):\n    idx = np.random.randint(len(train_data))\n    sample = train_data[idx]\n    ax.imshow(sample['image'], cmap='gray')\n    ax.set_title(f\"Label: {sample['label']}\")\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n\n# PyTorch alternative\nimport torchvision\nemnist_train = torchvision.datasets.EMNIST(\n    root='./data',\n    split='byclass',\n    train=True,\n    download=True\n)\n",
    #     "citation": "@inproceedings{cohen2017emnist,\n  title={EMNIST: Extending MNIST to handwritten letters},\n  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},\n  booktitle={2017 International Joint Conference on Neural Networks (IJCNN)},\n  pages={2921--2926},\n  year={2017}\n}",
    # },
    "Omniglot": {
        "description": "One-shot and few-shot learning dataset with 1,623 handwritten characters from 50 alphabets. Each character drawn by 20 people (32,460 total images). Includes stroke data with temporal information.",
        "num_training_samples": 19280,
        "num_validation_samples": 13180,
        "huggingface_url": "https://huggingface.co/datasets/GATE-engine/omniglot",
        "num_classes": 1623,
        "image_size": "105x105",
        "task_type": ["few-shot-classification", "one-shot-classification"],
        "release_year": 2015,
        "dependent_packages": [
            "torch",
            "torchvision",
            "numpy",
            "scipy",
            "PIL",
            "tensorflow-datasets",
            "datasets",
        ],
        "code": "# TensorFlow Datasets\nimport tensorflow_datasets as tfds\ndataset, info = tfds.load('omniglot', with_info=True, as_supervised=True)\n\n# PyTorch torchvision\nfrom torchvision.datasets import Omniglot\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_dataset = Omniglot(root='./data', background=True, \n                         download=True, transform=transform)\ntest_dataset = Omniglot(root='./data', background=False, \n                        download=True, transform=transform)\n",
        "citation": "@article{lake2015human,\n  title={Human-level concept learning through probabilistic program induction},\n  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},\n  journal={Science},\n  volume={350},\n  number={6266},\n  pages={1332--1338},\n  year={2015}\n}",
    },
    # "LSUN": {
    #     "description": "Large-scale Scene UNderstanding with ~1M labeled images per scene category (10 categories: bedroom, bridge, church, etc.) and 20 object categories. Images collected and labeled using deep learning with humans in the loop.",
    #     "num_training_samples": 3033042,
    #     "num_validation_samples": 300,
    #     "huggingface_url": "https://huggingface.co/datasets/pcuenq/lsun-bedrooms",
    #     "num_classes": 10,
    #     "image_size": "variable (smaller dimension typically 256px)",
    #     "task_type": "Scene Classification, Scene Understanding, Image Generation",
    #     "release_year": 2015,
    #     "dependent_packages": [
    #         "torch",
    #         "lmdb",
    #         "PIL",
    #         "numpy",
    #         "tensorflow-datasets",
    #         "datasets",
    #     ],
    #     "code": "# Using official LSUN with LMDB\nimport lmdb\nfrom PIL import Image\nimport io\n\ndef load_lsun_category(lmdb_path):\n    env = lmdb.open(lmdb_path, max_readers=1, readonly=True, \n                    lock=False, readahead=False, meminit=False)\n    \n    with env.begin(write=False) as txn:\n        cursor = txn.cursor()\n        for key, val in cursor:\n            img = Image.open(io.BytesIO(val))\n            yield img\n\n# PyTorch\nfrom torchvision.datasets import LSUN\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(256),\n    transforms.ToTensor()\n])\n\ndataset = LSUN(root='./data', classes=['bedroom_train'], transform=transform)\n",
    #     "citation": "@article{yu2015lsun,\n  title={LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop},\n  author={Yu, Fisher and Zhang, Yinda and Song, Shuran and Seff, Ari and Xiao, Jianxiong},\n  journal={arXiv preprint arXiv:1506.03365},\n  year={2015}\n}",
    # },
    # "MPII-Human-Pose": {
    #     "description": "Human pose estimation benchmark with ~25,000 images containing 40,000+ people with 16 annotated body joints per person. Collected from YouTube videos of 410 everyday activities.",
    #     "num_training_samples": 28000,
    #     "num_validation_samples": 11000,
    #     "huggingface_url": "https://huggingface.co/datasets/Voxel51/MPII_Human_Pose_Dataset",
    #     "num_classes": 410,
    #     "image_size": "variable (300x200 to 800x600)",
    #     "task_type": "Human Pose Estimation, Keypoint Detection, Activity Recognition",
    #     "release_year": 2014,
    #     "dependent_packages": [
    #         "torch",
    #         "numpy",
    #         "scipy",
    #         "opencv-python",
    #         "PIL",
    #         "matplotlib",
    #         "datasets",
    #     ],
    #     "code": "# Using Hugging Face with FiftyOne\nimport fiftyone as fo\nimport fiftyone.utils.huggingface as fouh\n\ndataset = fouh.load_from_hub('Voxel51/MPII_Human_Pose_Dataset')\nsession = fo.launch_app(dataset)\n\n# Manual loading\nimport json\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\n\nclass MPIIDataset(Dataset):\n    def __init__(self, img_dir, anno_file, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n        \n        with open(anno_file, 'r') as f:\n            self.annotations = json.load(f)\n    \n    def __len__(self):\n        return len(self.annotations)\n    \n    def __getitem__(self, idx):\n        anno = self.annotations[idx]\n        img_path = os.path.join(self.img_dir, anno['image'])\n        image = Image.open(img_path).convert('RGB')\n        joints = torch.FloatTensor(anno['joints'])\n        \n        if self.transform:\n            image = self.transform(image)\n        return image, joints\n",
    #     "citation": "@inproceedings{andriluka14cvpr,\n  author={Mykhaylo Andriluka and Leonid Pishchulin and Peter Gehler and Bernt Schiele},\n  title={2D Human Pose Estimation: New Benchmark and State of the Art Analysis},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2014}\n}",
    # },
    # "NYU-Depth-V2": {
    #     "description": "The NYU-Depth V2 dataset comprises video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect. It features densely labeled pairs of aligned RGB and depth images from indoor scenes across 464 different scenes in 3 cities.",
    #     "num_training_samples": 47584,
    #     "num_validation_samples": 654,
    #     "huggingface_url": "https://huggingface.co/datasets/sayakpaul/nyu_depth_v2",
    #     "num_classes": 40,
    #     "image_size": "640x480",
    #     "task_type": "Depth Estimation",
    #     "release_year": 2012,
    #     "dependent_packages": ["datasets", "numpy", "matplotlib", "PIL"],
    #     "code": 'from datasets import load_dataset\nimport numpy as np\n\n# Load the dataset\nds = load_dataset("sayakpaul/nyu_depth_v2")\n\n# Access training data\ntrain_set = ds["train"]\n\n# Visualize a sample\nsample = train_set[0]\nimage = sample["image"]\ndepth_map = sample["depth_map"]',
    #     "citation": "@inproceedings{Silberman:ECCV12,\n  author = {Nathan Silberman, Derek Hoiem, Pushmeet Kohli and Rob Fergus},\n  title = {Indoor Segmentation and Support Inference from RGBD Images},\n  booktitle = {ECCV},\n  year = {2012}\n}",
    # },
    # "QuickDraw": {
    #     "description": "Google Quick, Draw! dataset with 50 million drawings across 345 categories. Captured as timestamped vector sequences from the online game. Available in multiple formats: raw vectors, simplified vectors, 28x28 bitmaps, and Sketch-RNN format.",
    #     "num_training_samples": 50426266,
    #     "num_validation_samples": 862500,
    #     "huggingface_url": "https://huggingface.co/datasets/google/quickdraw",
    #     "num_classes": 345,
    #     "image_size": "28x28 (bitmap), 256x256 (simplified vector)",
    #     "task_type": "Image Classification, Sketch Recognition, Generative Modeling",
    #     "release_year": 2017,
    #     "dependent_packages": ["datasets", "numpy", "PIL", "matplotlib"],
    #     "code": "from datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load dataset\ndataset = load_dataset('google/quickdraw')\n\ntrain_data = dataset['train']\n\nsample = train_data[0]\nimage = sample['image']\nlabel = sample['label']\n\n# Visualize multiple samples\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\nfor i, ax in enumerate(axes.flat):\n    idx = np.random.randint(len(train_data))\n    sample = train_data[idx]\n    ax.imshow(sample['image'], cmap='gray')\n    ax.set_title(f'Class: {sample[\"label\"]}')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n",
    #     "citation": "@article{HaEck2017,\n  author = {David Ha and Douglas Eck},\n  title = {A Neural Representation of Sketch Drawings},\n  journal = {CoRR},\n  volume = {abs/1704.03477},\n  year = {2017},\n  url = {http://arxiv.org/abs/1704.03477}\n}",
    # },
    # "Chest-X-Ray-Pneumonia": {
    #     "description": "Medical imaging dataset with 5,863 chest X-ray images (anterior-posterior) organized into 2 categories: Pneumonia and Normal. Images from pediatric patients aged 1-5 years. Quality controlled and graded by expert physicians.",
    #     "num_training_samples": 5216,
    #     "num_validation_samples": 624,
    #     "huggingface_url": "https://huggingface.co/datasets/hf-vision/chest-xray-pneumonia",
    #     "num_classes": 2,
    #     "image_size": "variable (1000x1000 to 3000x3000)",
    #     "task_type": "Binary Classification, Medical Image Classification",
    #     "release_year": 2018,
    #     "dependent_packages": [
    #         "numpy",
    #         "PIL",
    #         "matplotlib",
    #         "torch",
    #         "torchvision",
    #         "tensorflow",
    #         "datasets",
    #     ],
    #     "code": "# Using Hugging Face\nfrom datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndataset = load_dataset('hf-vision/chest-xray-pneumonia')\n\ntrain_data = dataset['train']\ntest_data = dataset['test']\n\nsample = train_data[0]\nimage = sample['image']\nlabel = sample['label']  # 0: Normal, 1: Pneumonia\n\n# Visualize\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\nfor i, ax in enumerate(axes.flat):\n    idx = np.random.randint(len(train_data))\n    sample = train_data[idx]\n    ax.imshow(sample['image'], cmap='gray')\n    label_name = 'Normal' if sample['label'] == 0 else 'Pneumonia'\n    ax.set_title(f'Label: {label_name}')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n\n# Manual loading from Kaggle\nimport os\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass ChestXRayDataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None):\n        self.root_dir = os.path.join(root_dir, split)\n        self.transform = transform\n        self.images = []\n        self.labels = []\n        \n        normal_dir = os.path.join(self.root_dir, 'NORMAL')\n        for img_name in os.listdir(normal_dir):\n            self.images.append(os.path.join(normal_dir, img_name))\n            self.labels.append(0)\n        \n        pneumonia_dir = os.path.join(self.root_dir, 'PNEUMONIA')\n        for img_name in os.listdir(pneumonia_dir):\n            self.images.append(os.path.join(pneumonia_dir, img_name))\n            self.labels.append(1)\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image = Image.open(self.images[idx]).convert('RGB')\n        label = self.labels[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n",
    #     "citation": "@article{Kermany2018,\n  author = {Daniel S. Kermany and Michael Goldbaum and Wenjia Cai and others},\n  title = {Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning},\n  journal = {Cell},\n  volume = {172},\n  number = {5},\n  pages = {1122-1131.e9},\n  year = {2018},\n  doi = {10.1016/j.cell.2018.02.010}\n}",
    # },
    # "BraTS": {
    #     "description": "Multimodal Brain Tumor Segmentation Challenge. A multi-institutional collection of pre-operative MRI scans of glioma patients. Each patient includes 4 MRI modalities (T1, T1Gd, T2, T2-FLAIR) and manually annotated 3 nested tumor sub-regions by expert neuroradiologists.",
    #     "num_training_samples": 369,
    #     "num_validation_samples": 125,
    #     "huggingface_url": "https://huggingface.co/datasets/katielink/brats_mri_segmentation_v0.1.0",
    #     "num_classes": 4,
    #     "image_size": "240x240x155 voxels (3D volume)",
    #     "task_type": "3D Semantic Segmentation, Medical Image Segmentation",
    #     "release_year": 2020,
    #     "dependent_packages": ["nibabel", "torch", "numpy", "SimpleITK", "monai"],
    #     "code": "import nibabel as nib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load BraTS data (manual download required)\nt1 = nib.load('path/to/BraTS_case_t1.nii.gz').get_fdata()\nt2 = nib.load('path/to/BraTS_case_t2.nii.gz').get_fdata()\nflair = nib.load('path/to/BraTS_case_flair.nii.gz').get_fdata()\nseg = nib.load('path/to/BraTS_case_seg.nii.gz').get_fdata()\n\n# Visualize middle slice\nslice_idx = t1.shape[2] // 2\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\naxes[0].imshow(t1[:, :, slice_idx], cmap='gray')\naxes[0].set_title('T1')\naxes[1].imshow(t2[:, :, slice_idx], cmap='gray')\naxes[1].set_title('T2')\naxes[2].imshow(flair[:, :, slice_idx], cmap='gray')\naxes[2].set_title('FLAIR')\naxes[3].imshow(seg[:, :, slice_idx])\naxes[3].set_title('Segmentation')\nplt.show()\n",
    #     "citation": "@article{menze2015multimodal,\n  title={The multimodal brain tumor image segmentation benchmark (BRATS)},\n  author={Menze, Bjoern H and Jakab, Andras and others},\n  journal={IEEE Transactions on Medical Imaging},\n  volume={34},\n  number={10},\n  pages={1993--2024},\n  year={2015}\n}",
    # },
    # "PatchCamelyon": {
    #     "description": "The PatchCamelyon benchmark is a challenging image classification dataset consisting of 327,680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annotated with a binary label indicating presence of metastatic tissue.",
    #     "num_training_samples": 262144,
    #     "num_validation_samples": 65536,
    #     "huggingface_url": "https://huggingface.co/datasets/1aurent/PatchCamelyon",
    #     "num_classes": 2,
    #     "image_size": "96x96 (RGB)",
    #     "task_type": ["image-classification"],
    #     "release_year": 2018,
    #     "dependent_packages": ["datasets", "PIL", "Python"],
    #     "code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"1aurent/PatchCamelyon\")\n\n# Access examples\ntrain_example = dataset['train'][0]\nprint(train_example['image'])  # PIL Image\nprint(train_example['label'])  # 0 or 1",
    #     "citation": '@ARTICLE{Veeling2018-qh,\n  title = "Rotation Equivariant {CNNs} for Digital Pathology",\n  author = "Veeling, Bastiaan S and Linmans, Jasper and Winkens, Jim and Cohen, Taco and Welling, Max",\n  year = 2018,\n  eprint = "1806.03962"\n}',
    # },
    "EuroSAT": {
        "description": "A benchmark dataset for land use and land cover classification based on Sentinel-2 satellite imagery. Contains 27,000 labeled and geo-referenced samples covering 10 land use classes across 34 European countries.",
        "num_training_samples": 21600,
        "num_validation_samples": 5400,
        "huggingface_url": "https://huggingface.co/datasets/timm/eurosat-rgb",
        "num_classes": 10,
        "image_size": "64x64 (RGB)",
        "task_type": ["image-classification"],
        "release_year": 2019,
        "dependent_packages": ["datasets", "PIL", "numpy", "torch"],
        "code": "from datasets import load_dataset\n\n# Load EuroSAT RGB dataset\ndataset = load_dataset(\"timm/eurosat-rgb\")\n\n# Access the dataset\nall_data = dataset['train']\n\n# Access sample\nsample = all_data[0]\nimage = sample['image']  # PIL Image 64x64\nlabel = sample['label']  # Integer label (0-9)",
        "citation": "@article{helber2019eurosat,\n  title={Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification},\n  author={Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},\n  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},\n  volume={12},\n  number={7},\n  pages={2217--2226},\n  year={2019},\n  publisher={IEEE}\n}",
    },
    "UC-Merced": {
        "description": "The UC Merced Land Use dataset is a land use classification dataset containing 256x256 pixel, 1-foot resolution RGB images of urban locations around the United States extracted from the USGS National Map Urban Area Imagery collection. Consists of 21 land use classes representing different types of land cover.",
        "num_training_samples": 1680,
        "num_validation_samples": 420,
        "huggingface_url": "https://huggingface.co/datasets/blanchon/UC_Merced",
        "num_classes": 21,
        "image_size": "256x256",
        "task_type": ["image-classification"],
        "release_year": 2010,
        "dependent_packages": ["datasets", "PIL", "Python"],
        "code": "from datasets import load_dataset\n\n# Load the dataset\nUC_Merced = load_dataset(\"blanchon/UC_Merced\")\n\n# Access training split\ntrain_dataset = UC_Merced['train']\n\n# Access a sample\nsample = train_dataset[0]\nimage = sample['image']\nlabel = sample['label']  # Integer label (0-20)",
        "citation": "@inproceedings{yang2010bag,\n    author = {Yang, Yi and Newsam, Shawn},\n    title = {Bag-Of-Visual-Words and Spatial Extensions for Land-Use Classification},\n    booktitle = {ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},\n    year = {2010},\n    pages = {270--279}\n}",
    },
    "RESISC45": {
        "description": "RESISC45 (Remote Sensing Image Scene Classification) is a benchmark comprising 31,500 RGB images extracted using Google Earth, with each image having a resolution of 256x256 pixels. Images are collected from over 100 countries with high variability in image conditions.",
        "num_training_samples": 18900,
        "num_validation_samples": 12600,
        "huggingface_url": "https://huggingface.co/datasets/timm/resisc45",
        "num_classes": 45,
        "image_size": "256x256 (RGB)",
        "task_type": ["image-classification"],
        "release_year": 2017,
        "dependent_packages": ["datasets", "PIL", "Python"],
        "code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"timm/resisc45\")\n\n# Access training examples\ntrain_example = dataset['train'][0]\nprint(train_example['image'])\nprint(train_example['label'])\nprint(train_example['image_id'])",
        "citation": "@article{Cheng_2017,\n  title={Remote Sensing Image Scene Classification: Benchmark and State of the Art},\n  author={Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},\n  journal={Proceedings of the IEEE},\n  volume={105},\n  number={10},\n  pages={1865-1883},\n  year={2017}\n}",
    },
    # "DOTA": {
    #     "description": "Dataset for Object Detection in Aerial Images. A large-scale benchmark for object detection in aerial images. Contains 2,806 aerial images from various sensors and platforms. 15 common object categories labeled in Oriented Bounding Boxes (OBB) format for accurate representation of arbitrarily oriented objects.",
    #     "num_training_samples": 1411,
    #     "num_validation_samples": 458,
    #     "huggingface_url": "https://captain-whu.github.io/DOTA/",
    #     "num_classes": 15,
    #     "image_size": "~4000x4000 (typically split into 1024x1024 patches)",
    #     "task_type": "Oriented Object Detection, Aerial Image Analysis",
    #     "release_year": 2018,
    #     "dependent_packages": [
    #         "torch",
    #         "torchvision",
    #         "opencv-python",
    #         "ultralytics",
    #         "mmdetection",
    #     ],
    #     "code": "from ultralytics import YOLO\nimport cv2\nimport numpy as np\n\n# Load YOLO model with Oriented Bounding Box support\nmodel = YOLO('yolov8n-obb.pt')\n\n# Train on DOTA\nresults = model.train(\n    data='DOTAv1.yaml',\n    epochs=100,\n    imgsz=1024,\n    batch=16\n)\n\n# Inference\nimage = cv2.imread('path/to/aerial_image.jpg')\nresults = model(image)\n\n# Classes: plane, ship, storage-tank, baseball-diamond, tennis-court,\n#          basketball-court, ground-track-field, harbor, bridge,\n#          large-vehicle, small-vehicle, helicopter, roundabout,\n#          soccer-ball-field, swimming-pool\n\n# Visualize results\nannotated_image = results[0].plot()\ncv2.imshow('DOTA Detection', annotated_image)\ncv2.waitKey(0)\n",
    #     "citation": "@article{xia2018dota,\n  title={DOTA: A large-scale dataset for object detection in aerial images},\n  author={Xia, Gui-Song and Bai, Xiang and Ding, Jian and Zhu, Zhen and others},\n  journal={In IEEE CVPR},\n  pages={3974--3983},\n  year={2018}\n}",
    # },
    "FER2013": {
        "description": "FER-2013: 35,887 48x48 grayscale face images labeled with 7 emotions (angry, disgust, fear, happy, sad, surprise, neutral). Used for facial expression recognition.",
        "num_training_samples": 28709,
        "num_validation_samples": 3589,
        "huggingface_url": "https://huggingface.co/datasets/Jeneral/fer2013",
        "num_classes": 7,
        "image_size": "48x48 (grayscale)",
        "task_type": ["image-classification", "facial-expression-recognition"],
        "release_year": 2013,
        "dependent_packages": ["datasets", "numpy"],
        "code": "from datasets import load_dataset\n# Load FER2013 dataset\ndataset = load_dataset(\"Jeneral/fer2013\")\ntrain_data = dataset['train']\nimage = train_data[0]['image']\nlabel = train_data[0]['labels']",
        "citation": "@inproceedings{mollahosseini2016going,\n  title={Going deeper in facial expression recognition using deep neural networks},\n  author={Mollahosseini, Ahmad and Hasani, Behzad and Mahoor, Mohammad H},\n  booktitle={WACV},\n  pages={1--10},\n  year={2016}\n}",
    },
    "AffectNet": {
        "description": "AffectNet is one of the largest databases for facial expression, valence, and arousal in the wild. Contains over 1 million facial images collected from the Internet, with approximately 440,000 images manually annotated for 8 discrete facial expressions and continuous values of valence and arousal.",
        "num_training_samples": 287651,
        "num_validation_samples": 3500,
        "huggingface_url": "https://huggingface.co/datasets/chitradrishti/AffectNet",
        "num_classes": 8,
        "image_size": "variable (typically resized to 224x224 or 256x256)",
        "task_type": ["facial-expression-recognition", "valence-arousal-prediction"],
        "release_year": 2017,
        "dependent_packages": [
            "datasets",
            "torch",
            "tensorflow",
            "opencv-python",
            "PIL",
            "numpy",
        ],
        "code": "from datasets import load_dataset\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\n\n# Load AffectNet dataset\ndataset = load_dataset(\"chitradrishti/AffectNet\")\n\ntrain_data = dataset['train']\nval_data = dataset['validation']\n\n# Transform\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n])\n\n# Classes: 0=Neutral, 1=Happy, 2=Sad, 3=Surprise,\n#          4=Fear, 5=Disgust, 6=Anger, 7=Contempt\n# Also includes valence and arousal values (continuous)\n\nsample = train_data[0]\nimage = sample['image']\nexpression = sample['expression']\nvalence = sample['valence']\narousal = sample['arousal']\n",
        "citation": "@article{mollahosseini2017affectnet,\n  title={Affectnet: A database for facial expression, valence, and arousal computing in the wild},\n  author={Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H},\n  journal={IEEE Transactions on Affective Computing},\n  volume={10},\n  number={1},\n  pages={18--31},\n  year={2017}\n}",
    },
    "WIDER-FACE": {
        "description": "WIDER FACE: a face detection dataset with 32,203 images and 393,703 labeled faces across 61 event classes. Includes high variability in pose, scale, and occlusion.",
        "num_training_samples": 32203,
        "num_validation_samples": 0,
        "huggingface_url": "https://huggingface.co/datasets/zhanghao/FaceDetectionWIDERFACE",
        "num_classes": "N/A (detection)",
        "image_size": "variable",
        "task_type": ["face-detection"],
        "release_year": 2016,
        "dependent_packages": ["torchvision", "PIL"],
        "code": "from torchvision.datasets import WIDERFace\nwider_train = WIDERFace(root='./data', split='train', download=True)\nimage, target = wider_train[0]",
        "citation": "@inproceedings{yang2016wider,\n  title={WIDER FACE: A Face Detection Benchmark},\n  author={Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},\n  booktitle={CVPR},\n  pages={5525--5533},\n  year={2016}\n}",
    },
    "VGGFace2": {
        "description": "A large-scale face recognition dataset with 3.31 million images of 9,131 subjects. Downloaded from Google Image Search with large variations in pose, age, illumination, ethnicity, and profession. Widely used for face recognition and face verification tasks.",
        "num_training_samples": 3141890,
        "num_validation_samples": 169396,
        "huggingface_url": "https://huggingface.co/datasets/ProgramComputer/VGGFace2",
        "num_classes": 9131,
        "image_size": "variable (typically resized to 224x224)",
        "task_type": ["face-verification"],
        "release_year": 2018,
        "dependent_packages": [
            "torch",
            "keras-vggface",
            "tensorflow",
            "opencv-python",
            "PIL",
        ],
        "code": "from keras_vggface.vggface import VGGFace\nimport numpy as np\nfrom keras_vggface import utils\nfrom keras.preprocessing import image\n\n# Load pre-trained VGGFace2 model\nmodel = VGGFace(\n    model='resnet50',\n    include_top=True,\n    input_shape=(224, 224, 3),\n    pooling='avg'\n)\n\n# Load and preprocess image\nimg = image.load_img('path/to/face_image.jpg', target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = utils.preprocess_input(x, version=2)  # version=2 for VGGFace2\n\n# Get predictions\npreds = model.predict(x)\nprint(f'Predicted identity: {np.argmax(preds)}')\n\n# For face verification\nmodel_verification = VGGFace(\n    model='resnet50',\n    include_top=False,\n    input_shape=(224, 224, 3),\n    pooling='avg'\n)\n",
        "citation": "@inproceedings{cao2018vggface2,\n  title={Vggface2: A dataset for recognising faces across pose and age},\n  author={Cao, Qiong and Shen, Li and Xie, Weidi and Parkhi, Omkar M and Zisserman, Andrew},\n  booktitle={13th IEEE International Conference on Automatic Face \\& Gesture Recognition (FG 2018)},\n  pages={67--74},\n  year={2018}\n}",
    },
    # "KMNIST": {
    #     "description": "Kuzushiji-MNIST dataset for recognizing cursive Japanese (Kuzushiji) hiragana characters used in classical Japanese literature. Designed as a drop-in replacement for MNIST, consists of 70,000 28x28 grayscale images. Created to support digitization of Japanese classical literature.",
    #     "num_training_samples": 60000,
    #     "num_validation_samples": 10000,
    #     "huggingface_url": "https://huggingface.co/datasets/tanganke/kmnist",
    #     "num_classes": 10,
    #     "image_size": "28x28 (grayscale)",
    #     "task_type": "Handwritten Character Recognition, Kuzushiji Recognition",
    #     "release_year": 2018,
    #     "dependent_packages": ["datasets", "numpy", "torch", "torchvision", "PIL"],
    #     "code": "from datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load KMNIST dataset\ndataset = load_dataset(\"tanganke/kmnist\")\n\ntrain_data = dataset['train']\ntest_data = dataset['test']\n\n# 10 hiragana classes: o, ki, su, tsu, na, ha, ma, ya, re, wo\n\n# Access sample\nsample = train_data[0]\nimage = sample['image']\nlabel = sample['label']\n\n# Visualize samples\nfig, axes = plt.subplots(3, 3, figsize=(9, 9))\nfor i, ax in enumerate(axes.flat):\n    idx = np.random.randint(len(train_data))\n    sample = train_data[idx]\n    ax.imshow(sample['image'], cmap='gray')\n    ax.set_title(f\"Label: {sample['label']}\")\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n\n# PyTorch alternative\nfrom torchvision import datasets\nkmnist_train = datasets.KMNIST(\n    root='./data',\n    train=True,\n    download=True\n)\n",
    #     "citation": "@online{clanuwat2018deep,\n  title={Deep Learning for Classical Japanese Literature},\n  author={Clanuwat, Tarin and Bober-Irizar, Mikel and Kitamoto, Asanobu and Lamb, Alex and Yamamoto, Kazuaki and Ha, David},\n  year={2018},\n  eprint={1812.01718}\n}",
    # },
    # "IAM-Handwriting": {
    #     "description": "IAM Handwriting Database containing forms of handwritten English text. Includes 13,353 handwritten line images written by 657 different writers. Supports line-level and word-level recognition tasks. Reflects the diversity of real handwritten documents, widely used for OCR system evaluation.",
    #     "num_training_samples": 6482,
    #     "num_validation_samples": 976,
    #     "huggingface_url": "https://huggingface.co/datasets/Teklia/IAM-line",
    #     "num_classes": "Sequence recognition (~80 character classes)",
    #     "image_size": "128 pixels height (fixed), variable width",
    #     "task_type": "Handwritten Text Recognition (HTR), Sequence Recognition",
    #     "release_year": 1999,
    #     "dependent_packages": [
    #         "datasets",
    #         "PIL",
    #         "torch",
    #         "transformers",
    #         "opencv-python",
    #     ],
    #     "code": "from datasets import load_dataset\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nimport matplotlib.pyplot as plt\n\n# Load IAM dataset\ndataset = load_dataset(\"Teklia/IAM-line\", split=\"train\")\n\n# Load pre-trained TrOCR model\nprocessor = TrOCRProcessor.from_pretrained(\n    'microsoft/trocr-base-handwritten'\n)\nmodel = VisionEncoderDecoderModel.from_pretrained(\n    'microsoft/trocr-base-handwritten'\n)\n\n# Process sample\nsample = dataset[0]\nimage = sample['image']\ntext = sample['text']\n\n# Inference\npixel_values = processor(image, return_tensors=\"pt\").pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True\n)[0]\n\nprint(f'Ground truth: {text}')\nprint(f'Prediction: {generated_text}')\n\n# Visualize\nplt.imshow(image, cmap='gray')\nplt.title(f'Text: {text}')\nplt.axis('off')\nplt.show()\n",
    #     "citation": "@article{marti2002iam,\n  title={The IAM-database: an English sentence database for offline handwriting recognition},\n  author={Marti, U-V and Bunke, Horst},\n  journal={International Journal on Document Analysis and Recognition},\n  volume={5},\n  number={1},\n  pages={39--46},\n  year={2002}\n}",
    # },
    "DAVIS": {
        "description": "Densely Annotated VIdeo Segmentation benchmark for high-quality video object segmentation. Consists of Full HD resolution video sequences covering challenges like occlusion, motion blur, and appearance changes. Each video frame has pixel-accurate dense annotations.",
        "num_training_samples": 60,
        "num_validation_samples": 30,
        "huggingface_url": "https://davischallenge.org/",
        "num_classes": "DAVIS 2016: 1 object/video, DAVIS 2017: multiple objects",
        "image_size": "Full HD (1920x1080), 30fps",
        "task_type": ["video-object-segmentation"],
        "release_year": 2016,
        "dependent_packages": ["opencv-python", "numpy", "PIL", "scipy", "torch"],
        "code": "import numpy as np\nfrom PIL import Image\nimport cv2\nimport os\n\ndef load_davis_sequence(data_root, sequence_name):\n    \"\"\"Load a DAVIS sequence\"\"\"\n    images_path = os.path.join(\n        data_root, 'JPEGImages', '480p', sequence_name\n    )\n    annotations_path = os.path.join(\n        data_root, 'Annotations', '480p', sequence_name\n    )\n    \n    images = sorted(os.listdir(images_path))\n    annotations = sorted(os.listdir(annotations_path))\n    \n    frames = []\n    masks = []\n    \n    for img_name, ann_name in zip(images, annotations):\n        img = cv2.imread(os.path.join(images_path, img_name))\n        mask = np.array(Image.open(\n            os.path.join(annotations_path, ann_name)\n        ))\n        frames.append(img)\n        masks.append(mask)\n    \n    return frames, masks\n\n# Usage\nframes, masks = load_davis_sequence(\n    'path/to/DAVIS',\n    'bear'\n)\nprint(f'Loaded {len(frames)} frames')\n",
        "citation": "@inproceedings{Perazzi2016,\n  title={A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation},\n  author={Perazzi, Federico and Pont-Tuset, Jordi and McWilliams, Brian and Van Gool, Luc and Gross, Markus and Sorkine-Hornung, Alexander},\n  booktitle={CVPR},\n  year={2016}\n}",
    },
    "Mapillary-Vistas": {
        "description": "A diverse street-level imagery dataset with pixel-accurate and instance-specific human annotations for understanding street scenes around the world. The dataset contains high-resolution images annotated into semantic object categories using polygons for delineating individual objects.",
        "num_training_samples": 18000,
        "num_validation_samples": 7000,
        "huggingface_url": "https://huggingface.co/datasets/candylion/mapillary-vistas-v2",
        "num_classes": 124,
        "image_size": "variable (high-resolution)",
        "task_type": ["semantic-segmentation"],
        "release_year": 2020,
        "dependent_packages": ["datasets", "transformers", "PIL", "torch"],
        "code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"candylion/mapillary-vistas-v2\")\n\n# Access training data\ntrain_data = dataset['train']\n\n# Load a sample\nsample = train_data[0]\nimage = sample['image']\nannotation = sample['annotation']",
        "citation": "@inproceedings{Neuhold_ICCV_2017,\n  author = {Gerhard Neuhold and Tobias Ollmann and Samuel Rota Bulò and Peter Kontschieder},\n  title = {The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes},\n  booktitle = {International Conference on Computer Vision (ICCV)},\n  year = {2017}\n}",
    },
    # "Kvasir-SEG": {
    #     "description": "A Segmented Polyp Dataset for medical image segmentation in gastrointestinal endoscopy images. Contains 1,000 colon polyp images with corresponding segmentation masks, manually annotated and verified by experienced gastroenterologists.",
    #     "num_training_samples": 800,
    #     "num_validation_samples": 200,
    #     "huggingface_url": "https://huggingface.co/datasets/kowndinya23/Kvasir-SEG",
    #     "num_classes": 2,
    #     "image_size": "variable (332x487 to 1920x1072)",
    #     "task_type": "Medical Image Segmentation (Binary)",
    #     "release_year": 2020,
    #     "dependent_packages": [
    #         "datasets",
    #         "PIL",
    #         "numpy",
    #         "opencv-python",
    #         "torch",
    #         "albumentations",
    #     ],
    #     "code": "from datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n\n# Load Kvasir-SEG dataset\ndataset = load_dataset(\"kowndinya23/Kvasir-SEG\")\n\ntrain_data = dataset['train']\n\n# Access sample\nsample = train_data[0]\nimage = sample['image']\nmask = sample['mask']\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\naxes[0].imshow(image)\naxes[0].set_title('Original Image')\naxes[0].axis('off')\n\naxes[1].imshow(mask, cmap='gray')\naxes[1].set_title('Segmentation Mask')\naxes[1].axis('off')\n\n# Overlay\noverlay = np.array(image).copy()\nmask_array = np.array(mask)\noverlay[mask_array > 0] = [255, 0, 0]\naxes[2].imshow(overlay)\naxes[2].set_title('Overlay')\naxes[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n",
    #     "citation": "@inproceedings{jha2020kvasir,\n  title={Kvasir-seg: A segmented polyp dataset},\n  author={Jha, Debesh and Smedsrud, Pia H and Riegler, Michael A and others},\n  booktitle={International Conference on Multimedia Modeling},\n  pages={451--462},\n  year={2020}\n}",
    # },
    # "BDD100K": {
    #     "description": "Berkeley DeepDrive dataset, the largest and most diverse driving video dataset for autonomous driving research. Contains 100,000 driving videos (each ~40 seconds, 720p, 30fps) collected across diverse regions in the US. Supports 10 tasks covering various weather conditions, times of day, and scene types.",
    #     "num_training_samples": 70000,
    #     "num_validation_samples": 10000,
    #     "huggingface_url": "https://huggingface.co/datasets/dgural/bdd100k",
    #     "num_classes": "10 (detection), 19 (segmentation)",
    #     "image_size": "720p (1280x720)",
    #     "task_type": ["object-detection", "semantic-segmentation", "instance-segmentation", "lane-detection", "tracking"],
    #     "release_year": 2018,
    #     "dependent_packages": [
    #         "torch",
    #         "torchvision",
    #         "transformers",
    #         "opencv-python",
    #         "bdd100k",
    #     ],
    #     "code": 'from transformers import (\n    SegformerFeatureExtractor,\n    SegformerForSemanticSegmentation\n)\nfrom PIL import Image\nimport torch\nimport cv2\n\n# Load pre-trained segmentation model\nextractor = SegformerFeatureExtractor.from_pretrained(\n    "nvidia/segformer-b0-finetuned-cityscapes-1024-1024"\n)\nmodel = SegformerForSemanticSegmentation.from_pretrained(\n    "nvidia/segformer-b0-finetuned-cityscapes-1024-1024"\n)\n\n# Load image\nimage = Image.open("path/to/driving_scene.jpg")\n\n# Preprocess\ninputs = extractor(images=image, return_tensors="pt")\n\n# Inference\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits\n\n# Post-process\nsegmentation = logits.argmax(dim=1).squeeze(0)\n\nprint(f"Segmentation shape: {segmentation.shape}")\n\n# 10 detection classes: bike, bus, car, motor, person, rider,\n#                        traffic light, traffic sign, train, truck\n',
    #     "citation": "@inproceedings{yu2020bdd100k,\n  title={BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning},\n  author={Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and others},\n  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2020}\n}",
    # },
    "CIFAR-10": {
        "description": "CIFAR-10 is a dataset of 60,000 32x32 color images in 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. One of the most widely used fundamental benchmarks in deep learning research.",
        "num_training_samples": 50000,
        "num_validation_samples": 10000,
        "huggingface_url": "https://huggingface.co/datasets/uoft-cs/cifar10",
        "num_classes": 10,
        "image_size": "32x32 (RGB)",
        "task_type": ["image-classification"],
        "release_year": 2009,
        "dependent_packages": ["datasets", "torch", "torchvision", "PIL"],
        "code": "from datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load CIFAR-10 dataset\ndataset = load_dataset(\"uoft-cs/cifar10\")\n\ntrain_data = dataset['train']\ntest_data = dataset['test']\n\n# Class names\nclass_names = [\n    'airplane', 'automobile', 'bird', 'cat', 'deer',\n    'dog', 'frog', 'horse', 'ship', 'truck'\n]\n\n# Visualize samples\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\nfor i, ax in enumerate(axes.flat):\n    idx = np.random.randint(len(train_data))\n    sample = train_data[idx]\n    ax.imshow(sample['img'])\n    ax.set_title(class_names[sample['label']])\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n\n# PyTorch alternative\nimport torchvision\ncifar10_train = torchvision.datasets.CIFAR10(\n    root='./data',\n    train=True,\n    download=True\n)\n",
        "citation": "@techreport{krizhevsky2009learning,\n  title={Learning multiple layers of features from tiny images},\n  author={Krizhevsky, Alex and Hinton, Geoffrey},\n  institution={University of Toronto},\n  year={2009}\n}",
    },
    # "ObjectNet": {
    #     "description": "ObjectNet is a large bias-controlled dataset designed to push the limits of object recognition models. Collected with intentionally novel viewpoints and backgrounds for objects. Contains 50,000 images and is test-only (no training set) to measure model robustness.",
    #     "num_training_samples": 0,
    #     "num_validation_samples": 50000,
    #     "huggingface_url": "https://huggingface.co/datasets/timm/objectnet",
    #     "num_classes": 313,
    #     "image_size": "variable",
    #     "task_type": "Robust Object Recognition Evaluation",
    #     "release_year": 2019,
    #     "dependent_packages": ["datasets", "torch", "torchvision", "PIL"],
    #     "code": "from datasets import load_dataset\nimport torch\nimport torchvision.models as models\nfrom torchvision import transforms\n\n# Load ObjectNet dataset (test only)\ndataset = load_dataset(\"timm/objectnet\")\ntest_data = dataset['test']\n\n# Load pre-trained model\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n# Evaluate on ObjectNet\nsample = test_data[0]\nimage = sample['image']\nlabel = sample['label']\n\nimage_tensor = transform(image).unsqueeze(0)\n\nwith torch.no_grad():\n    output = model(image_tensor)\n    prediction = torch.argmax(output, dim=1)\n\nprint(f'Ground truth: {label}, Prediction: {prediction.item()}')\n",
    #     "citation": "@incollection{barbu2019objectnet,\n  title={ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models},\n  author={Barbu, Andrei and Mayo, David and Alverio, Julian and others},\n  booktitle={Advances in Neural Information Processing Systems},\n  pages={9448--9458},\n  year={2019}\n}",
    # },
    "GTSRB": {
        "description": "A multi-class classification dataset featuring 43 classes of traffic signs. The images were cropped from a larger set to focus on the traffic sign and eliminate background. Includes multiple augmented test sets to benchmark model robustness.",
        "num_training_samples": 26640,
        "num_validation_samples": 12630,
        "huggingface_url": "https://huggingface.co/datasets/tanganke/gtsrb",
        "num_classes": 43,
        "image_size": "variable",
        "task_type": ["image-classification"],
        "release_year": 2012,
        "dependent_packages": ["datasets", "PIL", "numpy", "torch"],
        "code": "from datasets import load_dataset\n\n# Load GTSRB dataset\ndataset = load_dataset('tanganke/gtsrb')\n\n# Access different splits\ntrain_data = dataset['train']\ntest_data = dataset['test']\n\n# Access robustness test sets\ncontrast_test = dataset['contrast']\ngaussian_noise_test = dataset['gaussian_noise']\n\n# Example usage\nsample = train_data[0]\nimage = sample['image']\nlabel = sample['label']",
        "citation": "@article{stallkampManVsComputer2012,\n  title = {Man vs. Computer: Benchmarking Machine Learning Algorithms for Traffic Sign Recognition},\n  author = {Stallkamp, J. and Schlipsing, M. and Salmen, J. and Igel, C.},\n  year = {2012},\n  journal = {Neural Networks},\n  volume = {32},\n  pages = {323--332}\n}",
    },
    "AQUA20": {
        "description": "A Benchmark Dataset for Underwater Species Classification in challenging conditions. Large-scale dataset with 8,171 samples across 20 different categories including fish, coral, marine plants, sharks, and shrimp. Contains visual constraints specific to underwater environments (color distortion, low contrast, variable lighting).",
        "num_training_samples": 6537,
        "num_validation_samples": 817,
        "huggingface_url": "https://huggingface.co/datasets/taufiktrf/AQUA20",
        "num_classes": 20,
        "image_size": "variable",
        "task_type": ["image-classification"],
        "release_year": 2024,
        "dependent_packages": ["datasets", "torch", "transformers", "PIL", "numpy"],
        "code": "from datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\n\n# Load AQUA20 dataset\ndataset = load_dataset(\"taufiktrf/AQUA20\")\n\ntrain_data = dataset['train']\nval_data = dataset['validation']\ntest_data = dataset['test']\n\n# 20 marine species categories\nspecies_names = [\n    'Clownfish', 'Coral', 'Crab', 'Dolphin', 'Eel',\n    'Jellyfish', 'Lobster', 'Nudibranchs', 'Octopus', 'Penguin',\n    'Pufferfish', 'Ray', 'Seahorse', 'Seal', 'Shark',\n    'Shrimp', 'Squid', 'Starfish', 'Turtle', 'Whale'\n]\n\n# Visualize samples\nfig, axes = plt.subplots(4, 5, figsize=(20, 16))\nfor i, ax in enumerate(axes.flat):\n    idx = np.random.randint(len(train_data))\n    sample = train_data[idx]\n    ax.imshow(sample['image'])\n    ax.set_title(species_names[sample['label']])\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n",
        "citation": "@article{aqua20_2024,\n  title={AQUA20: A Benchmark Dataset for Underwater Species Classification},\n  author={Taufikurrahman and others},\n  year={2024}\n}",
    },
    "ImageNet-1K": {
        "description": "ImageNet ILSVRC 2012 is a large-scale image classification dataset with 1,000 classes from WordNet. It contains 1,281,167 training images, 50,000 validation images, and 100,000 test images. It is a standard benchmark in computer vision.",
        "num_training_samples": 1281167,
        "num_validation_samples": 50000,
        "huggingface_url": "https://huggingface.co/datasets/ILSVRC/imagenet-1k",
        "num_classes": 1000,
        "image_size": "variable (typically resized to 224x224)",
        "task_type": ["image-classification"],
        "release_year": 2012,
        "dependent_packages": ["datasets", "torchvision", "PIL"],
        "code": "from datasets import load_dataset\n# Load ImageNet dataset\ndataset = load_dataset(\"ILSVRC/imagenet-1k\")\ntrain_data = dataset['train']\nval_data = dataset['validation']\ntest_data = dataset['test']\nimage = train_data[0]['image']\nlabel = train_data[0]['label']",
        "citation": "@article{russakovsky2015imagenet,\n  title={ImageNet Large Scale Visual Recognition Challenge},\n  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},\n  journal={International Journal of Computer Vision},\n  volume={115},\n  number={3},\n  pages={211--252},\n  year={2015}\n}",
    },
    # "WikiArt": {
    #     "description": "Dataset containing 81,444 pieces of visual art from various artists, taken from WikiArt.org, along with class labels for each image. The dataset includes artworks across different styles, genres, and artists, used for art style classification and generation tasks.",
    #     "num_training_samples": 81444,
    #     "num_validation_samples": 0,
    #     "huggingface_url": "https://huggingface.co/datasets/huggan/wikiart",
    #     "num_classes": "129 artists, 11 genres, 27 styles",
    #     "image_size": "variable",
    #     "task_type": "Image Generation",
    #     "release_year": 2018,
    #     "dependent_packages": ["datasets", "PIL", "torch"],
    #     "code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"huggan/wikiart\")\n\n# Access an image\nimage = dataset['train'][0]['image']\nartist = dataset['train'][0]['artist']\ngenre = dataset['train'][0]['genre']\nstyle = dataset['train'][0]['style']",
    #     "citation": "@misc{wikiart_dataset,\n  title = {WikiArt Dataset},\n  author = {WikiArt.org},\n  note = {Images obtained from WikiArt.org}\n}",
    # },
    # "CelebA-HQ": {
    #     "description": "CelebA-HQ is a high-quality dataset of 30,000 celebrity face images (1024x1024). Used for training high-resolution GANs.",
    #     "num_training_samples": 30000,
    #     "num_validation_samples": 0,
    #     "huggingface_url": "https://huggingface.co/datasets/Chris1/celebA-HQ",
    #     "num_classes": "N/A (generative)",
    #     "image_size": "1024x1024",
    #     "task_type": "Face Image Generation",
    #     "release_year": 2017,
    #     "dependent_packages": ["datasets", "PIL"],
    #     "code": "from datasets import load_dataset\n# Load CelebA-HQ dataset\ndataset = load_dataset(\"Chris1/celebA-HQ\", split='train')\nimage = dataset[0]['image']",
    #     "citation": "@inproceedings{karras2017progressive,\n  title={Progressive Growing of GANs for Improved Quality, Stability, and Variation},\n  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},\n  booktitle={International Conference on Learning Representations (ICLR)},\n  year={2017}\n}",
    # },
    "CheXpert": {
        "description": "CheXpert is a large chest X-ray dataset of 224,316 images from 65,240 patients, labeled for 14 observations (e.g., pneumonia, edema) with uncertainty labels. It is used for evaluating radiographic diagnosis models.",
        "num_training_samples": 224316,
        "num_validation_samples": 0,
        "huggingface_url": "https://huggingface.co/datasets/stanfordmlgroup/chexpert",
        "num_classes": 14,
        "image_size": "variable (e.g., 320x320)",
        "task_type": ["image-classification"],
        "release_year": 2019,
        "dependent_packages": ["datasets", "numpy", "pandas"],
        "code": "from datasets import load_dataset\n# Load CheXpert dataset\ndataset = load_dataset(\"stanfordmlgroup/chexpert\")\nimage = dataset['train'][0]['image']\nlabels = dataset['train'][0]['labels']",
        "citation": "@article{irvin2019chexpert,\n  title={CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison},\n  author={Irvin, Jeremy and Rajpurkar, Pranav and Ko, Eric and Yu, Yifan and Ciurea-Ilcus, Sebastian and Chute, Christopher and Marklund, Henrik and Ball, Robyn and Shpanskaya, Katie and Seekins, Joel and others},\n  journal={Proceedings of the AAAI Conference on Artificial Intelligence},\n  volume={33},\n  number={01},\n  pages={590--597},\n  year={2019}\n}",
    },
    "AFHQ": {
        "description": "Animal Faces-HQ (AFHQ) is a dataset of 15,000 high-quality animal face images (512x512) from 3 categories: cats, dogs, and wild animals (5000 each). Used for fine-grained classification and generative modeling.",
        "num_training_samples": 15000,
        "num_validation_samples": 0,
        "huggingface_url": "https://huggingface.co/datasets/elozano/afhq",
        "num_classes": 3,
        "image_size": "512x512 (RGB)",
        "task_type": ["image-classification"],
        "release_year": 2020,
        "dependent_packages": ["datasets", "PIL"],
        "code": "from datasets import load_dataset\n# Load AFHQ dataset\ndataset = load_dataset(\"elozano/afhq\")\nimage = dataset['train'][0]['image']\nlabel = dataset['train'][0]['label']",
        "citation": "@inproceedings{choi2020stargan,\n  title={StarGAN v2: Diverse Image Synthesis for Multiple Domains},\n  author={Choi, Yunjey and Uh, Jeongtae and Yoo, Jongwook and Ha, Jung-Woo},\n  booktitle={CVPR},\n  pages={8188--8197},\n  year={2020}\n}",
    },
    "ImageNet-A": {
        "description": "ImageNet-A: 7,500 natural adversarial examples (images) that fool ImageNet classifiers. Covers 200 classes.",
        "num_training_samples": 0,
        "num_validation_samples": 7500,
        "huggingface_url": "https://huggingface.co/datasets/imagenet_a",
        "num_classes": 200,
        "image_size": "variable",
        "task_type": ["image-classification"],
        "release_year": 2020,
        "dependent_packages": ["datasets", "PIL"],
        "code": "from datasets import load_dataset\nimagenet_a = load_dataset('imagenet_a', split='train')\nimage = imagenet_a[0]['image']\nlabel = imagenet_a[0]['label']",
        "citation": "@inproceedings{hendrycks2021natural,\n  title={Natural Adversarial Examples},\n  author={Hendrycks, Dan and Mu, Kevin and Cubuk, Ekin and Gilmer, Justin and Zhang, Balaji and Madry, Aleksander},\n  booktitle={CVPR},\n  pages={15262--15271},\n  year={2021}\n}",
    },
    # "ImageNet-O": {
    #     "description": "ImageNet-O: 2,000 out-of-distribution images (not in ImageNet classes) for testing model robustness.",
    #     "num_training_samples": 0,
    #     "num_validation_samples": 2000,
    #     "huggingface_url": "https://huggingface.co/datasets/imagenet_o",
    #     "num_classes": 1000,
    #     "image_size": "variable",
    #     "task_type": "Out-of-Distribution Detection",
    #     "release_year": 2021,
    #     "dependent_packages": ["datasets", "PIL"],
    #     "code": "imagenet_o = load_dataset('imagenet_o', split='train')\nimage = imagenet_o[0]['image']",
    #     "citation": "@inproceedings{hendrycks2021natural,\n  title={Natural Adversarial Examples},\n  author={Hendrycks, Dan and Mu, Kevin and Cubuk, Ekin and Gilmer, Justin and Zhang, Balaji and Madry, Aleksander},\n  booktitle={CVPR},\n  pages={15262--15271},\n  year={2021}\n}",
    # },
    "ImageNet-R": {
        "description": "ImageNet-R (Renditions): 30,000 images of stylized renditions of ImageNet classes (art, cartoons, etc) to test style robustness.",
        "num_training_samples": 0,
        "num_validation_samples": 30000,
        "huggingface_url": "https://huggingface.co/datasets/imagenet_r",
        "num_classes": 200,
        "image_size": "variable",
        "task_type": ["image-classification"],
        "release_year": 2020,
        "dependent_packages": ["datasets", "PIL"],
        "code": "imagenet_r = load_dataset('imagenet_r', split='train')\nimage = imagenet_r[0]['image']",
        "citation": "@inproceedings{hendrycks2021natural,\n  title={Natural Adversarial Examples},\n  author={Hendrycks, Dan and Mu, Kevin and Cubuk, Ekin and Gilmer, Justin and Zhang, Balaji and Madry, Aleksander},\n  booktitle={CVPR},\n  pages={15262--15271},\n  year={2021}\n}",
    },
    "CIFAR-10.1": {
        "description": "CIFAR-10.1: held-out test set of 2,000 images for CIFAR-10, used to measure model generalization.",
        "num_training_samples": 0,
        "num_validation_samples": 2000,
        "huggingface_url": "https://huggingface.co/datasets/cifar10.1",
        "num_classes": 10,
        "image_size": "32x32 (RGB)",
        "task_type": ["image-classification"],
        "release_year": 2019,
        "dependent_packages": ["datasets", "numpy"],
        "code": "from datasets import load_dataset\ncifar101 = load_dataset('cifar10.1', split='test')\nimage = cifar101[0]['img']\nlabel = cifar101[0]['label']",
        "citation": "@inproceedings{recht2019imagenet,\n  title={Do CIFAR-10 Classifiers Generalize to CIFAR-10?},\n  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},\n  booktitle={ICML},\n  pages={5389--5400},\n  year={2019}\n}",
    },
    "Beans": {
        "description": "A dataset of bean leaf images taken in the field using smartphone cameras in Uganda. Created to build machine learning models for distinguishing diseases in bean plants. Consists of 3 classes: healthy leaves and two disease classes (Angular Leaf Spot and Bean Rust).",
        "num_training_samples": 1034,
        "num_validation_samples": 261,
        "huggingface_url": "https://huggingface.co/datasets/AI-Lab-Makerere/beans",
        "num_classes": 3,
        "image_size": "500x500 (RGB)",
        "task_type": ["image-classification"],
        "release_year": 2020,
        "dependent_packages": ["datasets", "PIL", "numpy", "torch"],
        "code": "from datasets import load_dataset\n\n# Load Beans dataset\ndataset = load_dataset(\"AI-Lab-Makerere/beans\")\n\n# Access splits\ntrain_data = dataset['train']\nvalidation_data = dataset['validation']\ntest_data = dataset['test']\n\n# Example usage\nsample = train_data[10]\nimage = sample['image']\nlabel = sample['labels']",
        "citation": "@ONLINE{beansdata,\n  author = {Makerere AI Lab},\n  title = {Bean disease dataset},\n  month = {January},\n  year = {2020},\n  url = {https://github.com/AI-Lab-Makerere/ibean/}\n}",
    },
    "MIT-Indoor-Scenes": {
        "description": "Indoor scene recognition dataset with images from 67 different indoor categories. Images have been preprocessed with auto-orientation of pixel data and resized to uniform dimensions. The dataset covers various indoor environments like airports, bedrooms, churches, offices, etc.",
        "num_training_samples": 15571,
        "num_validation_samples": 0,
        "huggingface_url": "https://huggingface.co/datasets/keremberke/indoor-scene-classification",
        "num_classes": 67,
        "image_size": "416x416",
        "task_type": ["image-classification"],
        "release_year": 2009,
        "dependent_packages": ["datasets", "PIL", "Python"],
        "code": "from datasets import load_dataset\n\n# Load the dataset\nds = load_dataset(\"keremberke/indoor-scene-classification\", name=\"full\")\n\n# Access the first training example\nexample = ds['train'][0]\nimage = example['image']\nlabel = example['label']",
        "citation": "@inproceedings{quattoni2009recognizing,\n  title={Recognizing indoor scenes},\n  author={Quattoni, Ariadna and Torralba, Antonio},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={413--420},\n  year={2009},\n  organization={IEEE}\n}",
    },
    "Cats-vs-Dogs": {
        "description": "A large set of images of cats and dogs from the Asirra (Animal Species Image Recognition for Restricting Access) dataset. Contains 23,410 images after removing corrupted images. The dataset has enormous diversity in photo backgrounds, angles, poses, lighting.",
        "num_training_samples": 23410,
        "num_validation_samples": 0,
        "huggingface_url": "https://huggingface.co/datasets/microsoft/cats_vs_dogs",
        "num_classes": 2,
        "image_size": "variable",
        "task_type": ["image-classification"],
        "release_year": 2013,
        "dependent_packages": ["datasets", "PIL", "Python"],
        "code": 'from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset("microsoft/cats_vs_dogs")\n\n# Access a sample\nexample = dataset["train"][0]\nimage = example["image"]\nlabel = example["labels"]\n\n# Label mapping: 0 = cat, 1 = dog',
        "citation": "@InProceedings{asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization,\n  author = {Elson, Jeremy and Douceur, John (JD) and Howell, Jon and Saul, Jared},\n  title = {Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization},\n  booktitle = {Proceedings of 14th ACM Conference on Computer and Communications Security (CCS)},\n  year = {2007},\n  publisher = {Association for Computing Machinery, Inc.}\n}",
    },
    "Satellite-Building-Segmentation": {
        "description": "A dataset for building instance segmentation from satellite/aerial imagery. The dataset includes images from various locations captured under different conditions, annotated in COCO format for building detection and segmentation.",
        "num_training_samples": 6764,
        "num_validation_samples": 2901,
        "huggingface_url": "https://huggingface.co/datasets/keremberke/satellite-building-segmentation",
        "num_classes": 1,
        "image_size": "500x500",
        "task_type": ["instance-segmentation"],
        "release_year": 2023,
        "dependent_packages": ["datasets", "PIL", "numpy"],
        "code": "from datasets import load_dataset\n\n# Load the dataset\nds = load_dataset(\"keremberke/satellite-building-segmentation\", name=\"full\")\n\n# Access different splits\ntrain_data = ds['train']\nval_data = ds['validation']\ntest_data = ds['test']\n\n# Load a sample\nexample = train_data[0]\nimage = example['image']\nobjects = example['objects']",
        "citation": "@misc{buildings-instance-segmentation_dataset,\n  title = {Buildings Instance Segmentation Dataset},\n  type = {Open Source Dataset},\n  author = {Roboflow Universe Projects},\n  year = {2023},\n  publisher = {Roboflow}\n}",
    },
    "LoveDA": {
        "description": "A remote sensing land-cover dataset for domain adaptive semantic segmentation. The dataset contains high spatial resolution (0.3 m) remote sensing images from Nanjing, Changzhou, and Wuhan, encompassing two domains (urban and rural).",
        "num_training_samples": 2522,
        "num_validation_samples": 3465,
        "huggingface_url": "https://huggingface.co/datasets/chloechia/loveda",
        "num_classes": 7,
        "image_size": "1024x1024",
        "task_type": ["semantic-segmentation"],
        "release_year": 2021,
        "dependent_packages": ["datasets", "PIL", "numpy"],
        "code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"chloechia/loveda\")\n\n# Access training data\ntrain_data = dataset['train']\n\n# Load a sample\nsample = train_data[0]\nimage = sample['image']  # RGB image\nlabel = sample['label']  # Segmentation mask",
        "citation": "@inproceedings{NEURIPS_DATASETS_AND_BENCHMARKS2021_4e732ced,\n  author = {Wang, Junjue and Zheng, Zhuo and Ma, Ailong and Lu, Xiaoyan and Zhong, Yanfei},\n  title = {LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation},\n  booktitle = {NeurIPS Datasets and Benchmarks},\n  year = {2021}\n}",
    },
    "Teeth-Segmentation": {
        "description": "A medical imaging dataset for automatic semantic segmentation and measurement of total length of teeth in panoramic X-ray images. The dataset was created for dental diagnostic information for the management of dental disorders, diseases, and conditions.",
        "num_training_samples": 116,
        "num_validation_samples": 0,
        "huggingface_url": "https://huggingface.co/datasets/SerdarHelli/SegmentationOfTeethPanoramicXRayImages",
        "num_classes": 2,
        "image_size": "variable",
        "task_type": ["semantic-segmentation"],
        "release_year": 2022,
        "dependent_packages": ["datasets", "PIL", "pandas"],
        "code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"SerdarHelli/SegmentationOfTeethPanoramicXRayImages\")\n\n# Access training data\ntrain_data = dataset['train']\n\n# Load a sample\nsample = train_data[0]\nxray_image = sample['image']\nsegmentation_mask = sample['label']",
        "citation": "@article{helli10tooth,\n  title={Tooth Instance Segmentation on Panoramic Dental Radiographs Using U-Nets and Morphological Processing},\n  author={HELLI, Serdar and HAMAMCI, Andaç},\n  journal={Düzce Üniversitesi Bilim ve Teknoloji Dergisi},\n  volume={10},\n  number={1},\n  pages={39--50}\n}",
    },
    # "HMDB51": {
    #     "description": "HMDB51 is a large video database for human motion recognition collected from various sources including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories, with each category containing at least 101 clips.",
    #     "num_training_samples": 4737,
    #     "num_validation_samples": 2029,
    #     "huggingface_url": "https://huggingface.co/datasets/divm/hmdb51",
    #     "num_classes": 51,
    #     "image_size": "variable (320x240 to 560x240)",
    #     "task_type": "Action Recognition",
    #     "release_year": 2011,
    #     "dependent_packages": ["datasets", "pandas", "PIL", "torch"],
    #     "code": 'from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset("divm/hmdb51")\n\n# Access training data\ntrain_data = dataset["train"]\n\n# Get first video\nsample = train_data[0]\nprint(f"Video ID: {sample[\'video_id\']}")\nprint(f"Label: {sample[\'label\']}")',
    #     "citation": "@article{Kuehne2011HMDBAL,\n  title={HMDB: A Large Video Database for Human Motion Recognition},\n  author={Hilde Kuehne and Hueihan Jhuang and Estíbaliz Garrote and Tomaso Poggio and Thomas Serre},\n  journal={2011 International Conference on Computer Vision},\n  year={2011},\n  pages={2556-2563}\n}",
    # },
    "SceneParse150": {
        "description": "Scene parsing benchmark providing a standard training and evaluation platform for scene parsing algorithms. The dataset contains scene-centric images exhaustively annotated with 150 semantic categories including both stuff classes (wall, sky, road) and discrete objects (car, person, table).",
        "num_training_samples": 20210,
        "num_validation_samples": 5352,
        "huggingface_url": "https://huggingface.co/datasets/zhoubolei/scene_parse_150",
        "num_classes": 150,
        "image_size": "variable (min 512px)",
        "task_type": ["semantic-segmentation"],
        "release_year": 2017,
        "dependent_packages": ["datasets", "PIL", "numpy", "torch", "matplotlib"],
        "code": "from datasets import load_dataset\nimport numpy as np\n\n# Load the dataset\ndataset = load_dataset(\"zhoubolei/scene_parse_150\", split=\"train\")\n\n# Access a sample\nsample = dataset[10]\nimage = sample['image']\nscene_category = sample['scene_category']\nannotation_mask = np.array(sample['annotation'])",
        "citation": "@inproceedings{zhou2017scene,\n  title={Scene Parsing through ADE20K Dataset},\n  author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},\n  booktitle={CVPR},\n  year={2017}\n}",
    },
    # "Kinetics-400": {
    #     "description": "Kinetics-400 is a large-scale, high-quality dataset of YouTube video clips covering 400 human action classes. The dataset contains up to 650,000 video clips with human-object interactions and human-human interactions.",
    #     "num_training_samples": 240000,
    #     "num_validation_samples": 19881,
    #     "huggingface_url": "https://huggingface.co/datasets/liuhuanjim013/kinetics400",
    #     "num_classes": 400,
    #     "image_size": "variable video resolution",
    #     "task_type": "Action Recognition",
    #     "release_year": 2017,
    #     "dependent_packages": [
    #         "datasets",
    #         "torch",
    #         "transformers",
    #         "opencv-python",
    #     ],
    #     "code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"liuhuanjim013/kinetics400\")\n\n# Access the data\nfor video in dataset['train']:\n    print('Video ID: ' + video['video_id'])\n    print('Number of clips: ' + str(len(video['clips'])))\n    break",
    #     "citation": "@article{DBLP:journals/corr/KayCSZHVVGNSZ17,\n  author = {Will Kay and others},\n  title = {The Kinetics Human Action Video Dataset},\n  journal = {CoRR},\n  volume = {abs/1705.06950},\n  year = {2017}\n}",
    # },
    # "Something-Something-V2": {
    #     "description": "Something-Something dataset (version 2) is a large-scale collection of 220,847 labeled video clips of humans performing pre-defined, basic actions with everyday objects. Designed to train models in fine-grained understanding of human hand gestures and object interactions.",
    #     "num_training_samples": 168913,
    #     "num_validation_samples": 51934,
    #     "huggingface_url": "https://huggingface.co/datasets/HuggingFaceM4/something_something_v2",
    #     "num_classes": 174,
    #     "image_size": "variable (2-6 sec videos)",
    #     "task_type": "Action Recognition",
    #     "release_year": 2017,
    #     "dependent_packages": [
    #         "datasets",
    #         "torch",
    #         "transformers",
    #         "decord",
    #         "PIL",
    #     ],
    #     "code": 'from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset("HuggingFaceM4/something_something_v2")\n\n# Access training data\ntrain_data = dataset["train"]\n\n# Get first video\nsample = train_data[0]\nprint(f"Video ID: {sample[\'video_id\']}")\nprint(f"Text description: {sample[\'text\']}")\nprint(f"Label: {sample[\'label\']}")',
    #     "citation": '@inproceedings{goyal2017something,\n  title={The "something something" video database for learning and evaluating visual common sense},\n  author={Goyal, Raghav and others},\n  booktitle={ICCV},\n  pages={5842--5850},\n  year={2017}\n}',
    # },
    # "Human-Action-Recognition": {
    #     "description": "This dataset features 15 different classes of human activities captured in images. The dataset contains approximately 12,600+ labeled images for training and 5,400 images for testing. Each image depicts a single human activity category.",
    #     "num_training_samples": 12600,
    #     "num_validation_samples": 5400,
    #     "huggingface_url": "https://huggingface.co/datasets/Bingsu/Human_Action_Recognition",
    #     "num_classes": 15,
    #     "image_size": "variable (240x160 common)",
    #     "task_type": "Action Recognition",
    #     "release_year": 2023,
    #     "dependent_packages": [
    #         "datasets",
    #         "PIL",
    #         "torch",
    #         "torchvision",
    #         "numpy",
    #         "transformers",
    #     ],
    #     "code": 'from datasets import load_dataset\n\n# Load the dataset\nds = load_dataset("Bingsu/Human_Action_Recognition")\n\n# Access a sample\nsample = ds["train"][0]\nimage = sample[\'image\']\nlabel = sample[\'labels\']\n\nprint(f"Image size: {image.size}")\nprint(f"Label: {label}")',
    #     "citation": "@misc{human_action_recognition_dataset,\n  title={Human Action Recognition (HAR) Dataset},\n  author={Kaggle and DPhi},\n  year={2023},\n  note={Dataset for human activity recognition with 15 action classes}\n}",
    # },
    # "NIH-Chest-X-ray": {
    #     "description": "ChestX-ray dataset comprises 112,120 frontal-view X-ray images of 30,805 unique patients with text-mined fourteen disease image labels (multi-label classification). The fourteen common thoracic pathologies include: Atelectasis, Consolidation, Infiltration, Pneumothorax, Edema, Emphysema, Fibrosis, Effusion, Pneumonia, Pleural_thickening, Cardiomegaly, Nodule, Mass, and Hernia.",
    #     "num_training_samples": 86524,
    #     "num_validation_samples": 25596,
    #     "huggingface_url": "https://huggingface.co/datasets/alkzar90/NIH-Chest-X-ray-dataset",
    #     "num_classes": 15,
    #     "image_size": "1024x1024 (grayscale)",
    #     "task_type": "Multi-label Classification",
    #     "release_year": 2017,
    #     "dependent_packages": ["datasets", "PIL", "Python"],
    #     "code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"alkzar90/NIH-Chest-X-ray-dataset\")\n\n# Access examples\ntrain_example = dataset['train'][0]\nprint(train_example['image'])\nprint(train_example['labels'])\nprint(train_example['image_file_path'])",
    #     "citation": "@inproceedings{Wang_2017,\n  title = {{ChestX}-Ray8: Hospital-Scale Chest X-Ray Database and Benchmarks},\n  author = {Xiaosong Wang and Yifan Peng and Le Lu and Zhiyong Lu and others},\n  booktitle = {CVPR},\n  year = 2017\n}",
    # },
    "DocLayNet": {
        "description": "DocLayNet is a human-annotated document layout segmentation dataset containing 80,863 unique pages from 6 document categories. It provides page-by-page layout segmentation ground-truth using bounding-boxes for 11 distinct class labels from diverse and complex layouts.",
        "num_training_samples": 64690,
        "num_validation_samples": 16173,
        "huggingface_url": "https://huggingface.co/datasets/ds4sd/DocLayNet",
        "num_classes": 11,
        "image_size": "variable (high-resolution)",
        "task_type": ["object-detection"],
        "release_year": 2022,
        "dependent_packages": ["datasets", "PIL", "Python", "pycocotools"],
        "code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"ds4sd/DocLayNet\")\n\n# Access examples\nexample = dataset['train'][0]\nprint(example['image'])\nprint(example['annotations'])",
        "citation": "@article{doclaynet2022,\n  title = {DocLayNet: A Large Human-Annotated Dataset for Document-Layout Segmentation},\n  author = {Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S and Staar, Peter W J},\n  booktitle = {KDD},\n  year = {2022}\n}",
    },
    # "MedMNIST-v2": {
    #     "description": "MedMNIST v2 is a large-scale MNIST-like collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D. All images are pre-processed into 28 x 28 (2D) or 28 x 28 x 28 (3D) with corresponding classification labels. The total dataset consists of 708,069 2D images and 9,998 3D images.",
    #     "num_training_samples": 708069,
    #     "num_validation_samples": 0,
    #     "huggingface_url": "https://huggingface.co/datasets/albertvillanova/medmnist-v2",
    #     "num_classes": "varies by subset",
    #     "image_size": "28x28 (2D) or 28x28x28 (3D)",
    #     "task_type": "Multi-task Medical Image Classification",
    #     "release_year": 2023,
    #     "dependent_packages": ["datasets", "PIL", "Python", "numpy"],
    #     "code": "from datasets import load_dataset\n\n# Load the dataset (specify subset)\ndataset = load_dataset(\"albertvillanova/medmnist-v2\", name=\"pathmnist\")\n\n# Access examples\nexample = dataset['train'][0]\nprint(example['image'])  # 28x28 PIL Image\nprint(example['label'])  # Classification label",
    #     "citation": "@article{medmnistv2,\n  title={MedMNIST v2-A large-scale lightweight benchmark for 2D and 3D biomedical image classification},\n  author={Yang, Jiancheng and Shi, Rui and Wei, Donglai and Liu, Zequan and others},\n  journal={Scientific Data},\n  volume={10},\n  pages={41},\n  year={2023}\n}",
    # },
    # "DIV2K": {
    #     "description": "DIV2K is a high-quality image dataset for single-image super-resolution containing 2K resolution RGB images with large diversity of contents, including people, handmade objects, environments, flora and fauna, and natural sceneries.",
    #     "num_training_samples": 800,
    #     "num_validation_samples": 100,
    #     "huggingface_url": "https://huggingface.co/datasets/eugenesiow/Div2k",
    #     "num_classes": 0,
    #     "image_size": "2K (high-resolution)",
    #     "task_type": "Super-resolution",
    #     "release_year": 2017,
    #     "dependent_packages": ["datasets", "super_image", "PIL", "torch"],
    #     "code": "from datasets import load_dataset\nfrom super_image import EdsrModel\nfrom super_image.data import EvalDataset\n\n# Load the dataset\ndataset = load_dataset('eugenesiow/Div2k', 'bicubic_x2', split='validation')\neval_dataset = EvalDataset(dataset)\n\n# Load a pre-trained model\nmodel = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=2)",
    #     "citation": "@InProceedings{Agustsson_2017_CVPR_Workshops,\n  author = {Agustsson, Eirikur and Timofte, Radu},\n  title = {NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study},\n  booktitle = {CVPR Workshops},\n  year = {2017}\n}",
    # },
    "Flickr8k": {
        "description": "An image captioning dataset featuring images from Flickr with corresponding text captions. Each image is paired with 5 different captions describing the scene. The dataset contains various everyday activities and scenes, suitable for image-to-text generation and vision-language tasks.",
        "num_training_samples": 8000,
        "num_validation_samples": 0,
        "huggingface_url": "https://huggingface.co/datasets/ariG23498/flickr8k",
        "num_classes": 0,
        "image_size": "variable (164-500+ px)",
        "task_type": ["image-captioning"],
        "release_year": 2013,
        "dependent_packages": ["datasets", "PIL", "transformers", "torch"],
        "code": "from datasets import load_dataset\nimport pandas as pd\n\n# Load the dataset\ndataset = load_dataset(\"ariG23498/flickr8k\", split='train')\n\n# Access image and caption\nsample = dataset[0]\nimage = sample['image']\ncaption = sample['caption']\n\nprint(f\"Caption: {caption}\")\nimage.show()",
        "citation": "@inproceedings{hodosh2013framing,\n  title={Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics},\n  author={Hodosh, Micah and Young, Peter and Hockenmaier, Julia},\n  booktitle={JAIR},\n  year={2013}\n}",
    },
    # "Anime-Faces": {
    #     "description": "A dataset consisting of 21,551 anime faces scraped from www.getchu.com, which were then cropped using the anime face detection algorithm. All images are resized to 64×64 pixels for convenience. Designed for training generative models on anime-style face generation.",
    #     "num_training_samples": 21551,
    #     "num_validation_samples": 0,
    #     "huggingface_url": "https://huggingface.co/datasets/huggan/anime-faces",
    #     "num_classes": 0,
    #     "image_size": "64x64",
    #     "task_type": "Unconditional Image Generation",
    #     "release_year": 2017,
    #     "dependent_packages": ["datasets", "PIL", "torch"],
    #     "code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"huggan/anime-faces\")\n\n# Access an image\nimage = dataset['train'][0]['image']\n\n# Display image\nimage.show()\n\n# For GAN training\nfrom huggingface_hub import from_pretrained_keras\nmodel = from_pretrained_keras(\"merve/anime-faces-generator\")",
    #     "citation": "@misc{anime_faces_dataset,\n  title={Anime-Faces Dataset},\n  author={Getchu.com and nagadomi},\n  note={Scraped from www.getchu.com and processed with lbpcascade_animeface},\n  year={2017}\n}",
    # },
}
