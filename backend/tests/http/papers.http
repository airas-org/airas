### Search paper by queries (airas_db)

POST http://127.0.0.1:8000/airas/v1/papers/search
Content-Type: application/json

{
    "search_method": "airas_db",
    "queries": ["Diffusion model", "Optimizer"],
    "max_results_per_query": 3
}

### Search paper by queries (Qdrant)

POST http://127.0.0.1:8000/airas/v1/papers/search
Content-Type: application/json

{
    "search_method": "qdrant",
    "queries": ["Finetuning large language models (LLMs) has been empirically effective on a variety of downstream tasks. Existing approaches to finetuning an LLM either focus on parameter-efficient finetuning, which only updates a small number of trainable parameters, or attempt to reduce the memory footprint during the training phase of the finetuning. Typically, the memory footprint during finetuning stems from three contributors: model weights, optimizer states, and intermediate activations. However, existing works still require considerable memory, and none can simultaneously mitigate the memory footprint of all three sources. In this paper, we present quantized side tuing (QST), which enables memory-efficient and fast finetuning of LLMs by operating through a dual-stage process. First, QST quantizes an LLM’s model weights into 4-bit to reduce the memory footprint of the LLM’s original weights. Second, QST introduces a side network separated from the LLM, which utilizes the hidden states of the LLM to make task-specific predictions. Using a separate side network avoids performing back-propagation through the LLM, thus reducing the memory requirement of the intermediate activations. Finally, QST leverages several low-rank adaptors and gradient-free downsample modules to significantly reduce the trainable parameters, so as to save the memory footprint of the optimizer states. Experiments show that QST can reduce the total memory footprint by up to 2.3× and speed up the finetuning process by up to 3\\times while achieving competent performance compared with the state-of-the-art. When it comes to full finetuning, QST can reduce the total memory footprint up to 7\\times."],
    "max_results_per_query": 3,
    "collection_name": "airas_papers_db"
}

### Retrieve papers by titles

POST http://127.0.0.1:8000/airas/v1/papers/retrieval
Content-Type: application/json

{
    "paper_titles": ["Autoregressive Diffusion Models", "Blurring Diffusion Models", "Matryoshka Diffusion Models"]
}

### Generate paper from research data

POST http://127.0.0.1:8000/airas/v1/papers/generations
Content-Type: application/json

{
    "research_hypothesis": {
        "open_problems": "Traditional SGD converges slowly on deep networks with complex loss landscapes",
        "method": "Apply adaptive learning rate scheduling with momentum to dynamically adjust step sizes during training",
        "experimental_setup": "Train ResNet-18 and VGG-16 on CIFAR-10 and CIFAR-100 datasets using various optimizers",
        "primary_metric": "Test accuracy on held-out test set",
        "experimental_code": "PyTorch implementation with torchvision models and standard data augmentation",
        "expected_result": "AdamW with cosine annealing will achieve higher test accuracy and faster convergence than baseline SGD",
        "expected_conclusion": "Adaptive learning rate methods provide better optimization dynamics for deep neural network training"
    },
    "experimental_design": {
        "experiment_summary": "Compare adaptive learning rate methods (Adam, AdamW) against SGD with momentum on image classification tasks",
        "runner_config": {
            "runner_label": ["ubuntu-latest"],
            "description": "Standard GitHub Actions runner with 2-core CPU and 7 GB RAM"
        },
        "evaluation_metrics": [
            {
                "name": "Test Accuracy",
                "description": "Classification accuracy on test set, measured as (correct predictions / total examples)"
            }
        ],
        "models_to_use": ["ResNet-18", "VGG-16"],
        "datasets_to_use": ["CIFAR-10", "CIFAR-100"],
        "proposed_method": {
            "method_name": "AdamW-Cosine",
            "description": "AdamW optimizer with cosine annealing learning rate schedule"
        },
        "comparative_methods": [
            {
                "method_name": "SGD-Momentum",
                "description": "Standard SGD with momentum=0.9"
            }
        ]
    },
    "experiment_code": {
        "train_py": "import torch\nimport torch.nn as nn\n# Training code here",
        "evaluate_py": "import torch\n# Evaluation code here",
        "preprocess_py": "# Data preprocessing code",
        "model_py": "import torch.nn as nn\n# Model definition",
        "main_py": "# Main experiment entry point",
        "pyproject_toml": "[project]\nname = \"optimizer-comparison\"\nversion = \"0.1.0\"",
        "config_yaml": "learning_rate: 0.001\nbatch_size: 128"
    },
    "experimental_results": {
        "stdout": "Epoch 1/100: Train Loss: 1.234, Test Acc: 85.2%\nEpoch 100/100: Train Loss: 0.234, Test Acc: 92.5%",
        "stderr": "",
        "figures": ["accuracy_plot.png", "loss_curve.png"],
        "metrics_data": {
            "adamw": {"test_accuracy": 0.925, "convergence_epoch": 75},
            "sgd": {"test_accuracy": 0.902, "convergence_epoch": 95}
        }
    },
    "experimental_analysis": {
        "analysis_report": "The proposed AdamW method shows faster convergence and higher final accuracy compared to baseline SGD. AdamW achieved 92.5% accuracy on CIFAR-10, converging at epoch 75, while SGD reached only 90.2% accuracy and required 95 epochs to converge."
    },
    "research_study_list": [
        {
            "title": "Adam: A Method for Stochastic Optimization",
            "full_text": "We introduce Adam, an algorithm for first-order gradient-based optimization...",
            "references": [],
            "meta_data": {
                "arxiv_id": "1412.6980",
                "authors": ["Diederik P. Kingma", "Jimmy Ba"],
                "published_date": "2014-12-22",
                "venue": "ICLR",
                "pdf_url": "https://arxiv.org/pdf/1412.6980.pdf"
            },
            "llm_extracted_info": {
                "main_contributions": "Introduced Adam optimizer",
                "methodology": "Adaptive learning rates"
            }
        }
    ],
    "references_bib": "@article{kingma-2014-adam,\n  title={Adam: A Method for Stochastic Optimization},\n  author={Kingma, Diederik P. and Ba, Jimmy},\n  journal={arXiv preprint arXiv:1412.6980},\n  year={2014}\n}",
    "writing_refinement_rounds": 2
}
