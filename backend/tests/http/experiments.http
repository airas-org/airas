### Fetch run IDs from repository
POST http://127.0.0.1:8000/airas/v1/experiments/run-ids
Content-Type: application/json

# NOTE: The repo or branch name may contain confidential company information, 
# so it was included in the request body as a POST method.
{
    "github_config": {
        "github_owner": "auto-res2",
        "repository_name": "your-repo",
        "branch_name": "your-branch"
    }
}

###

### Fetch experimental results
POST http://127.0.0.1:8000/airas/v1/experiments/results
Content-Type: application/json

{
    "github_config": {
        "github_owner": "auto-res2",
        "repository_name": "your-repo",
        "branch_name": "your-branch"
    }
}

###

### Dispatch sanity check
POST http://127.0.0.1:8000/airas/v1/experiments/sanity-checks/dispatch
Content-Type: application/json

{
    "github_config": {
        "github_owner": "auto-res2",
        "repository_name": "matsuzawa-2026-02-10",
        "branch_name": "main"
    },
    "run_id": "proposed"
}

###

### Dispatch experiment validation
POST http://127.0.0.1:8000/airas/v1/experiments/validations/dispatch
Content-Type: application/json

{
    "github_config": {
        "github_owner": "auto-res2",
        "repository_name": "matsuzawa-2026-02-10",
        "branch_name": "main"
    },
    "research_topic": "Improving Chain-of-Thought reasoning with cycle-consistent grounding",
    "run_id": "proposed",
    "workflow_run_id": 21867927697,
    "run_stage": "sanity",
    "github_actions_agent": "open_code",
    "llm_mapping": {
        "dispatch_experiment_validation": {
            "llm_name": "anthropic/claude-haiku-4-5"
        }
    },
    "research_hypothesis": {
        "open_problems": "Auto-CoT-style pipelines still lack a label-free way to detect \"plausible but ungrounded\" demonstrations",
        "method": "Cycle-Consistent & Paraphrase-Invariant Reliability Auto-CoT (C3-AutoCoT)",
        "experimental_setup": "Tasks/Datasets: SVAMP, Models: google/flan-t5-base",
        "primary_metric": "accuracy",
        "experimental_code": "import re, random\nimport numpy as np",
        "expected_result": "On SVAMP with flan-t5-base: 0.30â€“0.40 accuracy",
        "expected_conclusion": "C3-AutoCoT strengthens Auto-CoT with triple-check verification"
    },
    "experimental_design": {
        "experiment_summary": "Task: Solve elementary arithmetic word problems by generating CoT rationale",
        "evaluation_metrics": [
            {
                "name": "accuracy",
                "description": "End-task correctness"
            }
        ],
        "models_to_use": ["Qwen3-8B (8B parameters)"],
        "datasets_to_use": ["SVAMP (MU-NLPC/Calc-svamp)"],
        "proposed_method": {
            "method_name": "C3-AutoCoT",
            "description": "Inference-only enhancement to Auto-CoT demonstration construction"
        },
        "comparative_methods": [
            {
                "method_name": "PIR-AutoCoT",
                "description": "Baseline that filters demos based on self-consistency and paraphrase invariance"
            }
        ]
    },
    "wandb_config": {
        "entity": "airas",
        "project": "2026-02-10-test"
    }
}

###

### Dispatch main experiment
POST http://127.0.0.1:8000/airas/v1/experiments/main-runs/dispatch
Content-Type: application/json

{
    "github_config": {
        "github_owner": "auto-res2",
        "repository_name": "matsuzawa-2026-02-10",
        "branch_name": "main"
    },
    "run_id": "proposed"
}

###

### Dispatch visualization
POST http://127.0.0.1:8000/airas/v1/experiments/visualizations/dispatch
Content-Type: application/json

{
    "github_config": {
        "github_owner": "auto-res2",
        "repository_name": "matsuzawa-2026-02-10",
        "branch_name": "main"
    },
    "run_ids": [
        "proposed",
        "comparative-1"
    ]
}

###

### Analyze experiment
POST http://127.0.0.1:8000/airas/v1/experiments/analyses
Content-Type: application/json

{
    "research_hypothesis": {
        "open_problems": "Current NLP models struggle with long-context understanding",
        "method": "Propose a hierarchical attention mechanism to improve long-context processing",
        "experimental_setup": "Compare the proposed method against standard Transformer on long document classification tasks",
        "primary_metric": "Accuracy on documents longer than 2000 tokens",
        "experimental_code": "Implement hierarchical attention in PyTorch and evaluate on IMDb and Yelp datasets",
        "expected_result": "Improved accuracy by 3-5% on long documents compared to baseline",
        "expected_conclusion": "Hierarchical attention effectively captures long-range dependencies"
    },
    "experimental_design": {
        "experiment_summary": "Evaluate hierarchical attention mechanism on long document classification",
        "evaluation_metrics": [
            {
                "name": "Accuracy",
                "description": "Classification accuracy on test set, measuring percentage of correctly classified documents"
            },
            {
                "name": "F1 Score",
                "description": "Macro F1 score across all classes to handle potential class imbalance"
            }
        ],
        "models_to_use": ["bert-base-uncased", "roberta-base"],
        "datasets_to_use": ["imdb", "yelp_polarity"],
        "proposed_method": {
            "method_name": "HierarchicalAttention",
            "description": "Hierarchical attention mechanism with chunk-level and document-level attention",
            "training_config": {
                "learning_rate": 2e-5,
                "batch_size": 16,
                "epochs": 5,
                "optimizer": "adamw",
                "warmup_steps": 500,
                "weight_decay": 0.01
            }
        },
        "comparative_methods": [
            {
                "method_name": "StandardTransformer",
                "description": "Standard Transformer baseline without hierarchical mechanism",
                "training_config": {
                    "learning_rate": 2e-5,
                    "batch_size": 16,
                    "epochs": 5,
                    "optimizer": "adamw"
                }
            }
        ]
    },
    "experiment_code": {
        "train_py": "# Training script placeholder",
        "evaluate_py": "# Evaluation script placeholder",
        "preprocess_py": "# Preprocessing script placeholder",
        "model_py": "# Model definition placeholder",
        "main_py": "# Main execution script placeholder",
        "pyproject_toml": "# Project configuration placeholder",
        "config_yaml": "# Config placeholder"
    },
    "experimental_results": {
        "metrics_data": {
            "HierarchicalAttention_imdb": {
                "accuracy": 0.892,
                "f1_score": 0.888
            },
            "StandardTransformer_imdb": {
                "accuracy": 0.854,
                "f1_score": 0.851
            },
            "HierarchicalAttention_yelp": {
                "accuracy": 0.901,
                "f1_score": 0.898
            },
            "StandardTransformer_yelp": {
                "accuracy": 0.867,
                "f1_score": 0.863
            }
        },
        "figures": ["confusion_matrix.png", "attention_heatmap.png"]
    }
}

###
