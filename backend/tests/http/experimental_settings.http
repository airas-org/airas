
POST http://127.0.0.1:8000/airas/v1/experimental_settings/generations
Content-Type: application/json

{
    "research_hypothesis": {
        "open_problems": "LLM-driven black-box prompt search (conversational hill-climb) still suffers from several unresolved issues that limit practical adoption: (1) overfitting to tiny few-shot folds and large generalization gaps, often driven by verbose, dataset-tailored templates; (2) search stagnation where many proposals are trivial lexical variants of the current best prompts, wasting API calls; (3) sensitivity to hand-tuned regularization coefficients (length/diversity weights) and to the choice of lexical similarity metric (token-set Jaccard misses semantic redundancies); (4) inability to detect and penalize unstable / high-variance candidates that perform well on one subsample but fail on others; (5) cost/energy overhead from excessive LLM evaluations for low-value proposals. These problems suggest we need an adjusted black-box objective that (a) captures semantic novelty (not just lexical difference), (b) quantifies candidate stability, and (c) adapts its regularization strengths online to reduce hyperparameter tuning and API cost — while remaining fully black-box and lightweight enough to validate with a Python script.",
        "method": "Regularized, Uncertainty-aware, and Adaptive Black-Box Prompt Search (RUA-BBPS).\n\nCore idea: replace the scalar fitness used to rank/elect proposals with an adjusted score that combines (i) accuracy, (ii) a compactness penalty, (iii) a semantic-novelty penalty computed via cheap MinHash-based similarity on n-gram shingles (captures semantic/phrase overlap while remaining dependency-light), and (iv) an uncertainty penalty estimated via small bootstrap resampling on the few-shot training fold (penalizes high-variance prompts that likely overfit). In addition, make the regularization coefficients adaptive: treat lambda_len, mu_sim, and gamma_unc (uncertainty weight) as online tunable weights updated with a simple bandit-like success signal that uses a small validation budget (occasional held-out calls) to favor coefficient settings that yield better held-out performance. This reduces sensitivity to manual tuning and keeps API cost minimal by limiting validation checks to a fixed small budget per restart.\n\nConcretely, adjusted score S_adj = acc_train - lambda_len * LenNorm(prompt) - mu_sim * MinHashSimMax(prompt, TopKPool) - gamma_unc * BootstrapStd(prompt).\n\nNovel operational contributions beyond prior work:\n- Use MinHash (n-gram shingle hashing) for efficient semantic-similarity estimation that is cheaper than embedding-based similarity and more semantically robust than token-set Jaccard.\n- Add an uncertainty penalty derived from bootstrap resampling of the few-shot train fold to detect overfit candidates early in search.\n- Introduce an online adaptive coefficient updater (budgeted validation + multiplicative weight update) to automatically calibrate regularization strengths per dataset/restart, reducing the need for expensive hyperparameter sweeps.\n- Keep the rest of the conversational LLM proposal mechanism unchanged, preserving black-box constraints and low engineering friction.\n\nFeasibility: all elements (MinHash, simple bootstrapping, bandit update) are implementable in plain Python and require no access to model internals or heavy external dependencies.",
        "experimental_setup": "Datasets: use the CoOp few-shot protocol; validate on a small subset for rapid iteration: Caltech101, Oxford Pets, Food101. Use k in {1,4} (1-shot for high variance, 4-shot to show stability improvements). Use the same 3-fold splits if available.\n\nVLM and LLM: CLIP (RN50) as the VLM evaluation function (local inference returning accuracy on a given subset). LLM proposer: gpt-3.5-turbo for production-like runs; provide a reproducible local-mode option by replacing the LLM with a small local LM/mock proposer to allow fully offline validation.\n\nBaselines:\n- Original hill_climb_with_llm (raw accuracy sorting) — P+N variant.\n- RBBPS (static lexical Jaccard + length penalty) as described in the starting hypothesis.\n- Ablations of RUA-BBPS: (i) no uncertainty penalty, (ii) lexical Jaccard instead of MinHash, (iii) fixed coefficients vs adaptive coefficients.\n\nHyperparameters and adaptation:\n- MinHash: shingle size n=3, signature size k_sig=64.\n- Bootstrap repeats B=20 for an inexpensive std estimate on the few-shot train fold.\n- Validation budget: one small held-out validation call per restart every T iterations (e.g., T=3) or a fixed budget of V_eval=5 validation calls per restart used by the multiplicative weight updater.\n- Adaptive updater: maintain a discrete set of candidate coefficient tuples (small grid) and use exponential weights updated by observed held-out returns; or use multiplicative factor updates on continuous weights using success/failure signals.\n\nEvaluation metrics:\n- Primary: held-out top-1 accuracy on the test fold(s) (same metric used in CoOp benchmarks).\n- Secondary: average prompt length (tokens), semantic diversity of top-K (MinHash-Jaccard estimate), bootstrap-based stability (mean BootstrapStd across top-K), total LLM API calls and token cost, and variation of final accuracy across restarts (std dev).\n\nProtocol: run each method for nrestart in {5,20} (low-cost and full), m=100, niter=10 (reduce for quick tests to nrestart=5, m=50, niter=5). For adaptive runs use the small validation budget. Report mean and std across restarts and dataset folds.",
        "primary_metric": "held-out top-1 accuracy (averaged across dataset folds and restarts)",
        "experimental_code": "import re\nimport random\nimport hashlib\nimport math\nfrom collections import Counter, defaultdict\n\n# ---------- Tokenization & shingle-based MinHash utilities ----------\n\ndef tokenize(prompt):\n    return re.findall(r\"\\w+\", prompt.lower())\n\ndef ngrams(tokens, n=3):\n    return [\" \".join(tokens[i:i+n]) for i in range(max(0, len(tokens)-n+1))]\n\ndef minhash_signature(tokens, n=3, k_sig=64):\n    # simple deterministic MinHash via multiple hash functions simulated by seed mixing\n    shingles = ngrams(tokens, n=n)\n    if not shingles:\n        return tuple([2**64-1]*k_sig)\n    sig = []\n    for i in range(k_sig):\n        minh = 2**64-1\n        for s in shingles:\n            h = int(hashlib.sha256((s + \"|\" + str(i)).encode()).hexdigest()[:16], 16)\n            if h < minh:\n                minh = h\n        sig.append(minh)\n    return tuple(sig)\n\ndef minhash_jaccard(sig_a, sig_b):\n    # estimate Jaccard via signature equality fraction\n    assert len(sig_a) == len(sig_b)\n    match = sum(1 for a,b in zip(sig_a, sig_b) if a==b)\n    return match / float(len(sig_a))\n\n# ---------- Length normalization ----------\n\ndef length_norm(prompt, max_len=40):\n    toks = tokenize(prompt)\n    return min(len(toks) / float(max_len), 1.0)\n\n# ---------- Bootstrap-based uncertainty estimator ----------\n\ndef bootstrap_std_estimate(vlm_eval_on_indexed_subset_fn, prompt, indices, B=20, sample_frac=0.7):\n    # vlm_eval_on_indexed_subset_fn(prompt, indices_subset) -> accuracy in [0,1]\n    if len(indices) == 0:\n        return 0.0\n    vals = []\n    for _ in range(B):\n        subset = [indices[i] for i in random.choices(range(len(indices)), k=max(1,int(len(indices)*sample_frac)))]\n        vals.append(vlm_eval_on_indexed_subset_fn(prompt, subset))\n    mean = sum(vals)/len(vals)\n    var = sum((v-mean)**2 for v in vals)/len(vals)\n    return math.sqrt(var)\n\n# ---------- Adjusted score and adaptive coefficient updater ----------\n\ndef evaluate_adjusted(prompt, vlm_eval_fn_on_full_train, vlm_eval_on_subset, topk_signatures, indices_train,\n                      lambda_len=0.05, mu_sim=0.3, gamma_unc=0.5, max_len=40,\n                      minhash_n=3, k_sig=64, B_boot=20):\n    # vlm_eval_fn_on_full_train(prompt) -> scalar acc in [0,1]\n    acc = vlm_eval_fn_on_full_train(prompt)\n    ln = length_norm(prompt, max_len=max_len)\n    toks = tokenize(prompt)\n    sig = minhash_signature(toks, n=minhash_n, k_sig=k_sig)\n    # semantic similarity: max similarity to top-k signatures\n    max_sim = 0.0\n    for s in topk_signatures:\n        sim = minhash_jaccard(sig, s)\n        if sim > max_sim:\n            max_sim = sim\n    # uncertainty via bootstrap std on train indices\n    unc = bootstrap_std_estimate(vlm_eval_on_subset, prompt, indices_train, B=B_boot)\n    adj = acc - lambda_len * ln - mu_sim * max_sim - gamma_unc * unc\n    return {\n        'acc': acc,\n        'adj': adj,\n        'len': len(toks),\n        'max_sim': max_sim,\n        'unc': unc,\n        'sig': sig\n    }\n\nclass AdaptiveCoeffUpdater:\n    \"\"\"Simple multiplicative weight updater across a small discrete grid of coefficient tuples.\n    Uses a small validation budget: evaluate top candidate(s) on held-out val and update weights.\n    \"\"\"\n    def __init__(self, grid, eta=0.2):\n        # grid: list of (lambda_len, mu_sim, gamma_unc)\n        self.grid = grid\n        self.weights = [1.0 for _ in grid]\n        self.eta = eta\n\n    def sample(self):\n        # sample an index proportionally to weights\n        total = sum(self.weights)\n        r = random.random() * total\n        cum = 0.0\n        for i,w in enumerate(self.weights):\n            cum += w\n            if r <= cum:\n                return i, self.grid[i]\n        return len(self.grid)-1, self.grid[-1]\n\n    def update(self, idx, reward):\n        # reward in [0,1] (higher is better). multiplicative update\n        self.weights[idx] *= math.exp(self.eta * reward)\n\n# ---------- Integration into hill-climb loop (scoring part only) ----------\n\ndef update_pool_with_ruabpps(sorted_prompts, proposals, scores_cache, sig_cache, vlm_eval_full,\n                             vlm_eval_on_subset, indices_train, lambda_len, mu_sim, gamma_unc,\n                             top_k=15, k_sig=64):\n    # sorted_prompts: current list of prompts (strings) sorted by previous adj score\n    # proposals: list of candidate prompts\n    # scores_cache: map prompt->raw acc (if computed)\n    topk = sorted_prompts[:top_k]\n    topk_sigs = [sig_cache.get(t) or minhash_signature(tokenize(t), k_sig=k_sig) for t in topk]\n\n    for p in proposals:\n        if p in scores_cache:\n            raw = scores_cache[p]\n        else:\n            raw = vlm_eval_full(p)\n            scores_cache[p] = raw\n        info = evaluate_adjusted(p, vlm_eval_full, vlm_eval_on_subset, topk_sigs, indices_train,\n                                 lambda_len=lambda_len, mu_sim=mu_sim, gamma_unc=gamma_unc, k_sig=k_sig)\n        scores_cache[p + \"__adj\"] = info['adj']\n        sig_cache[p] = info['sig']\n\n    combined = list(dict.fromkeys(sorted_prompts + proposals))\n    combined_sorted = sorted(combined, key=lambda t: scores_cache.get(t + \"__adj\", scores_cache.get(t, 0.0)), reverse=True)\n    return combined_sorted\n\n# ---------- Notes on expected integration ----------\n# - vlm_eval_full(prompt) should evaluate the prompt on the few-shot training fold and return accuracy in [0,1].\n# - vlm_eval_on_subset(prompt, indices_subset) should evaluate accuracy of prompt on the subset of training indices (for bootstrap).\n# - The adaptive updater is used outside this snippet: periodically (budgeted), top candidate from current pool is evaluated on held-out val; the updater updates weights for coefficient tuples based on val accuracy; subsequently the loop samples new coefficient tuple for the next block of iterations.\n\n# The above functions provide a complete, dependency-light foundation to validate RUA-BBPS in Python.\n",
        "expected_result": "We expect RUA-BBPS to deliver consistent improvements in held-out top-1 accuracy and robustness relative to (a) the original raw-accuracy hill-climb and (b) the static RBBPS described earlier. Specific expectations (averaged across selected datasets and folds):\n- Absolute held-out accuracy gain: +1.0 to +3.0 percentage points over raw-accuracy baseline (larger gains in very-low-shot k=1 settings where instability is pronounced).\n- Stability: reduced standard deviation across restarts (fewer runs stuck in poor local optima) by ~20–40% thanks to the uncertainty penalty discouraging brittle prompts.\n- Prompt cost: average prompt length reduced by ~15–30% (compact templates), lowering per-call token costs.\n- Diversity: semantic diversity of top-15 (MinHash-Jaccard mean) improved relative to raw baseline and lexical Jaccard variant (less stagnation on trivial variants).\n- Hyperparameter robustness: adaptive coefficient updater reduces the need for manual grid sweeps; adaptive runs should match or beat the best static-tuned runs while using fewer validation evaluations.\n- API cost: modest net reduction in LLM calls per effective improvement because fewer trivial proposals are retained and fewer restarts are needed to find good prompts.\n\nThese improvements are conservative, achievable with the budgeted bootstrap and MinHash approaches described, and are measurable with a Python evaluation harness.",
        "expected_conclusion": "Incorporating semantic novelty (MinHash-based), bootstrapped uncertainty, and adaptive regularization into the black-box LLM-in-the-loop prompt search objective materially improves generalization, search efficiency, and robustness compared to raw-accuracy ranking and to simple static regularization. The proposed approach preserves the core advantages of the original conversational hill-climb pipeline (no access to model internals, interpretable natural-language prompts, easy deployment) while addressing its main practical failure modes: overfitting, verbosity-driven token costs, and stagnation on trivial variants. Importantly, the adaptive component reduces manual tuning and makes the method broadly applicable across datasets and few-shot regimes. Because the design is lightweight and fully implementable in Python with local CLIP inference and budgeted validation, the hypothesis is empirically testable and practically relevant for researchers and practitioners seeking lower-cost, more reliable black-box prompt optimization."
    },
    "runner_config": {
        "runner_label": ["H200 144GM×1"],
        "description": "NVIDIA H200×1\n VRAM: 144GB×1\n RAM： 2048 GB or more"
    },
    "num_models_to_use": 2,
    "num_datasets_to_use": 2,
    "num_comparative_methods": 2
}
