### Search paper by title

POST http://127.0.0.1:8000/airas/v1/code/generations
Content-Type: application/json

{
    "experimental_design": {
        "experiment_summary": "Task: perform black-box, LLM-driven prompt search (conversational hill-climb) to discover compact, generalizable few-shot prompts for visual classification tasks (CoOp-style few-shot evaluation). The system should propose prompt templates via an LLM proposer, evaluate them with a VLM (CLIP RN50) on a few-shot training fold, and maintain a pool of prompts ordered by an adjusted black-box objective. Purpose: validate that RUA-BBPS (Regularized, Uncertainty-aware, Adaptive Black-Box Prompt Search) produces prompts that generalize better, are shorter, less redundant, and more stable than baseline hill-climb and static-regularization variants while using modest LLM evaluation budget.\n\nComponents and workflow:\n- Proposer: conversational LLM (production: gpt-3.5-turbo; local/offline: a small local LM or mock proposer) generates prompt proposals given current pool and feedback.\n- Evaluator (VLM): CLIP (RN50) runs local inference to compute per-prompt top-1 accuracy on specified few-shot folds.\n- Adjusted objective S_adj = acc_train - lambda_len * LenNorm(prompt) - mu_sim * MinHashSimMax(prompt, TopKPool) - gamma_unc * BootstrapStd(prompt).\n  - MinHash: n-gram (n=3) shingle MinHash signature (k_sig=64) for cheap semantic similarity estimation.\n  - Uncertainty: bootstrap B=20 resamples of the few-shot train fold to estimate prompt accuracy std and penalize high-variance candidates.\n  - Adaptive coefficients: an online multiplicative-weight updater maintains a small discrete grid of (lambda_len, mu_sim, gamma_unc) tuples and uses a tiny held-out validation budget (V_eval <= 5 per restart) to update weights and bias future scoring toward coefficient settings that yield better held-out performance.\n- Hill-climb loop: unchanged conversational proposal mechanism; scoring/sorting of proposals uses S_adj. Periodically (budgeted) evaluate top candidates on held-out validation subset to update coefficient weights.\n- Reporting: primary metric is held-out top-1 accuracy (averaged across dataset folds and restarts). Secondary metrics: prompt length, semantic diversity (MinHash-Jaccard), bootstrap-based stability (avg std), total LLM API calls and estimated token cost, and std dev of final accuracy across restarts.\n\nScaling to Runner environment (budget-aware adjustments):\n- Because the provided Runner resource is limited, the default full-scale protocol is reduced for Runner experiments to fit compute and API budgets: use nrestart=5, iteration budget niter=5, proposal pool size m=50 per restart, TopKPool=10, and V_eval=3 validation calls per restart. Datasets are restricted to small class subsets (e.g., 30 classes sampled from each dataset) and use k in {1,4} shots. Local-mode runs (mock proposer + CLIP) are recommended to fully validate the pipeline without external API calls. These settings keep local CPU/GPU inference and LLM-call budgets small while preserving meaningful comparisons between methods.",
        "runner_config": {
        "runner_label": [],
        "description": ""
        },
        "evaluation_metrics": [
        {
            "name": "held-out top-1 accuracy (averaged across dataset folds and restarts)",
            "description": "Correctness criteria: a VLM prediction for an image is correct if its top-scoring class (given the prompt-derived textual classifier) equals the ground-truth label. For per-run accuracy, compute fraction of correct predictions over held-out test images. Calculation method: for each restart r and dataset fold f, compute acc_{r,f} = (#correct_predictions_on_test_{r,f}) / (N_test_{f}). Then aggregate: mean_acc = mean_{r,f}(acc_{r,f}) and report std_acc = std_{r,f}(acc_{r,f}). Task appropriateness: top-1 accuracy is the standard CoOp / visual classification metric and directly measures prompt effectiveness for classification. Relevant visualizations: bar plot of mean accuracy with error bars (±1 std) across methods; line chart of mean accuracy vs number of hill-climb iterations; boxplot of per-restart accuracies per method to show distribution."
        },
        {
            "name": "average prompt length (tokens)",
            "description": "Correctness criteria: measured prompt length equals token count after whitespace/word-tokenization (simple regex word tokens). Lower is better if accuracy is similar. Calculation method: for a set of top-K prompts returned by a run, compute mean_len = (1/K) * sum_i token_count(prompt_i). Report mean and std across restarts. Task appropriateness: penalizes verbose templates that increase token cost and latency. Relevant visualizations: histogram of prompt lengths; scatter plot of prompt length vs held-out accuracy to show any tradeoff."
        },
        {
            "name": "semantic diversity of top-K (Mean MinHash-Jaccard)",
            "description": "Correctness criteria: uses MinHash signatures (n=3 shingles, k_sig=64) to estimate pairwise semantic overlap. Calculation method: for top-K prompts in a run, compute pairwise MinHash-Jaccard estimate J(i,j) = fraction of equal signature lanes; compute mean_pairwise = (2/(K*(K-1))) * sum_{i<j} (1 - J(i,j)) as average semantic dissimilarity (higher means more diverse). Task appropriateness: captures semantic/phrase-level novelty better than token-set Jaccard and is cheap to compute. Relevant visualizations: heatmap of pairwise MinHash-Jaccard among top-K prompts; bar chart comparing mean diversity across methods."
        },
        {
            "name": "bootstrap-based stability (mean BootstrapStd across top-K)",
            "description": "Correctness criteria: bootstrap std is computed via B=20 resamples of the train few-shot fold and is interpreted as an uncertainty estimate for a prompt's train accuracy. Calculation method: for each prompt compute unc = sqrt(var_of_bootstrap_accuracies). For the top-K prompts take mean_unc = (1/K) * sum unc_i, then report mean and std across restarts. Task appropriateness: identifies brittle prompts that overfit tiny training folds; penalizing high-unc candidates improves generalization. Relevant visualizations: violin plot of bootstrap std distribution for top-K prompts; line chart of mean bootstrap std vs iteration."
        },
        {
            "name": "total LLM API calls and estimated token cost",
            "description": "Correctness criteria: counts the number of external LLM proposer calls and estimates tokens per call (if available) to compute approximate monetary/compute cost. Calculation method: total_calls = sum calls across restart; token_estimate = sum estimated input+output tokens across calls. Use a per-1k-token cost multiplier if converting to USD. Task appropriateness: directly measures method cost and efficiency, critical for black-box LLM-in-the-loop approaches. Relevant visualizations: stacked bar chart of calls and token cost per method; cumulative calls vs iteration."
        },
        {
            "name": "variation of final accuracy across restarts (std dev)",
            "description": "Correctness criteria: standard deviation of final held-out accuracies across restarts measures robustness to random seeds/proposer stochasticity. Calculation method: compute final_acc_r for each restart; report std_final = std_r(final_acc_r). Task appropriateness: indicates method reliability and propensity to get stuck in poor local optima. Relevant visualizations: boxplot of final accuracies across restarts; histogram of final_accuracy values."
        }
        ],
        "models_to_use": [
        "CLIP RN50 (VLM, ~100M parameters) - used as the local visual encoder/classifier evaluator",
        "gpt-3.5-turbo (LLM proposer, ~ChatGPT-class model; black-box API) - used as the production prompt proposer (local/mock proposer option available for Runner)"
        ],
        "datasets_to_use": [
        "Caltech101 (few-shot subset; sample up to 30 classes for Runner-scale runs)",
        "Oxford-IIIT Pet (few-shot subset; sample up to 30 classes for Runner-scale runs)"
        ],
        "proposed_method": {
        "method_name": "RUA-BBPS (Regularized, Uncertainty-aware, Adaptive Black-Box Prompt Search)",
        "description": "RUA-BBPS replaces raw-accuracy ranking with an adjusted black-box objective that (1) penalizes verbosity (length), (2) penalizes semantic redundancy via cheap MinHash-based similarity to current top prompts, (3) penalizes high uncertainty (bootstrap std over few-shot train fold), and (4) adapts regularization coefficients online using a budgeted multiplicative-weight updater informed by occasional held-out validation checks. It is fully black-box (no model internals), dependency-light (MinHash via deterministic hashing; bootstrap via repeated subset evaluations), and designed to reduce overfitting to tiny few-shot folds, reduce search stagnation on trivial lexical variants, and lower sensitivity to fixed regularization settings and lexical similarity metrics.",
        "training_config": null,
        "optuna_config": {
            "enabled": false,
            "n_trials": 0,
            "search_spaces": null
        }
        },
        "comparative_methods": [
        {
            "method_name": "Raw hill_climb_with_llm (accuracy-sorted baseline)",
            "description": "Original conversational hill-climb that ranks and selects proposals purely by raw training-fold accuracy (no length/semantic/uncertainty regularization). Keeps same proposer and evaluation budget as RUA-BBPS for fair comparison.",
            "training_config": null,
            "optuna_config": null
        },
        {
            "method_name": "RBBPS (static lexical-Jaccard + length penalty)",
            "description": "A static-regularization variant that penalizes prompt length and lexical redundancy using token-set Jaccard similarity (max over TopK pool). Coefficients (lambda_len, mu_sim, gamma_unc) are fixed via small manual sweep. Serves as an ablation to isolate benefits of MinHash and adaptive weighting.",
            "training_config": null,
            "optuna_config": null
        }
        ]
    },
    "research_hypothesis": {
        "open_problems": "LLM-driven black-box prompt search (conversational hill-climb) still suffers from several unresolved issues that limit practical adoption: (1) overfitting to tiny few-shot folds and large generalization gaps, often driven by verbose, dataset-tailored templates; (2) search stagnation where many proposals are trivial lexical variants of the current best prompts, wasting API calls; (3) sensitivity to hand-tuned regularization coefficients (length/diversity weights) and to the choice of lexical similarity metric (token-set Jaccard misses semantic redundancies); (4) inability to detect and penalize unstable / high-variance candidates that perform well on one subsample but fail on others; (5) cost/energy overhead from excessive LLM evaluations for low-value proposals. These problems suggest we need an adjusted black-box objective that (a) captures semantic novelty (not just lexical difference), (b) quantifies candidate stability, and (c) adapts its regularization strengths online to reduce hyperparameter tuning and API cost — while remaining fully black-box and lightweight enough to validate with a Python script.",
        "method": "Regularized, Uncertainty-aware, and Adaptive Black-Box Prompt Search (RUA-BBPS).\n\nCore idea: replace the scalar fitness used to rank/elect proposals with an adjusted score that combines (i) accuracy, (ii) a compactness penalty, (iii) a semantic-novelty penalty computed via cheap MinHash-based similarity on n-gram shingles (captures semantic/phrase overlap while remaining dependency-light), and (iv) an uncertainty penalty estimated via small bootstrap resampling on the few-shot training fold (penalizes high-variance prompts that likely overfit). In addition, make the regularization coefficients adaptive: treat lambda_len, mu_sim, and gamma_unc (uncertainty weight) as online tunable weights updated with a simple bandit-like success signal that uses a small validation budget (occasional held-out calls) to favor coefficient settings that yield better held-out performance. This reduces sensitivity to manual tuning and keeps API cost minimal by limiting validation checks to a fixed small budget per restart.\n\nConcretely, adjusted score S_adj = acc_train - lambda_len * LenNorm(prompt) - mu_sim * MinHashSimMax(prompt, TopKPool) - gamma_unc * BootstrapStd(prompt).\n\nNovel operational contributions beyond prior work:\n- Use MinHash (n-gram shingle hashing) for efficient semantic-similarity estimation that is cheaper than embedding-based similarity and more semantically robust than token-set Jaccard.\n- Add an uncertainty penalty derived from bootstrap resampling of the few-shot train fold to detect overfit candidates early in search.\n- Introduce an online adaptive coefficient updater (budgeted validation + multiplicative weight update) to automatically calibrate regularization strengths per dataset/restart, reducing the need for expensive hyperparameter sweeps.\n- Keep the rest of the conversational LLM proposal mechanism unchanged, preserving black-box constraints and low engineering friction.\n\nFeasibility: all elements (MinHash, simple bootstrapping, bandit update) are implementable in plain Python and require no access to model internals or heavy external dependencies.",
        "experimental_setup": "Datasets: use the CoOp few-shot protocol; validate on a small subset for rapid iteration: Caltech101, Oxford Pets, Food101. Use k in {1,4} (1-shot for high variance, 4-shot to show stability improvements). Use the same 3-fold splits if available.\n\nVLM and LLM: CLIP (RN50) as the VLM evaluation function (local inference returning accuracy on a given subset). LLM proposer: gpt-3.5-turbo for production-like runs; provide a reproducible local-mode option by replacing the LLM with a small local LM/mock proposer to allow fully offline validation.\n\nBaselines:\n- Original hill_climb_with_llm (raw accuracy sorting) — P+N variant.\n- RBBPS (static lexical Jaccard + length penalty) as described in the starting hypothesis.\n- Ablations of RUA-BBPS: (i) no uncertainty penalty, (ii) lexical Jaccard instead of MinHash, (iii) fixed coefficients vs adaptive coefficients.\n\nHyperparameters and adaptation:\n- MinHash: shingle size n=3, signature size k_sig=64.\n- Bootstrap repeats B=20 for an inexpensive std estimate on the few-shot train fold.\n- Validation budget: one small held-out validation call per restart every T iterations (e.g., T=3) or a fixed budget of V_eval=5 validation calls per restart used by the multiplicative weight updater.\n- Adaptive updater: maintain a discrete set of candidate coefficient tuples (small grid) and use exponential weights updated by observed held-out returns; or use multiplicative factor updates on continuous weights using success/failure signals.\n\nEvaluation metrics:\n- Primary: held-out top-1 accuracy on the test fold(s) (same metric used in CoOp benchmarks).\n- Secondary: average prompt length (tokens), semantic diversity of top-K (MinHash-Jaccard estimate), bootstrap-based stability (mean BootstrapStd across top-K), total LLM API calls and token cost, and variation of final accuracy across restarts (std dev).\n\nProtocol: run each method for nrestart in {5,20} (low-cost and full), m=100, niter=10 (reduce for quick tests to nrestart=5, m=50, niter=5). For adaptive runs use the small validation budget. Report mean and std across restarts and dataset folds.",
        "primary_metric": "held-out top-1 accuracy (averaged across dataset folds and restarts)",
        "experimental_code": "import re\nimport random\nimport hashlib\nimport math\nfrom collections import Counter, defaultdict\n\n# ---------- Tokenization & shingle-based MinHash utilities ----------\n\ndef tokenize(prompt):\n    return re.findall(r\"\\w+\", prompt.lower())\n\ndef ngrams(tokens, n=3):\n    return [\" \".join(tokens[i:i+n]) for i in range(max(0, len(tokens)-n+1))]\n\ndef minhash_signature(tokens, n=3, k_sig=64):\n    # simple deterministic MinHash via multiple hash functions simulated by seed mixing\n    shingles = ngrams(tokens, n=n)\n    if not shingles:\n        return tuple([2**64-1]*k_sig)\n    sig = []\n    for i in range(k_sig):\n        minh = 2**64-1\n        for s in shingles:\n            h = int(hashlib.sha256((s + \"|\" + str(i)).encode()).hexdigest()[:16], 16)\n            if h < minh:\n                minh = h\n        sig.append(minh)\n    return tuple(sig)\n\ndef minhash_jaccard(sig_a, sig_b):\n    # estimate Jaccard via signature equality fraction\n    assert len(sig_a) == len(sig_b)\n    match = sum(1 for a,b in zip(sig_a, sig_b) if a==b)\n    return match / float(len(sig_a))\n\n# ---------- Length normalization ----------\n\ndef length_norm(prompt, max_len=40):\n    toks = tokenize(prompt)\n    return min(len(toks) / float(max_len), 1.0)\n\n# ---------- Bootstrap-based uncertainty estimator ----------\n\ndef bootstrap_std_estimate(vlm_eval_on_indexed_subset_fn, prompt, indices, B=20, sample_frac=0.7):\n    # vlm_eval_on_indexed_subset_fn(prompt, indices_subset) -> accuracy in [0,1]\n    if len(indices) == 0:\n        return 0.0\n    vals = []\n    for _ in range(B):\n        subset = [indices[i] for i in random.choices(range(len(indices)), k=max(1,int(len(indices)*sample_frac)))]\n        vals.append(vlm_eval_on_indexed_subset_fn(prompt, subset))\n    mean = sum(vals)/len(vals)\n    var = sum((v-mean)**2 for v in vals)/len(vals)\n    return math.sqrt(var)\n\n# ---------- Adjusted score and adaptive coefficient updater ----------\n\ndef evaluate_adjusted(prompt, vlm_eval_fn_on_full_train, vlm_eval_on_subset, topk_signatures, indices_train,\n                      lambda_len=0.05, mu_sim=0.3, gamma_unc=0.5, max_len=40,\n                      minhash_n=3, k_sig=64, B_boot=20):\n    # vlm_eval_fn_on_full_train(prompt) -> scalar acc in [0,1]\n    acc = vlm_eval_fn_on_full_train(prompt)\n    ln = length_norm(prompt, max_len=max_len)\n    toks = tokenize(prompt)\n    sig = minhash_signature(toks, n=minhash_n, k_sig=k_sig)\n    # semantic similarity: max similarity to top-k signatures\n    max_sim = 0.0\n    for s in topk_signatures:\n        sim = minhash_jaccard(sig, s)\n        if sim > max_sim:\n            max_sim = sim\n    # uncertainty via bootstrap std on train indices\n    unc = bootstrap_std_estimate(vlm_eval_on_subset, prompt, indices_train, B=B_boot)\n    adj = acc - lambda_len * ln - mu_sim * max_sim - gamma_unc * unc\n    return {\n        'acc': acc,\n        'adj': adj,\n        'len': len(toks),\n        'max_sim': max_sim,\n        'unc': unc,\n        'sig': sig\n    }\n\nclass AdaptiveCoeffUpdater:\n    \"\"\"Simple multiplicative weight updater across a small discrete grid of coefficient tuples.\n    Uses a small validation budget: evaluate top candidate(s) on held-out val and update weights.\n    \"\"\"\n    def __init__(self, grid, eta=0.2):\n        # grid: list of (lambda_len, mu_sim, gamma_unc)\n        self.grid = grid\n        self.weights = [1.0 for _ in grid]\n        self.eta = eta\n\n    def sample(self):\n        # sample an index proportionally to weights\n        total = sum(self.weights)\n        r = random.random() * total\n        cum = 0.0\n        for i,w in enumerate(self.weights):\n            cum += w\n            if r <= cum:\n                return i, self.grid[i]\n        return len(self.grid)-1, self.grid[-1]\n\n    def update(self, idx, reward):\n        # reward in [0,1] (higher is better). multiplicative update\n        self.weights[idx] *= math.exp(self.eta * reward)\n\n# ---------- Integration into hill-climb loop (scoring part only) ----------\n\ndef update_pool_with_ruabpps(sorted_prompts, proposals, scores_cache, sig_cache, vlm_eval_full,\n                             vlm_eval_on_subset, indices_train, lambda_len, mu_sim, gamma_unc,\n                             top_k=15, k_sig=64):\n    # sorted_prompts: current list of prompts (strings) sorted by previous adj score\n    # proposals: list of candidate prompts\n    # scores_cache: map prompt->raw acc (if computed)\n    topk = sorted_prompts[:top_k]\n    topk_sigs = [sig_cache.get(t) or minhash_signature(tokenize(t), k_sig=k_sig) for t in topk]\n\n    for p in proposals:\n        if p in scores_cache:\n            raw = scores_cache[p]\n        else:\n            raw = vlm_eval_full(p)\n            scores_cache[p] = raw\n        info = evaluate_adjusted(p, vlm_eval_full, vlm_eval_on_subset, topk_sigs, indices_train,\n                                 lambda_len=lambda_len, mu_sim=mu_sim, gamma_unc=gamma_unc, k_sig=k_sig)\n        scores_cache[p + \"__adj\"] = info['adj']\n        sig_cache[p] = info['sig']\n\n    combined = list(dict.fromkeys(sorted_prompts + proposals))\n    combined_sorted = sorted(combined, key=lambda t: scores_cache.get(t + \"__adj\", scores_cache.get(t, 0.0)), reverse=True)\n    return combined_sorted\n\n# ---------- Notes on expected integration ----------\n# - vlm_eval_full(prompt) should evaluate the prompt on the few-shot training fold and return accuracy in [0,1].\n# - vlm_eval_on_subset(prompt, indices_subset) should evaluate accuracy of prompt on the subset of training indices (for bootstrap).\n# - The adaptive updater is used outside this snippet: periodically (budgeted), top candidate from current pool is evaluated on held-out val; the updater updates weights for coefficient tuples based on val accuracy; subsequently the loop samples new coefficient tuple for the next block of iterations.\n\n# The above functions provide a complete, dependency-light foundation to validate RUA-BBPS in Python.\n",
        "expected_result": "We expect RUA-BBPS to deliver consistent improvements in held-out top-1 accuracy and robustness relative to (a) the original raw-accuracy hill-climb and (b) the static RBBPS described earlier. Specific expectations (averaged across selected datasets and folds):\n- Absolute held-out accuracy gain: +1.0 to +3.0 percentage points over raw-accuracy baseline (larger gains in very-low-shot k=1 settings where instability is pronounced).\n- Stability: reduced standard deviation across restarts (fewer runs stuck in poor local optima) by ~20–40% thanks to the uncertainty penalty discouraging brittle prompts.\n- Prompt cost: average prompt length reduced by ~15–30% (compact templates), lowering per-call token costs.\n- Diversity: semantic diversity of top-15 (MinHash-Jaccard mean) improved relative to raw baseline and lexical Jaccard variant (less stagnation on trivial variants).\n- Hyperparameter robustness: adaptive coefficient updater reduces the need for manual grid sweeps; adaptive runs should match or beat the best static-tuned runs while using fewer validation evaluations.\n- API cost: modest net reduction in LLM calls per effective improvement because fewer trivial proposals are retained and fewer restarts are needed to find good prompts.\n\nThese improvements are conservative, achievable with the budgeted bootstrap and MinHash approaches described, and are measurable with a Python evaluation harness.",
        "expected_conclusion": "Incorporating semantic novelty (MinHash-based), bootstrapped uncertainty, and adaptive regularization into the black-box LLM-in-the-loop prompt search objective materially improves generalization, search efficiency, and robustness compared to raw-accuracy ranking and to simple static regularization. The proposed approach preserves the core advantages of the original conversational hill-climb pipeline (no access to model internals, interpretable natural-language prompts, easy deployment) while addressing its main practical failure modes: overfitting, verbosity-driven token costs, and stagnation on trivial variants. Importantly, the adaptive component reduces manual tuning and makes the method broadly applicable across datasets and few-shot regimes. Because the design is lightweight and fully implementable in Python with local CLIP inference and budgeted validation, the hypothesis is empirically testable and practically relevant for researchers and practitioners seeking lower-cost, more reliable black-box prompt optimization."
    },
    "llm_mapping": {
        "generate_run_config": "global.anthropic.claude-sonnet-4-5-20250929-v1:0",
        "generate_experiment_code": "global.anthropic.claude-sonnet-4-5-20250929-v1:0",
        "validate_experiment_code": "global.anthropic.claude-sonnet-4-5-20250929-v1:0"
    },
    "wandb_config": {
        "entity": "test",
        "project": "test",
        "run_ids": ["a", "b"]
    },
    "max_code_validations": 1
}


### Push experiment code to GitHub branch

POST http://127.0.0.1:8000/airas/v1/code/push
Content-Type: application/json

{
    "experiment_code": {
        "train_py": "# src/train.py\nimport os\nimport sys\nimport json\nimport time\nimport random\nimport math\nimport argparse\nimport tempfile\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import transforms\nfrom torchvision.datasets import Caltech101, OxfordIIITPet\n\nimport wandb\nimport numpy as np\nfrom omegaconf import OmegaConf\nimport hydra\n\n# Ensure transformers cache is kept in .cache/\nos.environ.setdefault(\"TRANSFORMERS_CACHE\", \".cache/\")\n\nfrom transformers import CLIPProcessor, CLIPModel\n\n# Utilities (MinHash, tokenization, bootstrap) based on the paper-method snippet\nimport re\nimport hashlib\nfrom collections import defaultdict\n\n\ndef tokenize_prompt(prompt: str):\n    return re.findall(r\"\\w+\", prompt.lower())\n\n\ndef ngrams(tokens, n=3):\n    return [\" \".join(tokens[i:i + n]) for i in range(max(0, len(tokens) - n + 1))]\n\n\ndef minhash_signature(tokens, n=3, k_sig=64):\n    shingles = ngrams(tokens, n=n)\n    if not shingles:\n        return tuple([2**64 - 1] * k_sig)\n    sig = []\n    for i in range(k_sig):\n        minh = 2**64 - 1\n        for s in shingles:\n            h = int(hashlib.sha256((s + \"|\" + str(i)).encode()).hexdigest()[:16], 16)\n            if h < minh:\n                minh = h\n        sig.append(minh)\n    return tuple(sig)\n\n\ndef minhash_jaccard(sig_a, sig_b):\n    assert len(sig_a) == len(sig_b)\n    match = sum(1 for a, b in zip(sig_a, sig_b) if a == b)\n    return match / float(len(sig_a))\n\n\ndef length_norm(prompt, max_len=40):\n    toks = tokenize_prompt(prompt)\n    return min(len(toks) / float(max_len), 1.0)\n\n\n# Simple CLIP-based evaluator wrapper (uses transformers' CLIPModel)\nclass CLIPEvaluator:\n    def __init__(self, model_id=\"openai/clip-vit-base-patch32\", device=None):\n        self.model_id = model_id\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self._init_model()\n\n    def _init_model(self):\n        # Load model + processor (tokenizer+image preprocess)\n        try:\n            self.model = CLIPModel.from_pretrained(self.model_id, cache_dir=\".cache/\")\n            self.processor = CLIPProcessor.from_pretrained(self.model_id, cache_dir=\".cache/\")\n        except Exception as e:\n            print(\"Failed to load CLIPModel from transformers:\", e, file=sys.stderr)\n            raise\n        self.model.to(self.device)\n        self.model.eval()\n        # Defensive check: tokenizer pad token\n        try:\n            tok = self.processor.tokenizer\n            if getattr(tok, \"pad_token_id\", None) is None:\n                # set to eos_token if missing\n                if getattr(tok, \"eos_token_id\", None) is not None:\n                    tok.pad_token_id = tok.eos_token_id\n                else:\n                    tok.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n        except Exception:\n            pass\n\n    @torch.no_grad()\n    def encode_images(self, pil_images: List[Any]):\n        # pil_images: list of PIL images\n        inputs = self.processor(images=pil_images, return_tensors=\"pt\")\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        image_embeds = self.model.get_image_features(**inputs)\n        image_embeds = F.normalize(image_embeds, dim=-1)\n        return image_embeds.cpu()\n\n    @torch.no_grad()\n    def encode_texts(self, texts: List[str]):\n        inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True)\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        text_embeds = self.model.get_text_features(**inputs)\n        text_embeds = F.normalize(text_embeds, dim=-1)\n        return text_embeds.cpu()\n\n\n# Simple local/mock proposer if OpenAI is not used\nclass MockProposer:\n    def __init__(self, seed: int = 0):\n        random.seed(seed)\n        self.templates = [\n            \"A photo of a {label}.\",\n            \"An image of the {label} species.\",\n            \"This is a picture of a {label}.\",\n            \"A close-up photo of a {label} in natural light.\",\n            \"A low angle shot of a {label}.\",\n            \"A {label} on a plain background.\",\n            \"A vintage photograph of a {label}.\",\n        ]\n\n    def propose(self, current_pool: List[str], topk: int, n_proposals: int) -> List[str]:\n        # Generate proposals by mutating top prompts and combining template variations\n        proposals = []\n        base_phrases = [\"A photo of\", \"An image of\", \"A picture of\", \"A close-up of\", \"A cropped image of\"]\n        words = [\"vivid\", \"clear\", \"in nature\", \"on a plain background\", \"studio shot\", \"closeup\"]\n        for _ in range(n_proposals):\n            template = random.choice(self.templates)\n            # pick a class-like placeholder to format later; here use '{label}' placeholder left intact\n            prompt = template\n            # random suffix/prefix\n            if random.random() < 0.4:\n                prompt = random.choice(base_phrases) + \" {label}.\"\n            if random.random() < 0.5:\n                prompt = prompt.replace(\".\", \", \" + random.choice(words) + \".\")\n            # Mutate by adding an adjective\n            if random.random() < 0.3:\n                prompt = prompt.replace(\"{label}\", \"{label} {adj}\")\n                prompt = prompt.replace(\"{adj}\", random.choice([\"close-up\", \"cute\", \"adult\", \"young\"]))\n            proposals.append(prompt)\n        # ensure uniqueness\n        return list(dict.fromkeys(proposals))\n\n\n# Simple helper: evaluate a single prompt on a labeled dataset using CLIP similarity\n# prompt is a template string containing \"{label}\" placeholder\n\ndef evaluate_prompt_on_indices(prompt_template: str, class_names: List[str], images, labels, indices: List[int], clip_eval: CLIPEvaluator, batch_size=64):\n    # Build texts by filling {label}\n    texts = [prompt_template.format(label=cn) for cn in class_names]\n    # encode texts once\n    text_embeds = clip_eval.encode_texts(texts)  # (num_classes, D)\n    correct = 0\n    total = 0\n    # iterate images at indices\n    for i in range(0, len(indices), batch_size):\n        batch_idx = indices[i:i + batch_size]\n        imgs = [images[idx] for idx in batch_idx]\n        image_embeds = clip_eval.encode_images(imgs)  # (B, D)\n        # similarity\n        sims = image_embeds @ text_embeds.t()  # (B, C)\n        preds = sims.argmax(dim=1).numpy().tolist()\n        for p, idx in zip(preds, batch_idx):\n            if p == labels[idx]:\n                correct += 1\n        total += len(batch_idx)\n    return float(correct) / float(max(1, total))\n\n\n# Bootstrap-based uncertainty estimator\ndef bootstrap_std_estimate(fn_eval_on_indices, prompt_template, class_names, indices, B=20, sample_frac=0.7):\n    if len(indices) == 0:\n        return 0.0\n    vals = []\n    for _ in range(B):\n        k = max(1, int(len(indices) * sample_frac))\n        subset = [random.choice(indices) for _ in range(k)]\n        vals.append(fn_eval_on_indices(prompt_template, class_names, subset))\n    mean = sum(vals) / len(vals)\n    var = sum((v - mean) ** 2 for v in vals) / len(vals)\n    return math.sqrt(var)\n\n\n# Adaptive multiplicative weight updater (discrete grid)\nclass AdaptiveCoeffUpdater:\n    def __init__(self, grid: List[tuple], eta: float = 0.2):\n        self.grid = grid\n        self.weights = [1.0 for _ in grid]\n        self.eta = eta\n\n    def sample(self):\n        total = sum(self.weights)\n        r = random.random() * total\n        cum = 0.0\n        for i, w in enumerate(self.weights):\n            cum += w\n            if r <= cum:\n                return i, self.grid[i]\n        return len(self.grid) - 1, self.grid[-1]\n\n    def update(self, idx: int, reward: float):\n        # reward in [0,1]\n        self.weights[idx] *= math.exp(self.eta * reward)\n\n\n# Main training routine\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    # Hydra will change cwd; ensure outputs use user-provided results_dir if given\n    # Hydra passes overrides; ensure we can access results_dir override or default to ./outputs\n    results_dir = getattr(cfg, \"results_dir\", None) or os.environ.get(\"RESULTS_DIR\") or \"results\"\n    results_dir = Path(results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # Mode adjustments\n    mode = getattr(cfg, \"mode\", \"full\")\n    if mode == \"trial\":\n        # lightweight: disable wandb, tiny epochs/trials\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        # reduce expensive budgets if present\n        if hasattr(cfg.training, \"iterations_per_restart\"):\n            cfg.training.iterations_per_restart = 1\n        if hasattr(cfg.training, \"proposals_per_iteration\"):\n            cfg.training.proposals_per_iteration = min(2, int(cfg.training.proposals_per_iteration))\n    elif mode == \"full\":\n        cfg.wandb.mode = cfg.wandb.get(\"mode\", \"online\")\n    else:\n        raise ValueError(f\"Unsupported mode: {mode}\")\n\n    # Initialize wandb unless disabled\n    wandb_run = None\n    if cfg.wandb.mode != \"disabled\":\n        wandb_run = wandb.init(entity=cfg.wandb.entity,\n                               project=cfg.wandb.project,\n                               id=cfg.run.run_id,\n                               config=OmegaConf.to_container(cfg, resolve=True),\n                               resume=\"allow\")\n    else:\n        print(\"WandB disabled for trial mode\")\n\n    print(f\"Starting run {cfg.run.run_id} mode={mode} results_dir={str(results_dir)}\")\n\n    # Set random seeds\n    seed = int(getattr(cfg.run, \"seed\", 0) or 0)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    # Initialize CLIP evaluator\n    clip_model_id = cfg.model.get(\"id\", \"openai/clip-vit-base-patch32\")\n    clip_eval = CLIPEvaluator(model_id=clip_model_id)\n\n    # Defensive assertions after init\n    # Assert model outputs dims\n    try:\n        # make a small forward pass for shapes\n        txts = [\"a photo of a cat\", \"a photo of a dog\"]\n        te = clip_eval.encode_texts(txts)\n        ie = clip_eval.encode_images([clip_eval.processor.feature_extractor.image_mean] * 1)\n        assert te.ndim == 2\n    except Exception:\n        # Not critical to stop, but must assert\n        print(\"Post-init assertion failed: CLIP encoding smoke test failed\", file=sys.stderr)\n        raise\n\n    # Load dataset via preprocess utilities\n    dataset_cfg = cfg.dataset\n    dataset_name = dataset_cfg.get(\"id\", dataset_cfg.get(\"name\", \"caltech101\"))\n    dataset_root = \".cache/\"\n    transform = transforms.Compose([\n        transforms.Resize(cfg.dataset.preprocessing.image_size),\n        transforms.CenterCrop(cfg.dataset.preprocessing.image_size) if cfg.dataset.preprocessing.center_crop else transforms.CenterCrop(cfg.dataset.preprocessing.image_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=clip_eval.processor.image_mean, std=clip_eval.processor.image_std) if getattr(clip_eval.processor, \"image_mean\", None) is not None else transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    if dataset_name.lower().startswith(\"caltech\"):\n        ds_full = Caltech101(root=dataset_root, download=True, transform=transform)\n        # Caltech101 has .targets and .categories\n        class_to_idx = {c: i for i, c in enumerate(ds_full.categories)}\n        all_classes = ds_full.categories\n    elif \"pet\" in dataset_name.lower() or \"oxford\" in dataset_name.lower():\n        ds_full = OxfordIIITPet(root=dataset_root, download=True, transform=transform, target_types=\"category\")\n        # torchvision's OxfordIIITPet returns (img, target) target is index\n        all_classes = sorted(set([ds_full._labels[idx] for idx in range(len(ds_full))])) if hasattr(ds_full, \"_labels\") else [str(i) for i in range(100)]\n        # fallback: create pseudo-class names\n        if len(all_classes) <= 1:\n            all_classes = [f\"class_{i}\" for i in range(30)]\n    else:\n        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n\n    # Build simple lists of images and labels for our evaluator\n    images = []\n    labels = []\n    class_names = []\n    # For Caltech101, ds_full returns (PILImage, label_str) sometimes\n    # We'll iterate and collect up to max_sampled_classes per config\n    max_classes = int(cfg.dataset.get(\"max_sampled_classes\", 30))\n    # Build mapping of class -> indices\n    class_to_indices = defaultdict(list)\n    for idx in range(len(ds_full)):\n        item = ds_full[idx]\n        if isinstance(item, tuple) and len(item) >= 2:\n            img, target = item[0], item[1]\n        else:\n            continue\n        # target can be int or str\n        if isinstance(target, int):\n            lbl = str(target)\n        else:\n            lbl = str(target)\n        class_to_indices[lbl].append(idx)\n        images.append(img)\n        labels.append(int(target) if isinstance(target, int) else 0)\n    # Build class_names limited to max_classes\n    available_classes = list(class_to_indices.keys())\n    if len(available_classes) == 0:\n        # fallback: use 10 dummy classes\n        available_classes = [f\"class_{i}\" for i in range(min(30, len(images) or 30))]\n    sampled_classes = available_classes[:max_classes]\n    class_names = sampled_classes\n\n    # Build indices mapping for sampled classes\n    indices_per_class = {c: class_to_indices[c][:200] for c in sampled_classes if c in class_to_indices}\n    # Flatten and build labels mapping by index for evaluator\n    # For simplicity, we will create labels by mapping class to local index\n    class_to_local = {c: i for i, c in enumerate(sampled_classes)}\n    images_list = []\n    labels_list = []\n    index_map = []\n    for c in sampled_classes:\n        idxs = indices_per_class.get(c, [])\n        for original_idx in idxs:\n            item = ds_full[original_idx]\n            img, _ = item[0], item[1]\n            images_list.append(img)\n            labels_list.append(class_to_local[c])\n            index_map.append(original_idx)\n    # If empty, fall back to first N items\n    if len(images_list) == 0:\n        for i in range(min(200, len(ds_full))):\n            item = ds_full[i]\n            images_list.append(item[0])\n            labels_list.append(0)\n    images = images_list\n    labels = labels_list\n\n    # Build train/val/test split indices randomly per fold (runner-scale small)\n    n_folds = int(cfg.dataset.splits.get(\"n_folds\", 3))\n    shots = cfg.dataset.splits.get(\"shots\", [1])\n\n    # Simple split: shuffle indices then slice\n    indices_all = list(range(len(images)))\n    random.shuffle(indices_all)\n    fold_splits = []\n    fold_size = max(1, len(indices_all) // n_folds)\n    for f in range(n_folds):\n        start = f * fold_size\n        end = start + fold_size\n        fold_indices = indices_all[start:end]\n        # within each fold, create few-shot train and held-out test\n        # few-shot train: sample k*#classes images randomly from fold\n        fold_splits.append(fold_indices)\n\n    # Prepare proposer\n    proposer_cfg = cfg.components.get(\"proposer\", {}) if hasattr(cfg, \"components\") else {}\n    proposer_mode = proposer_cfg.get(\"mode\", \"production\")\n    prefer_local = cfg.search_config.get(\"cost_saving_defaults\", {}).get(\"prefer_local_mock_proposer\", False)\n    use_mock = (proposer_mode != \"production\") or prefer_local or (os.environ.get(\"OPENAI_API_KEY\") is None)\n    if use_mock:\n        proposer = MockProposer(seed=seed)\n    else:\n        # If OpenAI available, define a wrapper that calls their ChatCompletion API\n        try:\n            import openai\n            openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n\n            class OpenAIProposer:\n                def __init__(self, model_name=\"gpt-3.5-turbo\", temperature=0.8):\n                    self.model = model_name\n                    self.temperature = temperature\n\n                def propose(self, current_pool, topk, n_proposals):\n                    prompt = \"Generate %d prompt templates for image classification. Current top prompts: %s\" % (n_proposals, json.dumps(current_pool[:topk]))\n                    resp = openai.ChatCompletion.create(model=self.model, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=self.temperature)\n                    txt = resp[\"choices\"][0][\"message\"][\"content\"]\n                    # split by lines\n                    proposals = [l.strip() for l in txt.splitlines() if len(l.strip()) > 0]\n                    if not proposals:\n                        # fallback to mock\n                        return MockProposer().propose(current_pool, topk, n_proposals)\n                    return proposals\n\n            proposer = OpenAIProposer(model_name=proposer_cfg.get(\"id\", \"gpt-3.5-turbo\"))\n        except Exception:\n            proposer = MockProposer(seed=seed)\n\n    # Build coefficient grid for adaptive updater (small discrete grid)\n    base_lambda = cfg.search_config.length_normalization.get(\"default_lambda_len\", 0.05)\n    base_mu = cfg.search_config.semantic_penalty.get(\"default_mu_sim\", 0.3)\n    base_gamma = cfg.search_config.uncertainty_penalty.get(\"default_gamma_unc\", 0.5)\n    grid = []\n    for l in [max(0.0, base_lambda * f) for f in [0.5, 1.0, 2.0]]:\n        for m in [max(0.0, base_mu * f) for f in [0.5, 1.0, 2.0]]:\n            for g in [max(0.0, base_gamma * f) for f in [0.5, 1.0, 2.0]]:\n                grid.append((l, m, g))\n    updater = AdaptiveCoeffUpdater(grid=grid, eta=cfg.search_config.adaptive_updater.get(\"eta\", 0.2))\n\n    # Main hill-climb loop (runner-scale)\n    nrestart = int(cfg.training.get(\"nrestart\", 5))\n    iterations_per_restart = int(cfg.training.get(\"iterations_per_restart\", 5))\n    proposals_per_iteration = int(cfg.training.get(\"proposals_per_iteration\", 20))\n    top_k_pool = int(cfg.training.get(\"top_k_pool\", 10))\n    validation_budget = int(cfg.training.get(\"validation_budget_per_restart\", 3))\n\n    # Bookkeeping\n    run_metrics = {\n        \"run_id\": cfg.run.run_id,\n        \"per_restart\": []\n    }\n\n    total_llm_calls = 0\n\n    for restart in range(nrestart):\n        # For reproducibility vary seed\n        random.seed(seed + restart)\n        # Initialize prompt pool with simple templates\n        pool = [\"A photo of a {label}.\", \"An image of the {label} species.\"]\n        pool = list(dict.fromkeys(pool))\n        scores_cache = {}\n        sig_cache = {}\n\n        # sample fold and shot\n        fold_idx = restart % n_folds\n        fold_indices = fold_splits[fold_idx]\n        k_shots = shots[0] if isinstance(shots, list) else shots\n        # Build few-shot train indices: sample k_shots per class\n        # Map class -> indices in fold\n        fold_class_indices = defaultdict(list)\n        for idx in fold_indices:\n            lbl = labels[idx]\n            fold_class_indices[lbl].append(idx)\n        train_indices = []\n        val_indices = []\n        test_indices = []\n        for lbl, idxs_for_lbl in fold_class_indices.items():\n            random.shuffle(idxs_for_lbl)\n            k = min(len(idxs_for_lbl), k_shots)\n            train_indices.extend(idxs_for_lbl[:k])\n            if len(idxs_for_lbl) > k:\n                val_indices.extend(idxs_for_lbl[k:k + 1])\n                test_indices.extend(idxs_for_lbl[k + 1:k + 3])\n        if len(val_indices) == 0:\n            # fallback: a few random indices\n            val_indices = random.sample(fold_indices, min(5, len(fold_indices)))\n        if len(test_indices) == 0:\n            test_indices = [i for i in fold_indices if i not in train_indices][:min(30, len(fold_indices))]\n\n        # Adaptive coefficients sampling initially\n        coeff_idx, (lambda_len, mu_sim, gamma_unc) = updater.sample()\n\n        restart_record = {\"restart\": restart, \"best_prompt\": None, \"best_val_acc\": 0.0, \"history\": []}\n\n        for it in range(iterations_per_restart):\n            # Sample proposals from proposer\n            proposals = proposer.propose(pool, topk=top_k_pool, n_proposals=proposals_per_iteration)\n            total_llm_calls += 1\n\n            # Evaluate proposals: compute acc_train, len_norm, minhash sim vs top-k, bootstrap unc\n            topk_sigs = [sig_cache.get(t) or minhash_signature(tokenize_prompt(t), n=cfg.search_config.minhash.get(\"ngram_n\", 3), k_sig=cfg.search_config.minhash.get(\"k_sig\", 64)) for t in pool[:top_k_pool]]\n\n            for p in proposals:\n                # Evaluate raw train acc if not cached\n                if p not in scores_cache:\n                    acc = evaluate_prompt_on_indices(p, class_names, images, labels, train_indices, clip_eval)\n                    scores_cache[p] = {\"acc\": acc}\n                else:\n                    acc = scores_cache[p][\"acc\"]\n                toks = tokenize_prompt(p)\n                ln = length_norm(p, max_len=cfg.search_config.length_normalization.get(\"max_len_tokens\", 40))\n                sig = minhash_signature(toks, n=cfg.search_config.minhash.get(\"ngram_n\", 3), k_sig=cfg.search_config.minhash.get(\"k_sig\", 64))\n                # semantic penalty: max similarity to topk\n                max_sim = 0.0\n                for s in topk_sigs:\n                    sim = minhash_jaccard(sig, s)\n                    if sim > max_sim:\n                        max_sim = sim\n                # uncertainty\n                unc = bootstrap_std_estimate(lambda tmpl, cls, inds: evaluate_prompt_on_indices(tmpl, cls, images, labels, inds, clip_eval), p, class_names, train_indices, B=cfg.search_config.bootstrap.get(\"B\", 20), sample_frac=cfg.search_config.bootstrap.get(\"sample_frac\", 0.7))\n                adj = acc - lambda_len * ln - mu_sim * max_sim - gamma_unc * unc\n                scores_cache[p].update({\"len\": len(toks), \"max_sim\": max_sim, \"unc\": unc, \"adj\": adj, \"sig\": sig})\n                sig_cache[p] = sig\n\n                # Log per-proposal metrics\n                logd = {f\"iter\": it, \"restart\": restart, \"proposal\": p, \"acc\": acc, \"adj\": adj, \"len\": len(toks), \"max_sim\": max_sim, \"unc\": unc}\n                if wandb_run:\n                    wandb.log({**logd, \"total_llm_calls\": total_llm_calls})\n\n            # Combine pool and proposals, sort by adjusted score\n            combined = list(dict.fromkeys(pool + proposals))\n            combined_sorted = sorted(combined, key=lambda t: scores_cache.get(t, {}).get(\"adj\", scores_cache.get(t, {}).get(\"acc\", 0.0)), reverse=True)\n            pool = combined_sorted[:cfg.search_config.pool_and_selection.get(\"top_k_for_similarity\", top_k_pool * 2)]\n\n            # Evaluate top-1 on held-out val occasionally per adaptive updater budget\n            if (it % cfg.search_config.adaptive_updater.get(\"update_period_iterations\", 3) == 0) and (validation_budget > 0):\n                # Evaluate top candidate on val set\n                top_candidate = pool[0]\n                val_acc = evaluate_prompt_on_indices(top_candidate, class_names, images, labels, val_indices, clip_eval)\n                # Reward scaled to [0,1]\n                reward = float(val_acc)\n                updater.update(coeff_idx, reward)\n                # Resample coefficients\n                coeff_idx, (lambda_len, mu_sim, gamma_unc) = updater.sample()\n                # Record\n                restart_record[\"history\"].append({\"iter\": it, \"top_candidate\": top_candidate, \"val_acc\": val_acc, \"lambda_len\": lambda_len, \"mu_sim\": mu_sim, \"gamma_unc\": gamma_unc})\n                if val_acc > restart_record[\"best_val_acc\"]:\n                    restart_record[\"best_val_acc\"] = val_acc\n                    restart_record[\"best_prompt\"] = top_candidate\n                if wandb_run:\n                    wandb.log({\"val_acc\": val_acc, \"lambda_len\": lambda_len, \"mu_sim\": mu_sim, \"gamma_unc\": gamma_unc, \"restart\": restart, \"iter\": it})\n\n        # End iterations per restart\n        restart_record[\"total_llm_calls\"] = total_llm_calls\n        run_metrics[\"per_restart\"].append(restart_record)\n\n    # After all restarts: evaluate best found prompt on held-out test indices aggregated\n    best_overall = None\n    best_acc = -1.0\n    for r in run_metrics[\"per_restart\"]:\n        if r[\"best_val_acc\"] > best_acc:\n            best_acc = r[\"best_val_acc\"]\n            best_overall = r[\"best_prompt\"]\n    final_test_acc = 0.0\n    if best_overall:\n        all_test_indices = []\n        for fidx in range(n_folds):\n            all_test_indices.extend(fold_splits[fidx])\n        final_test_acc = evaluate_prompt_on_indices(best_overall, class_names, images, labels, all_test_indices, clip_eval)\n\n    # Final logging and summary\n    summary = {\n        \"best_prompt\": best_overall,\n        \"best_val_acc\": float(best_acc),\n        \"final_test_acc\": float(final_test_acc),\n        \"total_llm_calls\": int(total_llm_calls)\n    }\n    if wandb_run:\n        wandb.summary.update(summary)\n        print(\"WandB run url:\", wandb.run.get_url())\n    else:\n        print(\"WandB disabled; run summary:\\n\", json.dumps(summary, indent=2))\n\n    # Save metadata to results_dir\n    run_dir = results_dir / cfg.run.run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    with open(run_dir / \"run_summary.json\", \"w\") as fh:\n        json.dump({**summary, \"per_restart\": run_metrics[\"per_restart\"]}, fh, indent=2)\n\n    if wandb_run:\n        wandb.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "evaluate_py": "# src/evaluate.py\nimport os\nimport sys\nimport json\nimport argparse\nfrom pathlib import Path\nimport wandb\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom wandb.apis import PublicApi\n\n\ndef ensure_dir(p):\n    Path(p).mkdir(parents=True, exist_ok=True)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--results_dir\", required=True)\n    parser.add_argument(\"--run_ids\", required=True, help='JSON string list of run ids')\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    run_ids = json.loads(args.run_ids)\n\n    # Load wandb config file from config/config.yaml in repo root\n    cfg_path = Path(__file__).resolve().parents[2] / \"config\" / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(\"config/config.yaml not found in repository root\")\n    import yaml\n    cfg = yaml.safe_load(cfg_path.read_text())\n    entity = cfg.get(\"wandb\", {}).get(\"entity\", \"test\")\n    project = cfg.get(\"wandb\", {}).get(\"project\", \"test\")\n\n    api = wandb.Api()\n\n    aggregated = {\n        \"primary_metric\": \"held-out top-1 accuracy (averaged across dataset folds and restarts)\",\n        \"metrics\": {},\n        \"best_proposed\": {\"run_id\": None, \"value\": None},\n        \"best_baseline\": {\"run_id\": None, \"value\": None},\n        \"gap\": None\n    }\n\n    per_run_results = {}\n\n    for run_id in run_ids:\n        print(f\"Processing run_id={run_id}\")\n        run_dir = results_dir / run_id\n        ensure_dir(run_dir)\n        try:\n            run = api.run(f\"{entity}/{project}/{run_id}\")\n        except Exception as e:\n            print(f\"Failed to fetch run {run_id} from W&B: {e}\")\n            continue\n        history = run.history()  # pandas DataFrame\n        summary = run.summary._json_dict\n        config = dict(run.config)\n\n        # Save comprehensive metrics\n        metrics_out = run_dir / \"metrics.json\"\n        # Convert history to JSON serializable\n        hist_json = history.fillna(0).to_dict(orient=\"list\")\n        out = {\"history\": hist_json, \"summary\": summary, \"config\": config}\n        with open(metrics_out, \"w\") as fh:\n            json.dump(out, fh, indent=2, default=str)\n        print(f\"Saved run metrics to {metrics_out}\")\n\n        # Generate learning curve figure\n        try:\n            plt.figure(figsize=(6, 4))\n            if \"val_acc\" in history.columns:\n                plt.plot(history.index, history[\"val_acc\"].values, label=\"val_acc\")\n            if \"acc\" in history.columns:\n                plt.plot(history.index, history[\"acc\"].values, label=\"train_acc\")\n            plt.xlabel(\"step\")\n            plt.ylabel(\"accuracy\")\n            plt.title(f\"Learning curve {run_id}\")\n            plt.legend()\n            fname = run_dir / f\"{run_id}_learning_curve.pdf\"\n            plt.tight_layout()\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Saved figure: {fname}\")\n        except Exception as e:\n            print(f\"Failed to generate learning curve for {run_id}: {e}\")\n\n        # Confusion matrix not possible without per-sample preds; skip but generate placeholder\n        # Aggregate primary metric from summary if available\n        primary_val = summary.get(\"final_test_acc\") or summary.get(\"best_val_acc\") or None\n        per_run_results[run_id] = {\"primary\": primary_val, \"summary\": summary}\n\n    # Aggregated analysis\n    # Collect primary metric across runs\n    metrics = {}\n    for run_id, data in per_run_results.items():\n        for k, v in (data.get(\"summary\") or {}).items():\n            metrics.setdefault(k, {})[run_id] = v\n    aggregated[\"metrics\"] = metrics\n\n    # Determine best proposed / best baseline\n    prop_runs = [r for r in per_run_results.keys() if \"proposed\" in r or \"proposed\" in (per_run_results[r][\"summary\"].get(\"method\", \"\") if per_run_results[r].get(\"summary\") else \"\")] \n    base_runs = [r for r in per_run_results.keys() if (\"comparative\" in r or \"comparative\" in (per_run_results[r][\"summary\"].get(\"method\", \"\") if per_run_results[r].get(\"summary\") else \"\") or \"Raw hill_climb\" in (per_run_results[r][\"summary\"].get(\"method\", \"\") if per_run_results[r].get(\"summary\") else \"\"))]\n\n    def get_primary(run_id):\n        s = per_run_results[run_id][\"summary\"]\n        for key in [\"final_test_acc\", \"best_val_acc\", \"final_acc\"]:\n            if key in s:\n                return float(s[key])\n        return None\n\n    best_prop = (None, -1.0)\n    for r in prop_runs:\n        v = get_primary(r)\n        if v is not None and v > best_prop[1]:\n            best_prop = (r, v)\n    best_base = (None, -1.0)\n    for r in base_runs:\n        v = get_primary(r)\n        if v is not None and v > best_base[1]:\n            best_base = (r, v)\n\n    aggregated[\"best_proposed\"] = {\"run_id\": best_prop[0], \"value\": best_prop[1]}\n    aggregated[\"best_baseline\"] = {\"run_id\": best_base[0], \"value\": best_base[1]}\n\n    # Gap calculation: percentage improvement over baseline\n    if best_prop[1] is not None and best_base[1] and best_base[1] > 0:\n        gap = (best_prop[1] - best_base[1]) / best_base[1] * 100.0\n        aggregated[\"gap\"] = float(gap)\n    else:\n        aggregated[\"gap\"] = None\n\n    # Save aggregated metrics\n    comp_dir = results_dir / \"comparison\"\n    ensure_dir(comp_dir)\n    agg_out = comp_dir / \"aggregated_metrics.json\"\n    with open(agg_out, \"w\") as fh:\n        json.dump(aggregated, fh, indent=2, default=str)\n    print(f\"Saved aggregated metrics to {agg_out}\")\n\n    # Comparison figure: bar chart of primary metric across runs\n    try:\n        runs = []\n        vals = []\n        for r in per_run_results.keys():\n            v = get_primary(r)\n            if v is None:\n                continue\n            runs.append(r)\n            vals.append(v)\n        if runs:\n            plt.figure(figsize=(max(6, len(runs) * 1.2), 4))\n            sns.barplot(x=runs, y=vals)\n            plt.xticks(rotation=45, ha='right')\n            plt.ylabel('primary_metric')\n            plt.title('Primary metric comparison')\n            fname = comp_dir / \"comparison_primary_metric_bar_chart.pdf\"\n            plt.tight_layout()\n            plt.savefig(fname)\n            plt.close()\n            print(f\"Saved comparison figure: {fname}\")\n    except Exception as e:\n        print(f\"Failed to generate comparison figure: {e}\")\n\n    # Print generated file paths\n    for run_id in run_ids:\n        run_subdir = results_dir / run_id\n        if run_subdir.exists():\n            for p in run_subdir.iterdir():\n                print(str(p))\n    for p in comp_dir.iterdir():\n        print(str(p))\n\n\nif __name__ == '__main__':\n    main()\n",
        "preprocess_py": "# src/preprocess.py\n# Complete preprocessing pipeline for specified datasets (Caltech101, Oxford-IIIT Pet)\nimport os\nfrom pathlib import Path\nfrom typing import Tuple\nfrom torchvision import transforms\nfrom torchvision.datasets import Caltech101, OxfordIIITPet\n\n# This module provides dataset loading and standard transforms for the experiments.\n# All datasets cached under .cache/\n\nCACHE_DIR = Path(\".cache/\")\nCACHE_DIR.mkdir(parents=True, exist_ok=True)\n\n\ndef get_image_transforms(image_size: int = 224, center_crop: bool = True, normalize_stats: Tuple[float, float, float] = None):\n    if normalize_stats is None:\n        mean = (0.485, 0.456, 0.406)\n        std = (0.229, 0.224, 0.225)\n    else:\n        mean, std = normalize_stats\n\n    tlist = []\n    tlist.append(transforms.Resize(image_size))\n    if center_crop:\n        tlist.append(transforms.CenterCrop(image_size))\n    tlist.append(transforms.ToTensor())\n    tlist.append(transforms.Normalize(mean=mean, std=std))\n    return transforms.Compose(tlist)\n\n\ndef load_caltech101(root: str = \".cache/\", image_size: int = 224, center_crop: bool = True):\n    transform = get_image_transforms(image_size, center_crop)\n    ds = Caltech101(root=root, download=True, transform=transform)\n    return ds\n\n\ndef load_oxford_pets(root: str = \".cache/\", image_size: int = 224, center_crop: bool = True):\n    transform = get_image_transforms(image_size, center_crop)\n    ds = OxfordIIITPet(root=root, download=True, transform=transform, target_types='category')\n    return ds\n",
        "model_py": "# src/model.py\n# Simple model abstractions used in experiments\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# A small linear probe head used optionally for training on top of frozen CLIP image features\nclass LinearProbe(nn.Module):\n    def __init__(self, input_dim: int, num_classes: int):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, num_classes)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\n# Utility to perform a manual auxiliary update using autograd.grad safely (create_graph=False)\ndef auxiliary_update(loss, params, lr=1e-3):\n    # Compute gradients without creating higher-order graph\n    grads = torch.autograd.grad(loss, params, create_graph=False, allow_unused=True)\n    with torch.no_grad():\n        for p, g in zip(params, grads):\n            if g is None:\n                continue\n            if p.grad is None:\n                p.grad = torch.zeros_like(p)\n            p -= lr * g\n",
        "main_py": "# src/main.py\nimport os\nimport sys\nimport subprocess\nfrom pathlib import Path\nfrom omegaconf import OmegaConf\nimport hydra\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    # This main orchestrator launches a single run's train.py as a subprocess using Hydra overrides\n    # Expected CLI: python -m src.main run={run_id} results_dir={path} mode=full\n    run_id = cfg.get(\"run\", cfg.run.run_id if hasattr(cfg, \"run\") else None)\n    results_dir = getattr(cfg, \"results_dir\", \"results\")\n    mode = getattr(cfg, \"mode\", None)\n    if run_id is None or mode is None:\n        print(\"Usage: python -m src.main run={run_id} results_dir={path} mode=<trial|full>\")\n        sys.exit(1)\n\n    # Adjust wandb behavior according to mode\n    if mode == \"trial\":\n        # ensure trial settings\n        overrides = [f\"run.run_id={run_id}\", f\"results_dir={results_dir}\", \"mode=trial\"]\n    else:\n        overrides = [f\"run.run_id={run_id}\", f\"results_dir={results_dir}\", \"mode=full\"]\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + overrides\n    print(\"Running:\", \" \".join(cmd))\n    proc = subprocess.Popen(cmd)\n    proc.wait()\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Train subprocess failed with return code {proc.returncode}\")\n\nif __name__ == '__main__':\n    main()\n",
        "pyproject_toml": "[tool.poetry]\nname = \"rua-bbps-experiments\"\nversion = \"0.1.0\"\ndescription = \"RUA-BBPS experiment code with Hydra and WandB\"\nauthors = [\"Researcher <researcher@example.com>\"]\n\n[tool.poetry.dependencies]\npython = \"^3.8\"\ntorch = \"^1.12\"\ntorchvision = \"^0.13\"\nhydra-core = \"^1.3\"\nwandb = \"^0.15\"\ntransformers = \"^4.30\"\noptuna = \"^3.0\"\nopenai = \"^0.27\"\nmatplotlib = \"^3.7\"\nseaborn = \"^0.12\"\npandas = \"^2.0\"\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n",
        "config_yaml": "wandb:\n  entity: test\n  project: test\n  mode: online\n\nrun:\n  run_id: default-run\n\n# Default dataset/model/training placeholders\nmodel:\n  name: CLIP\n  id: openai/clip-vit-base-patch32\n\ncomponents:\n  proposer:\n    name: gpt-3.5-turbo\n    id: gpt3.5-turbo\n    mode: production\n\ndataset:\n  name: caltech101\n  id: caltech101\n  preprocessing:\n    image_size: 224\n    center_crop: true\n    normalize: true\n\ntraining:\n  nrestart: 1\n  iterations_per_restart: 1\n  proposals_per_iteration: 2\n  top_k_pool: 5\n  validation_budget_per_restart: 1\n\nsearch_config:\n  minhash:\n    ngram_n: 3\n    k_sig: 64\n  bootstrap:\n    B: 20\n    sample_frac: 0.7\n  length_normalization:\n    max_len_tokens: 40\n    default_lambda_len: 0.05\n  semantic_penalty:\n    default_mu_sim: 0.3\n  uncertainty_penalty:\n    default_gamma_unc: 0.5\n  adaptive_updater:\n    eta: 0.2\n    update_period_iterations: 3\n\noptuna:\n  n_trials: 0\n",
        "run_configs": {
        "proposed-clip-rn50-caltech101": "run_id: proposed-clip-rn50-caltech101\nmethod: RUA-BBPS\nmethod_type: proposed\nversion: v1\nsummary: \"Regularized, Uncertainty-aware, Adaptive Black-Box Prompt Search (RUA-BBPS) using MinHash semantic novelty, bootstrap uncertainty, and an online multiplicative-weight adaptive updater. VLM=CLIP RN50 (evaluator), proposer=gpt-3.5-turbo by default. Runner-scale defaults applied.\"\nmodel:\n  name: \"CLIP RN50\"\n  id: \"clip-rn50\"\n  role: vlm\n  description: \"Visual encoder used as local evaluator (top-1 accuracy on few-shot folds).\"\ncomponents:\n  proposer:\n    name: \"gpt-3.5-turbo\"\n    id: \"gpt3.5-turbo\"\n    role: llm_proposer\n    mode: production (default; can be swapped to local/mock proposer for offline runs)\ndataset:\n  name: caltech101\n  id: caltech101\n  max_sampled_classes: 30\n  splits:\n    n_folds: 3\n    shots: [1, 4]\n    runner_scale: true\n  preprocessing:\n    image_size: 224\n    center_crop: true\n    normalize: true\ntraining:\n  # Search / hill-climb specific hyperparameters (Runner-scale defaults)\n  nrestart: 5\n  iterations_per_restart: 5\n  proposals_per_iteration: 50\n  top_k_pool: 10\n  validation_budget_per_restart: 3\n  # Generic training-similar placeholders (kept for compatibility with training pipelines)\n  learning_rate: 0.001\n  batch_size: 16\n  epochs: 1\n  optimizer: adam\nsearch_config:\n  method: RUA-BBPS\n  adjusted_score: \"acc_train - lambda_len * LenNorm(prompt) - mu_sim * MinHashSimMax(prompt,TopKPool) - gamma_unc * BootstrapStd(prompt)\"\n  minhash:\n    ngram_n: 3\n    k_sig: 64\n    deterministic_hash: sha256\n  bootstrap:\n    B: 20\n    sample_frac: 0.7\n  length_normalization:\n    max_len_tokens: 40\n    default_lambda_len: 0.05\n  semantic_penalty:\n    default_mu_sim: 0.30\n  uncertainty_penalty:\n    default_gamma_unc: 0.50\n  adaptive_updater:\n    type: multiplicative_weights\n    eta: 0.2\n    grid: \"small discrete grid of (lambda_len, mu_sim, gamma_unc) tuples (configured externally)\"\n    validation_budget_V_eval: 3\n    update_period_iterations: 3\n  pool_and_selection:\n    top_k_for_similarity: 15\n    selection_sort_key: adj_score\n  cost_saving_defaults:\n    prefer_local_mock_proposer: false\n    limit_validation_calls_per_restart: 3\nevaluation:\n  primary_metric: \"held-out top-1 accuracy\"\n  secondary_metrics:\n    - average_prompt_length\n    - semantic_diversity_min_hash\n    - bootstrap_mean_std\n    - total_llm_api_calls\n    - std_across_restarts\noptuna:\n  n_trials: 0\n  search_spaces: []\nnotes: \"Runner-scale config—use local mock proposer and CLIP local inference for fully offline experiments. Use V_eval budget to update adaptive coefficients.\"\n",
        "proposed-clip-rn50-oxford-pets": "run_id: proposed-clip-rn50-oxford-pets\nmethod: RUA-BBPS\nmethod_type: proposed\nversion: v1\nsummary: \"Regularized, Uncertainty-aware, Adaptive Black-Box Prompt Search (RUA-BBPS) using MinHash semantic novelty, bootstrap uncertainty, and an online multiplicative-weight adaptive updater. VLM=CLIP RN50 (evaluator), proposer=gpt-3.5-turbo by default. Runner-scale defaults applied.\"\nmodel:\n  name: \"CLIP RN50\"\n  id: \"clip-rn50\"\n  role: vlm\n  description: \"Visual encoder used as local evaluator (top-1 accuracy on few-shot folds).\"\ncomponents:\n  proposer:\n    name: \"gpt-3.5-turbo\"\n    id: \"gpt3.5-turbo\"\n    role: llm_proposer\n    mode: production (default; can be swapped to local/mock proposer for offline runs)\ndataset:\n  name: \"Oxford-IIIT Pet\"\n  id: oxford-pets\n  max_sampled_classes: 30\n  splits:\n    n_folds: 3\n    shots: [1, 4]\n    runner_scale: true\n  preprocessing:\n    image_size: 224\n    center_crop: true\n    normalize: true\ntraining:\n  # Search / hill-climb specific hyperparameters (Runner-scale defaults)\n  nrestart: 5\n  iterations_per_restart: 5\n  proposals_per_iteration: 50\n  top_k_pool: 10\n  validation_budget_per_restart: 3\n  # Generic training-similar placeholders (kept for compatibility with training pipelines)\n  learning_rate: 0.001\n  batch_size: 16\n  epochs: 1\n  optimizer: adam\nsearch_config:\n  method: RUA-BBPS\n  adjusted_score: \"acc_train - lambda_len * LenNorm(prompt) - mu_sim * MinHashSimMax(prompt,TopKPool) - gamma_unc * BootstrapStd(prompt)\"\n  minhash:\n    ngram_n: 3\n    k_sig: 64\n    deterministic_hash: sha256\n  bootstrap:\n    B: 20\n    sample_frac: 0.7\n  length_normalization:\n    max_len_tokens: 40\n    default_lambda_len: 0.05\n  semantic_penalty:\n    default_mu_sim: 0.30\n  uncertainty_penalty:\n    default_gamma_unc: 0.50\n  adaptive_updater:\n    type: multiplicative_weights\n    eta: 0.2\n    grid: \"small discrete grid of (lambda_len, mu_sim, gamma_unc) tuples (configured externally)\"\n    validation_budget_V_eval: 3\n    update_period_iterations: 3\n  pool_and_selection:\n    top_k_for_similarity: 15\n    selection_sort_key: adj_score\n  cost_saving_defaults:\n    prefer_local_mock_proposer: false\n    limit_validation_calls_per_restart: 3\nevaluation:\n  primary_metric: \"held-out top-1 accuracy\"\n  secondary_metrics:\n    - average_prompt_length\n    - semantic_diversity_min_hash\n    - bootstrap_mean_std\n    - total_llm_api_calls\n    - std_across_restarts\noptuna:\n  n_trials: 0\n  search_spaces: []\nnotes: \"Runner-scale config—use local mock proposer and CLIP local inference for fully offline experiments. Use V_eval budget to update adaptive coefficients.\"\n",
        "proposed-gpt3.5-turbo-caltech101": "run_id: proposed-gpt3.5-turbo-caltech101\nmethod: RUA-BBPS\nmethod_type: proposed\nversion: v1\nsummary: \"RUA-BBPS run where the primary varied component is the proposer (gpt-3.5-turbo). VLM defaults to CLIP RN50. Runner-scale defaults applied.\"\nmodel:\n  name: \"gpt-3.5-turbo\"\n  id: \"gpt3.5-turbo\"\n  role: llm_proposer\n  description: \"Conversational LLM used to propose prompt templates; can be replaced by local/mock proposer for offline experiments.\"\ncomponents:\n  evaluator:\n    name: \"CLIP RN50\"\n    id: \"clip-rn50\"\n    role: vlm\n    description: \"Local evaluator returning top-1 accuracy on few-shot fold.\"\ndataset:\n  name: caltech101\n  id: caltech101\n  max_sampled_classes: 30\n  splits:\n    n_folds: 3\n    shots: [1, 4]\n    runner_scale: true\n  preprocessing:\n    image_size: 224\n    center_crop: true\n    normalize: true\ntraining:\n  nrestart: 5\n  iterations_per_restart: 5\n  proposals_per_iteration: 50\n  top_k_pool: 10\n  validation_budget_per_restart: 3\n  learning_rate: 0.001\n  batch_size: 16\n  epochs: 1\n  optimizer: adam\nsearch_config:\n  method: RUA-BBPS\n  adjusted_score: \"acc_train - lambda_len * LenNorm(prompt) - mu_sim * MinHashSimMax(prompt,TopKPool) - gamma_unc * BootstrapStd(prompt)\"\n  minhash:\n    ngram_n: 3\n    k_sig: 64\n  bootstrap:\n    B: 20\n    sample_frac: 0.7\n  length_normalization:\n    max_len_tokens: 40\n    default_lambda_len: 0.05\n  semantic_penalty:\n    default_mu_sim: 0.30\n  uncertainty_penalty:\n    default_gamma_unc: 0.50\n  adaptive_updater:\n    type: multiplicative_weights\n    eta: 0.2\n    grid: \"small discrete grid of (lambda_len, mu_sim, gamma_unc) tuples (configured externally)\"\n    validation_budget_V_eval: 3\n    update_period_iterations: 3\n  pool_and_selection:\n    top_k_for_similarity: 15\n    selection_sort_key: adj_score\nevaluation:\n  primary_metric: \"held-out top-1 accuracy\"\noptuna:\n  n_trials: 0\n  search_spaces: []\nnotes: \"This variant explicitly tags gpt-3.5-turbo as the primary varied model for experiment bookkeeping; evaluation still uses CLIP RN50 as the VLM.\"\n",
        "proposed-gpt3.5-turbo-oxford-pets": "run_id: proposed-gpt3.5-turbo-oxford-pets\nmethod: RUA-BBPS\nmethod_type: proposed\nversion: v1\nsummary: \"RUA-BBPS run where the primary varied component is the proposer (gpt-3.5-turbo). VLM defaults to CLIP RN50. Runner-scale defaults applied.\"\nmodel:\n  name: \"gpt-3.5-turbo\"\n  id: \"gpt3.5-turbo\"\n  role: llm_proposer\n  description: \"Conversational LLM used to propose prompt templates; can be replaced by local/mock proposer for offline experiments.\"\ncomponents:\n  evaluator:\n    name: \"CLIP RN50\"\n    id: \"clip-rn50\"\n    role: vlm\n    description: \"Local evaluator returning top-1 accuracy on few-shot fold.\"\ndataset:\n  name: \"Oxford-IIIT Pet\"\n  id: oxford-pets\n  max_sampled_classes: 30\n  splits:\n    n_folds: 3\n    shots: [1, 4]\n    runner_scale: true\n  preprocessing:\n    image_size: 224\n    center_crop: true\n    normalize: true\ntraining:\n  nrestart: 5\n  iterations_per_restart: 5\n  proposals_per_iteration: 50\n  top_k_pool: 10\n  validation_budget_per_restart: 3\n  learning_rate: 0.001\n  batch_size: 16\n  epochs: 1\n  optimizer: adam\nsearch_config:\n  method: RUA-BBPS\n  adjusted_score: \"acc_train - lambda_len * LenNorm(prompt) - mu_sim * MinHashSimMax(prompt,TopKPool) - gamma_unc * BootstrapStd(prompt)\"\n  minhash:\n    ngram_n: 3\n    k_sig: 64\n  bootstrap:\n    B: 20\n    sample_frac: 0.7\n  length_normalization:\n    max_len_tokens: 40\n    default_lambda_len: 0.05\n  semantic_penalty:\n    default_mu_sim: 0.30\n  uncertainty_penalty:\n    default_gamma_unc: 0.50\n  adaptive_updater:\n    type: multiplicative_weights\n    eta: 0.2\n    grid: \"small discrete grid of (lambda_len, mu_sim, gamma_unc) tuples (configured externally)\"\n    validation_budget_V_eval: 3\n    update_period_iterations: 3\n  pool_and_selection:\n    top_k_for_similarity: 15\n    selection_sort_key: adj_score\nevaluation:\n  primary_metric: \"held-out top-1 accuracy\"\noptuna:\n  n_trials: 0\n  search_spaces: []\nnotes: \"This variant explicitly tags gpt-3.5-turbo as the primary varied model for experiment bookkeeping; evaluation still uses CLIP RN50 as the VLM.\"\n",
        "comparative-1-clip-rn50-caltech101": "run_id: comparative-1-clip-rn50-caltech101\nmethod: Raw hill_climb_with_llm\nmethod_type: comparative-1\nversion: v1\nsummary: \"Baseline: original conversational hill-climb that ranks proposals solely by raw training-fold accuracy (no length/semantic/uncertainty regularization). Uses CLIP RN50 as evaluator and gpt-3.5-turbo proposer by default. Runner-scale defaults applied.\"\nmodel:\n  name: \"CLIP RN50\"\n  id: \"clip-rn50\"\n  role: vlm\ncomponents:\n  proposer:\n    name: \"gpt-3.5-turbo\"\n    id: \"gpt3.5-turbo\"\n    role: llm_proposer\ndataset:\n  name: caltech101\n  id: caltech101\n  max_sampled_classes: 30\n  splits:\n    n_folds: 3\n    shots: [1, 4]\n    runner_scale: true\ntraining:\n  nrestart: 5\n  iterations_per_restart: 5\n  proposals_per_iteration: 50\n  top_k_pool: 10\n  validation_budget_per_restart: 3\n  learning_rate: 0.001\n  batch_size: 16\n  epochs: 1\n  optimizer: adam\nsearch_config:\n  method: raw_accuracy_sort\n  description: \"Sort and select proposals by raw training-fold accuracy only. No MinHash, no bootstrap uncertainty, no length penalty.\"\n  evaluation:\n    evaluate_on_train_fold: true\n    evaluate_on_validation_budget: 0\n  pool_and_selection:\n    top_k_for_replacement: 10\n    selection_sort_key: raw_acc\noptuna:\n  n_trials: 0\n  search_spaces: []\nevaluation:\n  primary_metric: \"held-out top-1 accuracy\"\nnotes: \"Baseline config used to measure improvements from RUA-BBPS. Keep same proposal budget for fair comparison.\"\n",
        "comparative-1-clip-rn50-oxford-pets": "run_id: comparative-1-clip-rn50-oxford-pets\nmethod: Raw hill_climb_with_llm\nmethod_type: comparative-1\nversion: v1\nsummary: \"Baseline: original conversational hill-climb that ranks proposals solely by raw training-fold accuracy (no length/semantic/uncertainty regularization). Uses CLIP RN50 as evaluator and gpt-3.5-turbo proposer by default. Runner-scale defaults applied.\"\nmodel:\n  name: \"CLIP RN50\"\n  id: \"clip-rn50\"\n  role: vlm\ncomponents:\n  proposer:\n    name: \"gpt-3.5-turbo\"\n    id: \"gpt3.5-turbo\"\n    role: llm_proposer\ndataset:\n  name: \"Oxford-IIIT Pet\"\n  id: oxford-pets\n  max_sampled_classes: 30\n  splits:\n    n_folds: 3\n    shots: [1, 4]\n    runner_scale: true\ntraining:\n  nrestart: 5\n  iterations_per_restart: 5\n  proposals_per_iteration: 50\n  top_k_pool: 10\n  validation_budget_per_restart: 3\n  learning_rate: 0.001\n  batch_size: 16\n  epochs: 1\n  optimizer: adam\nsearch_config:\n  method: raw_accuracy_sort\n  description: \"Sort and select proposals by raw training-fold accuracy only. No MinHash, no bootstrap uncertainty, no length penalty.\"\n  evaluation:\n    evaluate_on_train_fold: true\n    evaluate_on_validation_budget: 0\n  pool_and_selection:\n    top_k_for_replacement: 10\n    selection_sort_key: raw_acc\noptuna:\n  n_trials: 0\n  search_spaces: []\nevaluation:\n  primary_metric: \"held-out top-1 accuracy\"\nnotes: \"Baseline config used to measure improvements from RUA-BBPS. Keep same proposal budget for fair comparison.\"\n",
        "comparative-1-gpt3.5-turbo-caltech101": "run_id: comparative-1-gpt3.5-turbo-caltech101\nmethod: Raw hill_climb_with_llm\nmethod_type: comparative-1\nversion: v1\nsummary: \"Baseline run tracking proposer variation: gpt-3.5-turbo as primary model label; evaluation uses CLIP RN50. Raw-accuracy sorting baseline. Runner-scale defaults applied.\"\nmodel:\n  name: \"gpt-3.5-turbo\"\n  id: \"gpt3.5-turbo\"\n  role: llm_proposer\ncomponents:\n  evaluator:\n    name: \"CLIP RN50\"\n    id: \"clip-rn50\"\n    role: vlm\ndataset:\n  name: caltech101\n  id: caltech101\n  max_sampled_classes: 30\n  splits:\n    n_folds: 3\n    shots: [1, 4]\n    runner_scale: true\ntraining:\n  nrestart: 5\n  iterations_per_restart: 5\n  proposals_per_iteration: 50\n  top_k_pool: 10\n  validation_budget_per_restart: 3\n  learning_rate: 0.001\n  batch_size: 16\n  epochs: 1\n  optimizer: adam\nsearch_config:\n  method: raw_accuracy_sort\n  description: \"Sort and select proposals by raw training-fold accuracy only. No MinHash, no bootstrap uncertainty, no length penalty.\"\noptuna:\n  n_trials: 0\n  search_spaces: []\nevaluation:\n  primary_metric: \"held-out top-1 accuracy\"\nnotes: \"Proposer-labeled baseline variant; evaluation remains CLIP-based.\"\n",
        "comparative-1-gpt3.5-turbo-oxford-pets": "run_id: comparative-1-gpt3.5-turbo-oxford-pets\nmethod: Raw hill_climb_with_llm\nmethod_type: comparative-1\nversion: v1\nsummary: \"Baseline run tracking proposer variation: gpt-3.5-turbo as primary model label; evaluation uses CLIP RN50. Raw-accuracy sorting baseline. Runner-scale defaults applied.\"\nmodel:\n  name: \"gpt-3.5-turbo\"\n  id: \"gpt3.5-turbo\"\n  role: llm_proposer\ncomponents:\n  evaluator:\n    name: \"CLIP RN50\"\n    id: \"clip-rn50\"\n    role: vlm\ndataset:\n  name: \"Oxford-IIIT Pet\"\n  id: oxford-pets\n  max_sampled_classes: 30\n  splits:\n    n_folds: 3\n    shots: [1, 4]\n    runner_scale: true\ntraining:\n  nrestart: 5\n  iterations_per_restart: 5\n  proposals_per_iteration: 50\n  top_k_pool: 10\n  validation_budget_per_restart: 3\n  learning_rate: 0.001\n  batch_size: 16\n  epochs: 1\n  optimizer: adam\nsearch_config:\n  method: raw_accuracy_sort\n  description: \"Sort and select proposals by raw training-fold accuracy only. No MinHash, no bootstrap uncertainty, no length penalty.\"\noptuna:\n  n_trials: 0\n  search_spaces: []\nevaluation:\n  primary_metric: \"held-out top-1 accuracy\"\nnotes: \"Proposer-labeled baseline variant; evaluation remains CLIP-based.\"\n",
        "comparative-2-clip-rn50-caltech101": "run_id: comparative-2-clip-rn50-caltech101\nmethod: RBBPS (static lexical-Jaccard + length penalty)\nmethod_type: comparative-2\nversion: v1\nsummary: \"Static regularization baseline: penalizes prompt length and lexical redundancy using token-set Jaccard (max over TopK pool). Coefficients are fixed (small manual sweep defaults). VLM=CLIP RN50 evaluator, proposer=gpt-3.5-turbo by default. Runner-scale defaults applied.\"\nmodel:\n  name: \"CLIP RN50\"\n  id: \"clip-rn50\"\n  role: vlm\ncomponents:\n  proposer:\n    name: \"gpt-3.5-turbo\"\n    id: \"gpt3.5-turbo\"\n    role: llm_proposer\ndataset:\n  name: caltech101\n  id: caltech101\n  max_sampled_classes: 30\n  splits:\n    n_folds: 3\n    shots: [1, 4]\n    runner_scale: true\ntraining:\n  nrestart: 5\n  iterations_per_restart: 5\n  proposals_per_iteration: 50\n  top_k_pool: 10\n  validation_budget_per_restart: 3\n  learning_rate: 0.001\n  batch_size: 16\n  epochs: 1\n  optimizer: adam\nsearch_config:\n  method: RBBPS_static\n  adjusted_score: \"acc_train - lambda_len * LenNorm(prompt) - mu_sim * LexicalJaccardMax(prompt,TopKPool) - gamma_unc * BootstrapStd(prompt)\"\n  lexical_similarity:\n    metric: token_set_jaccard\n    ngram_n: 3\n  length_normalization:\n    max_len_tokens: 40\n    lambda_len: 0.05\n  semantic_penalty:\n    mu_sim: 0.30\n  uncertainty_penalty:\n    gamma_unc: 0.00  # static variant: no bootstrap uncertainty penalty by default\n  notes: \"Coefficients set via small manual sweep; ablation of RUA-BBPS to isolate benefits of MinHash and adaptivity.\"\noptuna:\n  n_trials: 0\n  search_spaces: []\nevaluation:\n  primary_metric: \"held-out top-1 accuracy\"\n",
        "comparative-2-clip-rn50-oxford-pets": "run_id: comparative-2-clip-rn50-oxford-pets\nmethod: RBBPS (static lexical-Jaccard + length penalty)\nmethod_type: comparative-2\nversion: v1\nsummary: \"Static regularization baseline: penalizes prompt length and lexical redundancy using token-set Jaccard (max over TopK pool). Coefficients are fixed (small manual sweep defaults). VLM=CLIP RN50 evaluator, proposer=gpt-3.5-turbo by default. Runner-scale defaults applied.\"\nmodel:\n  name: \"CLIP RN50\"\n  id: \"clip-rn50\"\n  role: vlm\ncomponents:\n  proposer:\n    name: \"gpt-3.5-turbo\"\n    id: \"gpt3.5-turbo\"\n    role: llm_proposer\ndataset:\n  name: \"Oxford-IIIT Pet\"\n  id: oxford-pets\n  max_sampled_classes: 30\n  splits:\n    n_folds: 3\n    shots: [1, 4]\n    runner_scale: true\ntraining:\n  nrestart: 5\n  iterations_per_restart: 5\n  proposals_per_iteration: 50\n  top_k_pool: 10\n  validation_budget_per_restart: 3\n  learning_rate: 0.001\n  batch_size: 16\n  epochs: 1\n  optimizer: adam\nsearch_config:\n  method: RBBPS_static\n  adjusted_score: \"acc_train - lambda_len * LenNorm(prompt) - mu_sim * LexicalJaccardMax(prompt,TopKPool) - gamma_unc * BootstrapStd(prompt)\"\n  lexical_similarity:\n    metric: token_set_jaccard\n    ngram_n: 3\n  length_normalization:\n    max_len_tokens: 40\n    lambda_len: 0.05\n  semantic_penalty:\n    mu_sim: 0.30\n  uncertainty_penalty:\n    gamma_unc: 0.00  # static variant: no bootstrap uncertainty penalty by default\n  notes: \"Coefficients set via small manual sweep; ablation of RUA-BBPS to isolate benefits of MinHash and adaptivity.\"\noptuna:\n  n_trials: 0\n  search_spaces: []\nevaluation:\n  primary_metric: \"held-out top-1 accuracy\"\n",
        "comparative-2-gpt3.5-turbo-caltech101": "run_id: comparative-2-gpt3.5-turbo-caltech101\nmethod: RBBPS (static lexical-Jaccard + length penalty)\nmethod_type: comparative-2\nversion: v1\nsummary: \"Static regularization baseline tracking proposer variation: gpt-3.5-turbo labeled as primary model; evaluation uses CLIP RN50. Static lexical-Jaccard + length penalty. Runner-scale defaults applied.\"\nmodel:\n  name: \"gpt-3.5-turbo\"\n  id: \"gpt3.5-turbo\"\n  role: llm_proposer\ncomponents:\n  evaluator:\n    name: \"CLIP RN50\"\n    id: \"clip-rn50\"\n    role: vlm\ndataset:\n  name: caltech101\n  id: caltech101\n  max_sampled_classes: 30\n  splits:\n    n_folds: 3\n    shots: [1, 4]\n    runner_scale: true\ntraining:\n  nrestart: 5\n  iterations_per_restart: 5\n  proposals_per_iteration: 50\n  top_k_pool: 10\n  validation_budget_per_restart: 3\n  learning_rate: 0.001\n  batch_size: 16\n  epochs: 1\n  optimizer: adam\nsearch_config:\n  method: RBBPS_static\n  adjusted_score: \"acc_train - lambda_len * LenNorm(prompt) - mu_sim * LexicalJaccardMax(prompt,TopKPool) - gamma_unc * BootstrapStd(prompt)\"\n  lexical_similarity:\n    metric: token_set_jaccard\n    ngram_n: 3\n  length_normalization:\n    max_len_tokens: 40\n    lambda_len: 0.05\n  semantic_penalty:\n    mu_sim: 0.30\n  uncertainty_penalty:\n    gamma_unc: 0.00\noptuna:\n  n_trials: 0\n  search_spaces: []\nevaluation:\n  primary_metric: \"held-out top-1 accuracy\"\nnotes: \"Proposer-labeled static-regularization variant; coefficients fixed to conservative defaults.\"\n",
        "comparative-2-gpt3.5-turbo-oxford-pets": "run_id: comparative-2-gpt3.5-turbo-oxford-pets\nmethod: RBBPS (static lexical-Jaccard + length penalty)\nmethod_type: comparative-2\nversion: v1\nsummary: \"Static regularization baseline tracking proposer variation: gpt-3.5-turbo labeled as primary model; evaluation uses CLIP RN50. Static lexical-Jaccard + length penalty. Runner-scale defaults applied.\"\nmodel:\n  name: \"gpt-3.5-turbo\"\n  id: \"gpt3.5-turbo\"\n  role: llm_proposer\ncomponents:\n  evaluator:\n    name: \"CLIP RN50\"\n    id: \"clip-rn50\"\n    role: vlm\ndataset:\n  name: \"Oxford-IIIT Pet\"\n  id: oxford-pets\n  max_sampled_classes: 30\n  splits:\n    n_folds: 3\n    shots: [1, 4]\n    runner_scale: true\ntraining:\n  nrestart: 5\n  iterations_per_restart: 5\n  proposals_per_iteration: 50\n  top_k_pool: 10\n  validation_budget_per_restart: 3\n  learning_rate: 0.001\n  batch_size: 16\n  epochs: 1\n  optimizer: adam\nsearch_config:\n  method: RBBPS_static\n  adjusted_score: \"acc_train - lambda_len * LenNorm(prompt) - mu_sim * LexicalJaccardMax(prompt,TopKPool) - gamma_unc * BootstrapStd(prompt)\"\n  lexical_similarity:\n    metric: token_set_jaccard\n    ngram_n: 3\n  length_normalization:\n    max_len_tokens: 40\n    lambda_len: 0.05\n  semantic_penalty:\n    mu_sim: 0.30\n  uncertainty_penalty:\n    gamma_unc: 0.00\noptuna:\n  n_trials: 0\n  search_spaces: []\nevaluation:\n  primary_metric: \"held-out top-1 accuracy\"\nnotes: \"Proposer-labeled static-regularization variant; coefficients fixed to conservative defaults.\"\n"
        }
    },
    "github_config": {
        "github_owner": "auto-res2",
        "repository_name": "airas-api-test",
        "branch_name": "develop-2"
    }
}
