
POST http://127.0.0.1:8001/airas/v1/hypotheses/generations
Content-Type: application/json

{
    "research_objective": "New Optimizer for Large Language Models",
    "research_study_list": [
        {
            "title": "Language Models as Black-Box Optimizers for Vision-Language Models",
            "full_text": "Language Models as Black-Box Optimizers for Vision-Language Models Shihong Liu* Zhiqiu Lin∗ Samuel Yu∗ Ryan Lee Tiffany Ling Deepak Pathak Deva Ramanan Carnegie Mellon University Abstract Vision-language models (VLMs) pre-trained on web- scale datasets have demonstrated remarkable capabilities on downstream tasks when fine-tuned with minimal data. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box ap- proaches for fine-tuning. As such, we aim to develop a black- box approach to optimize VLMs through natural language prompts, thereby avoiding the need to access model param- eters, feature embeddings, or even output logits. We pro- pose employing chat-based LLMs to search for the best text prompt for VLMs. Specifically, we adopt an automatic “hill- climbing” procedure that converges to an effective prompt by evaluating the performance of current prompts and asking LLMs to refine them based on textual feedback, all within a conversational process without human-in-the-loop. In a challenging 1-shot image classification setup, our simple approach surpasses the white-box continuous prompting method (CoOp) by an average of 1.5% across 11 datasets including ImageNet. Our approach also outperforms both human-engineered and LLM-generated prompts. We high- light the advantage of conversational feedback that incor- porates both positive and negative prompts, suggesting that LLMs can utilize the implicit “gradient” direction in textual feedback for a more efficient search. In addition, we find that the text prompts generated through our strategy are not only more interpretable but also transfer well across differ- ent VLM architectures in a black-box manner. Lastly, we apply our framework to optimize the state-of-the-art black- box VLM (DALL-E 3) for text-to-image generation, prompt inversion, and personalization. 1. Introduction Vision-language models [ 1, 27, 53, 66] (VLMs) excel at a wide range of classic vision and multimodal [ 2, 10, 15, 31, 72] tasks, surpassing the performance of their fully- *Co-first authors. Published at CVPR 2024. supervised counterparts on downstream tasks even when fine-tuned with minimal data [33, 76]. However, fine-tuning VLMs typically requires transparent white-box access to the model weights, such as gradient-based approaches that rely on backpropagation. VLMs as black-box services. Despite community ef- forts to collect web-scale public datasets [ 57, 58] and to replicate proprietary VLMs [3, 23], an increasing number of models [1, 4, 13, 46, 66, 73] are not releasing their weights due to privacy and legal concerns [29, 38]. Therefore, one cannot use popular white-box fine-tuning strategies (such as LoRA [22] and Adapter [ 21]) that rely on model weights, feature embeddings, and output logits. Given that contem- porary black-box VLMs [46, 48] like DALL-E [4, 54] still offer a language-based user interface and may be accessed through APIs that facilitate input and output in natural lan- guage, this allows users to customize these models through optimizing textual prompts. Manual prompting. Manual prompt engineering has been proven successful in adapting black-box LLMs to lan- guage tasks [24, 68]. Similarly, carefully crafted prompts can enhance the performance of VLMs. For instance, CLIP has demonstrated improved zero-shot recognition performance using specifically tailored prompts, such as \"a photo of a {class}\" for Internet photos and \"a satellite image of a {class}\" for satellite imagery. Despite its effectiveness, manual prompting can be a laborious pro- cess, inspiring efforts to explore automated prompt creation and thereby remove the need for human involvement. These strategies typically leverage an LLM as a knowledge base to create rich visual descriptors that augment the prompts for each class [41, 52] in a zero-shot fashion. Human-free prompting with conversational LLMs (our approach). We show how to effectively leverage chat- based LLMs [46] to emulate human-level prompt engineer- ing without any human input. We first address an illustrative low-shot image classification task, aiming to find the best class-agnostic prompt (or “template”) for image classifica- tion with CLIP. We start with a random set of prompts and evaluate the one-shot training accuracy of each. Then, akin to arXiv:2309.05950v5  [cs.CL]  14 May 2024Figure 1. Prompting VLMs using chat-based LLMs. Similar to how human prompt engineers iteratively test and refine prompts, we employ ChatGPT [46, 48] to continuously optimize prompts for vision-language models (VLMs). Our iterative approach assesses the performance of ChatGPT-generated prompts on a few-shot dataset (highlighted in blue) and provides feedback (marked in violet) to ChatGPT through simple conversations, as depicted in the illustrative figure. This straightforward method delivers state-of-the-art results for one-shot image classification across 11 datasets using CLIP, operated in a black-box manner without accessing model weights, feature embeddings, or output logits. We show that providing both positive (in green) and negative prompts (in red) enhances efficiency. Remarkably, our approach outperforms both white-box methods such as gradient-based continuous prompting (CoOp [76]) and human-engineered prompts [53] in this extremely low-shot scenario. This figure only shows a typical conversation using ChatGPT’s web user interface. Our code implementation follows this pattern using the ChatGPT API. We detail and ablate the prompts in section 8. human prompt engineering, our method repeatedly presents ChatGPT with the best and worst prompts, asking it to review the results and suggest an improvement (see Figure 1). Learning with implicit “gradients” provided through conversational feedback. One of our key findings is that LLMs can learn the difference between effective and inef- fective prompts, and can use this implicit “gradient” direc- tion provided through language to perform more efficient searches. Compared to previous automatic prompting meth- ods that only use LLMs as a knowledge base [ 41, 52] or paraphrasing tool [ 77], we show a novel use of LLMs as an optimizer that can utilize the patterns hidden in textual feedback. In our experiments, we find that the inclusion of such feedback greatly improves the efficiency and accuracy of our method, sometimes surpassing existing white-box methods [69, 76] on challenging one-shot scenarios. Optimizing text-to-image generation with DALL- E 3. We further demonstrate our optimization frame- work on a state-of-the-art black-box VLM, DALL-E [ 4], for two illustrative one-shot generative tasks: (1) Text- to-image (T2I) generation (see Figure 3), where we sample challenging text queries from Winoground [ 65] that involve reasoning over compositions of objects, at- tributes, and relations. Examples include “ an animal watches a person” and “ there is less milk than orange juice”, which DALL-E 3 might initially fail to generate. (2) Prompt inversion (see Figure 4), which attempts to reverse-engineer the textual prompt to generate a specific image for later customization (personalization) [55] (see Table 5). To achieve this, we leverage conversational feedback from a multimodal LLM (GPT4-V [ 46]) to iter- atively refine the prompts based on the current generated images. We present qualitative results in Table 4 and con- duct a user study to demonstrate that our framework can be more efficient than manual prompting, even for graphical designers experienced with AI content-generation tools. Our contributions. In this work, we introduce a novel prompting method for VLMs, utilizing an LLM as an op- timizer. Our black-box approach can surprisingly com- pete with various white-box methods in a low-shot setting.Additionally, we extensively explore various strategies for conversing with ChatGPT, uncovering several key factors that significantly enhance the efficiency of this tool. We also show that our discovered natural language prompts are not only interpretable but also transfer better across CLIP architectures, eg., from RN50 to ViT/B-16, than continu- ous prompts discovered by previous white-box prompting method [76]. Finally, we show practical applications of our framework on text-to-image generation using black-box DALL-E 3. We release our code for future research on prompt optimization and AI-driven content creation 1. 2. Related Works LLMs for multimodal tasks. Cutting-edge LLMs like GPTs [46, 48] have been successfully applied to multimodal tasks, either through zero-shot composition with pre-trained multimodal models [28, 74] or by jointly fine-tuning with modality-specific encoders [1, 27] on large-scale multimodal datasets [58]. LLMs are also utilized as neuro-symbolic reasoners [16, 37, 60, 75], translating natural language in- structions into modular programs (like Python code) that invoke APIs of multimodal models. In this work, we show the potential of LLMs as a black-box optimizer for multi- modal foundation models with language interfaces, and more specifically vision-language models (VLMs). Prompt optimization of foundation models. Fol- lowing the success of in-context learning [ 6], which ap- pends user-generated natural language instruction and few- shot samples to text inputs, prompting [ 35] has emerged as the preferred fine-tuning paradigm for LLMs due to its superior performance and parameter-efficiency. How- ever, recent prompt optimization methods, including con- tinuous prefix-tuning [7, 30, 63, 64, 71] and discrete token- searching [ 11, 12, 61], still operate in a white-box man- ner, requiring access to either the tokenizer or output logits. Moreover, black-box prompting methods, such as heuristic- based editing [42, 51], are tailored towards language-only tasks and are thus not applicable in VLM settings. LLMs for prompt optimization. APE [77] leverages an LLM to automatically write prompts using few-shot samples based on instruction induction [20] and paraphras- ing [43, 56]. However, it is only designed to address lan- guage tasks, while we focus on multimodal tasks using black- box VLMs. LLMs have also proven to be an effective ex- ternal knowledge base [41, 52, 59] for generating prompts in a zero-shot setting for multimodal models. For example, DCLIP [41] uses GPT3 to come up with rich visual descrip- tions to improve zero-shot classification with CLIP [53]. We extend this line of work to show that LLMs can iteratively optimize prompts for VLMs in a black-box fashion given few-shot samples. We further illustrate that prompt optimiza- 1Project site: llm-can-optimize-vlm.github.io tion with LLMs can be made more efficient by leveraging conversational feedback, such as providing ChatGPT with explicit language feedback on how well the most recent prompt performs. Our findings align with the perspective [9] of LLMs as meta-optimizers that can implicitly perform gradient search through in-context learning. Few-shot adaptation of VLMs. Prompting has also been successfully adopted in VLMs [14], as demonstrated by methods like CoOp [76] that fine-tune an ensemble of con- tinuous prefix tokens using cross-entropy loss. [33] achieves state-of-the-art few-shot performance with a cross-modal (image and text) cross-entropy loss. However, these methods all require access to model parameters for gradient backprop- agation. We also note that while some concurrent works, such as BlackVIP [ 45] and LFA [47], claim to operate in a “black-box” setting, they still require access to privileged information including output logits and embeddings. In this work, we introduce a truly black-box and gradient-free approach that yields competitive results to white-box ap- proaches in extremely low-shot scenarios. 3. Prompting VLMs Using Chat-Based LLMs We now present our approach for prompting VLMs using chat-based LLMs as optimizers. Preliminaries. Motivated by recent proprietary VLMs [4, 46], we adopt a stricter yet practical black-box setting com- pared to prior works [45, 47], requiring minimal knowledge about the model’s inner workings. This is crucial since re- leasing output logits or embeddings can potentially facilitate unauthorized knowledge extraction through distillation meth- ods [18]. Our objective is to enhance the performance of a VLM equipped with a language interface capable of process- ing a textual prompt p ∈ T. We assume that the targeted task is accompanied by a training dataset denoted asDtrain ⊂ D, and its performance can be evaluated with respect to the prompt, represented as a function F : D × T → R. For ex- ample, in a classification task, Dtrain = {x, y}n where x is an image and y is its class label. The black-box VLM takes the image as input and returns a predicted label. We measure the performance of the textual prompt by calculating the average classification accuracy as F(Dtrain, p). Our goal in prompt engineering is to search for the optimal prompt p∗ without accessing or modifying the black-box VLM. Background: human prompt engineering. Our method draws inspiration from the typical workflow of human prompt engineers. Prompt engineering is often an itera- tive process that involves: (a) creating an initial prompt U = {p1} based on the understanding of a task, (b) evaluat- ing the performance of prompts in U, (c) refining prompts based on the outcomes, (d) repeating the last two steps until convergence, and (e) returning the prompt p∗ with the high- est F(Dtrain, p∗). This hands-on approach helps optimize the model’s performance, but it can be tedious and labor-Algorithm 1 We formalize human prompt engineering with the following algorithm, which motivates our LLM-based algorithm (2). Require: Dtrain = {x, y}n: training samples, F : D×T → R: evaluation function 1: Create initial prompts: U ← {p1} 2: Evaluate prompts on training set: S ← {F(Dtrain, p1)} 3: while not converged do 4: Generate a new prompt p′ based on S 5: Evaluate the new prompt: s′ = F(Dtrain, p′) 6: U ← U ∪ {p′} 7: S ← S ∪ {s′} 8: end while 9: return optimal prompt p∗ ← arg maxp∈U F(Dtrain, p) intensive. Algorithm 1 formally illustrates this process. Example: prompting for image classification with CLIP [53]. CLIP is one of the most popular VLM that takes a set of class-specific prompts when performing “zero-shot” image classification. [ 53] details the laborious prompting procedure over the course of a year. Interestingly, they find that a default class-agnostic prompt (or so-called “template”), “a photo of a {class}” can provide a decent boost in accuracy for most datasets compared to using vanilla class labels. In this scenario, the evaluation function F is the classification accuracy on the test set, and the prompt p = {“a photo of a {class}”|c ∈ C}, where C is the set of class names for a given dataset. Prompting with chat-based LLMs (our approach). Given the strong in-context reasoning capabilities of LLMs, we envision them as a black-box optimizers that can im- prove prompts based on their performance outcomes, akin to how human prompt engineers iteratively refine prompts. Specifically, we maintain a pool of prompts U and their cor- responding performance outcomes S. In each iteration, we provide the LLM with both positive and negative prompts, such as the highest and lowest-performing candidates. Such textual feedback through in-context prompts offers LLMs an implied ”gradient” direction [9], making optimization more efficient than taking random local steps. We facilitate this feedback mechanism through conversations with state-of- the-art chat-based LLMs like ChatGPT [ 48] as illustrated in Figure 1. We note that such a multi-turn conversation is not the only way of conversing with ChatGPT, and ablate different in-context feedback mechanisms in section 8. 4. Illustrative Few-Shot Classification Task We illustrate our approach using a few-shot image classi- fication task. Specifically, a prompt p ∈ T consists of a set of class-specific prompts – that is, one textual description per class. The evaluation function F takes the prompt p, along Algorithm 2 LLM-based prompt engineering on the illustra- tive classification task. Our algorithm requires a chat-based LLM and a (black-box) evaluation function, such as accuracy. We highlight mechanisms for “exploration” (restart and re- set) in blue and “exploitation” (iter) in red. We mark the key component of “conversational feedback” of our approach in violet. The actual prompts are attached in section 8. Require: Dtrain = {x, y}n: training samples, F : D×T → R: evaluation function. Require: nrestart: number of initial sampled prompt sets, nreset: number of resets for a prompt set, niter: number of hill-climbing iterations, m: size of one initial prompt set, k: number of prompts send to ChatGPT. 1: p∗ ← ∅ 2: for 1::nrestart do 3: Sample a new prompt set, Uinit ← {p1, ..., pm} 4: for 1::nreset do 5: Reset to initial prompt set: U ← Uinit 6: for 1::niter do 7: Sort U by score outcomes {F(Dtrain, p)}p∈U 8: Utop ← top-k prompts in U 9: Ubot ← bottom-k prompts in U 10: Get a new prompt pnew ← LLM(Utop, Ubot) 11: U ← U ∪ {pnew} 12: end for 13: p∗ ← arg maxp∈U∪{p∗} F(Dtrain, p) 14: end for 15: end for 16: return prompt with highest score p∗ with an image dataset Dtrain, and returns the accuracy us- ing the black-box VLM. To prevent overfitting and simplify our search space, we restrict our search to finding a single class-agnostic template, e.g., a photo of a {}, filling in the blank with label names provided with the dataset. Outline of our approach (Alg. 2). To start, we sample entirely random initial prompts from a text corpus such as LAION-COCO [57] captions. Our approach follows the classical stochastic hill-climbing framework with random- restart [56], which prevents ChatGPT from being trapped in local optima by balancing “exploration” and “exploitation”. Our restart mechanism is implemented by sampling nrestart initial prompt sets to encourage exploration. Because Chat- GPT performs stochastic top-k sampling for text generation (as we adopt the default temperature of 1.0), we also im- plement a reset mechanism to foster additional exploration by retrying a given prompt set nreset times. For exploitation, we converse with ChatGPT for niter iterations. We find that it is critical to balance exploration and exploitation for op- timal performance, and thoroughly examine this trade-off in section 9. Lastly, we present ChatGPT both the top and bottom-performing prompts, denoted as (Utop, Ubot). Weshow that this simple adjustment can improve the efficiency of our approach in Figure 2. Experimental setup. We apply our approach to the few-shot image classification benchmark introduced in CoOp [76], which is the most commonly studied setup for fine-tuning VLMs. This benchmark involves a collection of 11 datasets covering diverse image domains including ImageNet [ 10] and more niche datasets such as FGVC- Aircraft [ 39]. For each dataset, we adhere to the same three-fold k-shot train sets in [ 33], reporting the average accuracy across all folds. Importantly, our method only uti- lizes the train set to compute the score and does not require the few-shot validation set. We use CLIP following prior work [33, 76] to emulate a black-box VLM, and we employ ChatGPT (GPT3.5) as the chat-based LLM. Implementation details. To start, we sample entirely random 1M initial prompts from a text corpus (LAION- COCO [57] captions). For each caption, we extract all the noun phrases using spaCy part-of-speech tagging [19]. Sub- sequently, we replace one noun phrase in the caption with ‘‘{}’’ (a placeholder where the class name will be inserted) to create a template. Given that each caption contains an average of 2 noun phrases, our initial prompt pool consists of approximately 2M templates. We run our algorithm with nrestart = 20 restarts, nresets = 50 resets, and niter = 10 iter- ations. We opt to sample m = 100 prompts per restart and present the top and bottom k = 15 prompts to ChatGPT. We ablate different sets of hyperparameters and explain how we balance the tradeoff between exploration and exploitation in section 9. We adopt gpt-3.5-turbo-0301 model for ChatGPT using OpenAI’s official API and keep the default sampling temperature of 1.0. We also ablate gpt-4 in Ta- ble 10 and find it achieves similar performance. The exact prompts used to converse with ChatGPT are documented in section 8. For a fair comparison, we use CLIP-RN50 for our experiments following prior work [33, 76]. We will open-source our code and release the initial prompt pool (LAIONCOCO-1M) to the public. Oracle white-box baselines. Our black-box setup sub- stantially differs from, and is more constrained than, the sce- narios considered in previous white-box baselines. Specifi- cally, we do not expose the pre-trained weights, model archi- tectures, feature embeddings, or even output logits of VLMs. These constraints render many established gradient-based fine-tuning baselines inapplicable. Among the oracle white- box approaches we later compare to, CoOp [76] performs continuous prompting and requires backpropagation across all layers. WiSE-FT [69] ensembles fine-tuned weights with the original CLIP weights. Cross-Modal Adapta- tion [33] fine-tunes a linear classifier leveraging both im- age and text embeddings from CLIP. BlackVIP [45] and LFA [47] are two most recent baselines that apply CLIP logits or embeddings for gradient back-propagation. Fi- nally, while DCLIP [41] queries GPT3 for rich visual de- scriptors for each class and does not require gradient-based fine-tuning, it performs prompt ensembling using 4-6 class- specific prompts, which breaches our black-box assumption for accessing the output logits. Black-box methods. We additionally benchmark our method against truly black-box solutions, including the vanilla class-agnostic templates “ {class}” and “ a photo of a {class}”. Also, we compare our ap- proach to the best Hand-Engineered templates released by OpenAI, searched using test set performance to represent the theoretical upper bound of human performance, eg., “a centered satellite photo of {class}.” for EuroSAT [17]. Finally, we present two versions of conver- sational feedback of our approach: (a) using 30 positive (P only) or (b) using 15 positive and 15 negative prompts (P+N) in each iteration. For a fair comparison, both of our approaches start with the same initial sampled prompts, referred to as LAIONCOCO-1M. We also show the per- formance of the best initial sampled prompt searched using trainset performance. SOTA one-shot performance against existing methods on 11 datasets. We report the test set performance of our method versus the aforementioned baselines in a challeng- ing 1-shot classification scenario in Table 1. First, com- pared to the top-performing initial prompts selected from LAIONCOCO-1M based on train set performance, our prompt optimization using ChatGPT notably improves the initial prompts by an average of 5% (56% to 61%). Remark- ably, our black-box approach surpasses the two white-box gradient-based fine-tuning techniques CoOp and WiSE-FT by at least 1.5%. Given that both CoOp and our method op- timize a single class-agnostic template, we attribute this gap in performance to reduced overfitting. More specifically, we posit that our optimization space of natural language effec- tively acts as a regularizer in extremely low-shot tasks, stand- ing as a more robust alternative to the continuous prompting approach of CoOp. Furthermore, our method benefits from textual feedback and shows improved performance by 1.0% when using both positive and negative prompts. In section 9, we show that our approach remains effective across different CLIP and ChatGPT variants. Incorporating negative prompts leads to more effi- cient optimization. In Figure 2, we demonstrate that incor- porating both positive and negative prompts fosters better optimization efficiency, achieving higher accuracy within a much fewer number of resets. Specifically, we hypothesize that LLMs can leverage the implicit “gradient” direction suggested in textual feedback to achieve faster convergence. For additional analysis, we ablate different ways of provid- ing conversational feedback to ChatGPT in section 8 and conclude that iteratively updating both positive and negative prompts is the key for efficient optimization.Method Dataset AvgCaltechImageNetAircraftFood Pets Cars SUN UCF DTD EuroSATFlowers Cross-Modal [33]89.1 61.6 20.6 77.1 85.7 59.0 63.4 64.7 49.9 61.8 76.3 64.7 WiSE-FT [69] 85.5 58.3 18.6 71.9 81.7 55.7 56.6 59.4 44.2 52.3 65.8 59.1 CoOp [76] 87.5 57.2 9.6 74.3 85.9 55.6 60.3 61.9 44.4 50.6 68.1 59.6 LFA [47] 81.6 52.4 17.0 63.1 75.3 41.4 58.4 56.7 38.4 60.7 74.9 56.4 BlackVIP [45] 85.8 58.8 15.3 76.7 85.2 56.4 57.0 58.8 40.1 30.0 61.1 56.8 DCLIP [41] - 59.6 - 76.4 83.8 - - - 41.7 34.7 - - {} 78.5 55.3 15.5 74.0 78.9 52.2 53.4 55.5 41.4 32.1 57.3 54.0 a photo of a{} 84.5 57.9 15.9 74.0 83.2 53.9 58.0 56.9 38.8 28.6 60.2 55.6 Hand-Engineered [53]86.3 58.2 17.3 77.3 85.8 55.6 58.5 61.5 42.3 37.6 66.1 58.8 LAIONCOCO-1M81.4 56.2 17.4 76.5 79.6 51.3 54.9 55.8 43.1 38.6 61.3 56.0 Ours (P only) 89.0 59.4 17.9 77.8 85.7 55.7 60.4 58.7 43.6 46.7 66.6 60.1 Ours (P+N) 89.1 59.6 18.1 78.3 88.1 56.2 61.0 60.2 44.8 49.0 67.2 61.1 Oracle white-box approaches Manual prompting approaches Our black-box approaches Table 1. Comparison of our method with other baselines on one-shot classification tasks. We report the average accuracy of each method across three folds, optimized using 1-shot training sets. We bold the best black-box result for each dataset, and underline the second best result. First, we note that our approach can effectively improve upon the initial prompts selected from LAIONCOCO-1M from 56% to 61%. Our approach is also competitive against the best Human-Engineered prompts released by OpenAI [ 53] searched using test set performance. Additionally, we show that using both positive and negative prompts improves the overall accuracy by1%. For reference, we report oracle white-box approaches in gray. Remarkably, we also surpass white-box solutions such as WiSE-FT [69] and CoOp [76] by 1.5%. These methods require either gradient-based fine-tuning (CoOp/WiSE-FT/Cross-Modal) or prompt ensembling using output logits (DCLIP). While our approach is less effective than the SOTA white-box method (Cross-Modal Adaptation), we stress that our black-box setup is significantly more challenging, because we restrict the optimization space to natural language and do not access the pre-trained weights, model architectures, feature embeddings, and output logits of VLMs. 0 5 10 15 20 25 30 35 40 45 50 Number of resets 55.0 55.5 56.0 56.5 57.0 57.5 58.0 58.5 59.0 59.5 60.0 60.5 61.0 61.5 62.0 62.5T est Accuracy (%) Average of all 11 datasets Ours (P+N) Ours (P only) 0 5 10 15 20 25 30 35 40 45 50 Number of resets 55.0 55.5 56.0 56.5 57.0 57.5 58.0 58.5 59.0 59.5 60.0 60.5T est Accuracy (%) Imagenet Ours (P+N) Ours (P only) Figure 2. Conversational feedback incorporating both positive and negative prompts leads to improved efficiency.We fix the number of restarts to 20 and iterations to 10, and ablate different numbers of resets on all 11 datasets (left) and ImageNet (right). Notably, our approach using “P+N” (both top-15 and bottom-15 prompts) can optimize faster within a much fewer number of resets than using “P-Only” (top-30 prompts), resulting in the highest overall performance. 5. More Benefits of Natural Language Prompts In this section, we delve deeper into the advantages of uti- lizing natural language prompts compared to the continuous prompts [76]. We highlight that the prompts derived through our method are interpretable; for instance, they often con- tain descriptions of the targeted image domain. Our prompts can also transfer across CLIP architectures in a black-box manner, such as from RN50 to ViT/B-16. Interpretable natural language prompts. While CoOp [76] concedes that continuous prompts can be difficult to interpret, our method – without explicitly instructing Chat- GPT to generate interpretation – often yields interpretable results. Table 2 showcases the templates returned by our al- gorithm for each dataset, frequently including keywords that reflect the targeted image domain. For example, the template for Food101 [5] mentions “diverse cuisine and ingredients”, and the template for UCF101 [ 62] (an action recognition dataset) mentions “in motion”. Likewise, these templates identify general stylistic attributes of the datasets; they refer to “bright and natural lighting” for ImageNet [10] and note images that “emphasize the subject” for Caltech101 [ 26]. These prompts are particularly intriguing because we do not provide ChatGPT with any information about the down- stream task, yet it manages to generate prompts containing domain-specific keywords that are similar to those engi- neered by human experts. Black-box prompt transfer. Our text prompts also main- tain consistently high performance across different CLIP backbones. For comparison, since CoOp uses the same tok- enizer for all CLIP architectures (including RN50, RN101, ViT/B-32, and ViT/B-16) and optimizes continuous prompts of the same shape (16 x 512), we assess the transferabilityDataset Example of Top Templates Caltech [26] An image of a{}with a blurred background that emphasizes the subject DTD [8] The essential elements of{}are amplified with visual simplicity EuroSAT [17] A top-down view of{}arranged in a pattern{} Aircraft [39] A clear, high-quality image of a single{}with a white background Food [5] A {}featuring diverse cuisine and ingredients ImageNet [10] An image of a{}with bright and natural lighting Flowers [44] A clear and vivid photograph of the{}in its natural setting Pets [50] A {}with distinct and recognizable characteristics Cars [25] A {}featuring a wide range of color options for easy selection SUN [70]A high-resolution photo of a{}with clear background and natural lighting UCF [62] A black and white photo of a{}in motion Table 2. Example templates returned by our algorithm on each dataset. Although we do not provide ChatGPT with any information regarding the targeted dataset, we observe that the resulting templates are remarkably similar to human-engineered templates, with many domain-specific details such as “motion” and “cuisine”, and stylistic elements such as “bright and natural lighting”. Method RN50 →RN101 →ViT-B/32→ViT-B/16 a photo of a{} 57.9 60.6 61.9 66.6 CoOp 63.0 20.6 31.7 39.5 Ours 59.9 60.7 62.2 67.0 Table 3. Black-box prompt transfer from ResNet-50 to other CLIP architectures. We evaluate both our natural language prompts and CoOp’s continuous prompts on 16-shot ImageNet, which are trained using the RN50 CLIP backbone. As a reference point, we include the baseline prompt “a photo of a {}”, and show that the prompts derived from our method using RN50 con- sistently surpass it after transferring to different backbones. In contrast, while CoOp achieves better 16-shot ImageNet perfor- mance using RN50, its performance plummets during the transfer, e.g., from 63% to a mere 21% for RN101. of these learned continuous prompts from RN50 to other backbones using the official weights on 16-shot ImageNet. Table 3 showcases the results of this experiment, where we also include the baseline prompt “a photo of a {}” for reference. We observe a significant decline in accuracy when transferring CoOp’s prompts (up to a 40% decrease despite utilizing more powerful backbones), implying that continu- ous prompts tend to overfit to the specific CLIP model. In contrast, our natural language prompts maintain their perfor- mance and outperform the baseline across all backbones. 6. Application: Text-to-Image Generation In this section, we present a direct application of our prompt optimization framework to generative tasks using a truly black-box text-to-image (T2I) VLM, DALL-E 3 [4]. Optimizing T2I using a multimodal LLM. DALL-E 3 can generate high-fidelity images following diverse user queries, but crafting effective prompts is tricky even for de- signers experienced with AI content generation tools [ 36]. Therefore, we are motivated to implement our LLM-based optimization framework to assist with creative visual design. Our framework is shown in Figure 3 for the illustrative task User Query Init. Image LLM Feedback Final Image There is less milk than orange juice. Incorrect, the milk bottle appears full, more than orange juice... A shorter person is covering the eyes of a taller person. Incorrect, the taller person is covering the shorter person’s eyes. Instead, ... The scarf should feature red and white stripes, and the fur is fluffy... The coat should be buttoned and the lighting exhibits a stronger contrast... Text-to-image generation Prompt inversion Table 4. Examples of T2I optimization. We show that our frame- work (Figure 3) can automatically improve the faithfulness of im- ages generated by DALL-E 3, with respect to user-specified textual topics (for T2I generation) or reference images (for prompt inver- sion). This is achieved through three rounds of prompt optimization, using feedback from the multimodal LLM (GPT4-V). Table 11 and Table 12 shows more examples with actual prompts. of text-to-image generation. In this task, the user specifies a query (topic) in text, such as “an animal watches a person”, and the goal is to write a prompt that can gener- ate an image reflecting this topic. We adopt a multimodal LLM GPT4-V [46] (gpt-4-1106-preview) to provide feedback on the generated image and optimize the prompt. We find that this framework is surprisingly effective due to GPT4-V’s strong visual reasoning capabilities, which can often spot subtle errors in generated images and offer more accurate prompts. Task setup. For T2I generation, we experiment with a subset of 100 text queries from Winoground [65] that involve complex attribute and relation reasoning, which DALL-EFigure 3. Improving text-to-image (T2I) generation using chat-based multimodal LLMs. We apply our framework to optimize prompts for the state-of-the-art black-box generative VLM, DALL-E 3 [4], using the multimodal GPT4-V [46]. For complicated user queries that DALL-E 3 may initially fail to generate, we send the generated image (in violet) along with the current prompt to GPT4-V to ask for feedback on improvements (in red) and then generate a new prompt (in blue). We show that such a simple framework is surprisingly effective at correcting DALL-E 3 mistakes on some challenging Winoground [65] text queries that involve action, logical, and spatial reasoning. We conduct a human evaluation on the quality of generated images in Table 6 and include the actual prompts in section 8. We open-source our code at link to facilitate future research on AI-driven content generation. Figure 4. Prompt inversion using chat-based multimodal LLMs. We apply our framework to reverse engineer the text prompt to generate the same user-queried image. We send the generated image (in violet) along with the original image to GPT4-V to ask for feedback on improvements (in red) and then generate a new prompt (in blue). The final reversed-engineered text prompt allows users to readily perform personalized (customized) generation (see Table 5).User Query Inverted Image Example 1 Example 2 Example 3 Example 4 Example 5 Give the dog a cat friend. Make the dog be in the middle of a jump. Make the dog do a handstand. Make the dog lie down on its side. Make the dog swim in water. Make the owl fight a hawk. Make the owl flap its wings. Make the owl fully green. Make the owl stand in front of the moon. Make the owl walk in the city. Table 5. Customization via prompt inversion. Users can simply append extra descriptions to the inverted prompts to customize their main characters in queried images. might initially fail to generate. Our framework refines the prompts to capture the user-specified topics using a few (three) iterations. We also attempt a reverse task of prompt inversion: given a user-specified reference (query) image, our framework reverse-engineers the prompt to have DALL- E generate the same object or scene in the query image (see Figure 4). This enables users to easily make customiza- tions [55] (see Table 5), such as having the character in a reference image perform various actions or change scenes. For this task, we sample 100 random queries from Diffu- sionDB [67]. We provide qualitative results in Table 4, Ta- ble 11, and Table 12. We hire two volunteers to assess the faithfulness of the images generated by our method, and to compare these with the images manually prompted by two designers (each with one year of experience in AI content generation), as shown in Table 6. Remarks on limitations. While we show promising re- sults, we note some failure cases in Table 13 and Table 14 due to the inherent limitations of foundation models. For example, GPT4-V might fail to describe abstract and artistic details, and DALL-E 3 often fails to generate the correct number of objects. We believe that our framework can bene- fit from more capable foundation models in the future. 7. Discussion and Limitations Summary. We present the first attempt to leverage LLMs as prompt engineers for VLMs. For one-shot image classi- fication, our method surpasses human-engineered prompts and even rivals white-box approaches. Central to the success of our method is the utilization of conversational feedback, enabling chat-based LLMs to efficiently steer VLMs in the right direction. This process leads to naturally interpretable prompts bearing considerable resemblance to those crafted Task Method Init. (std) Final (std)∆ Human 2.28 (.45) 2.86 (.61) 0.58Text-to-Image Ours 2.62 (.36) 3.56 (.54) 0.94 Human 1.58 (.48) 2.76 (.53) 1.18Prompt InversionOurs 1.94 (.39) 3.68 (.47) 1.74 Table 6. Our method enhances faithfulness in T2I generation. We hire two human annotators to assess the faithfulness of images generated from user queries, e.g., textual topics for Text-to-Image, or reference images for Prompt Inversion. The scores are mea- sured on a 1-to-5 Likert scale, with 1 signifying contradiction and 5 indicating perfect alignment with the user’s goal. Our approach benefits from three iterations of prompt optimization and consis- tently outperforms human-engineered prompts by designers who have one year of experience in AI content generation. by humans. Importantly, our natural language prompting setup is a lot more constrained than the assumed scenarios of previous white-box or even some black-box settings [45], because we do not require the model weights and outputs of VLMs. Finally, our framework can be extended to generative tasks using the state-of-the-art black-box DALL-E 3. Limitations and future work. While we try to minimize the overall cost and the total number of API calls, the energy consumption associated with LLMs remains a substantial concern. It is vital to note that we do not intend to compete directly with white-box baselines that can improve visual and text representations with more data. Further details on the higher-shot performance of our method can be found in section 9. VLMs are trained on noisy and imbalanced web data [49], which may result in biased performance [40]. Lastly, we are limited to costly human evaluation for T2I generation in this study. Future work may adopt automatic evaluation [32, 34] for large-scale experiments.8. Details of Conversing with ChatGPT Multi-turn conversation. We use ChatGPT to generate a set of new prompts based on the top and bottom performing prompts (line 10 of Algorithm 2). The exact prompts we use are: Hi ChatGPT, assume you are a pattern learner. I have two lists of CLIP templates: one with good templates and the other with bad templates. There are latent patterns that make a template good or bad. Based on these patterns, give me a better template for image classification while avoiding worse template. Here is the list of good templates: - good1 - good2 - ... Here is the list of bad templates: - bad1 - bad2 - ... Here are my requirements: - Please only reply with the template. - The template should be fewer than 15 words. - The template should have a similar structure to the above templates. Positive Response (if the new prompt outperforms the top-k) The performance of the template ‘‘newTemplate’’ improves to X.XX%. Please give me a better template. Negative Response The performance of the template ‘‘newTemplate’’ drops to X.XX%. Please give me a better template. Alternative implementation: sending only the initial prompts (default). Multi-turn conversation requires ap- pending all chat history to ChatGPT’s official API at every iteration, which costs more input tokens and money. In Fig- ure 5, we show that one can only send the initial prompts (without any response) to ChatGPT at every iteration to get equivalent and even slightly better performance. However, it is important to also update the top-k and bottom-k prompts at every iteration (Iterative) for efficiency. We show that the Non-Iterative version that keeps re-using the initial top-k and bottom-k prompts leads to worse performance. There- fore, in our paper, we stick to Iterative for all experiments. 0 5 10 15 20 25 30 35 40 45 50 Number of resets 55.0 55.5 56.0 56.5 57.0 57.5 58.0 58.5 59.0 59.5 60.0 60.5 61.0 61.5 62.0 62.5T est Accuracy (%) Average of all 11 datasets Iterative Non-Iterative Conversational Feedback 0 5 10 15 20 25 30 35 40 45 50 Number of resets 55.0 55.5 56.0 56.5 57.0 57.5 58.0 58.5 59.0 59.5 60.0 60.5T est Accuracy (%) Imagenet Iterative Non-Iterative Conversational Feedback Figure 5. Updating initial prompts can be as effective as multi- turn conversation. We ablate different ways of conversing with ChatGPT on all 11 datasets (left) and ImageNet (right). Notably, we find that only updating the top-k and bottom-k prompts (Iterative) is as performant and thus a cheaper alternative because sending response to ChatGPT costs more input tokens. On the other hand, reusing the initial prompts (Non-Iterative) leads to worse overall performance.Positive Only (P only). When using only positive prompts, we can remove negative prompts and provide twice as many positive examples: Hi ChatGPT, assume you are a pattern learner. I have one list of CLIP templates: one with good templates. There are latent patterns that make a template good. Based on these patterns, give me a better template for image classification. Here is the list of good templates: - good1 - good2 - ... Here are my requirements: - Please only reply with the template. - The template should be fewer than 15 words. - The template should have a similar structure to the above templates. 9. Additional Experimental Results In this section, we present additional experiments to gain further insights into our method. 510 20 50 100 250 Number of Resets 57.5 58.0 58.5 59.0 59.5 60.0 60.5T est Accuracy (%) Exploration (Reset) vs. Exploitation (Iteration) Human Engineered Accuracy Standard Deviation Figure 6. Balancing exploration and exploitation. We use a fixed budget of 500 ChatGPT API calls per restart, and ablate the optimal number of resets to use in our algorithm on 1-shot ImageNet. The number of iterations is thus inversely proportional to the number of resets; for example, 10 resets would allow for 50 iterations per reset. We take the average over three runs and also report the standard deviation. We find the optimal balance of exploration and exploitation to be 10 iterations and 50 resets. In contrast, “pure” exploration (2 iterations, 250 resets) leads to 0.9% lower accuracy due to insufficient optimization. On the other hand, when exploitation is overly prioritized (100 iterations, 5 resets), our method gets 1.3% lower accuracy. Balancing exploration and exploitation can improve the final performance. Our method extensively leverages Backbone Method Our Approach Hand-Engineered Linear Probe ResNet-50 59.6 58.2 55.9 ResNet-101 61.8 61.6 59.8 ViT-B/32 62.6 62.0 59.6 ViT-B/16 67.8 66.7 65.9 Table 7. Our method can generalize to various CLIP architec- tures. We run our method on 1-shot ImageNet across multiple CLIP backbones, and compare it to the best Human-Engineered prompt and Linear-Probing [53] performance. the ChatGPT API, necessitating an investigation into strate- gies for minimizing optimization costs. This leads us to ex- amine the classic dilemma of exploration versus exploitation, a foundational concept in reinforcement learning. Specifi- cally, we use a fixed budget of 500 API calls per restart, and investigate the optimal combination of the number of resets and iterations in Figure 6. For example, we can allocate 50 resets with 10 iterations each to encourage more exploration, or 10 resets with 50 iterations each to foster more exploita- tion. We find that the optimal balance point is 50 resets of 10 iterations each, and note that no other combination is within 1 standard deviation of the optimal performance. As shown in the performance curve, having too much exploration (250 resets), or too little (5 resets) will result in a roughly 1% decrease in performance. In general, we find it is useful to spend more budget on exploration as ChatGPT can be stuck at local minima within one reset. Reimplementing (iterative) APE for VLM optimiza- tion. We attempt to implement our own version of iterative APE using the given prompts in [77] while making minimal changes such that it fits in our automatic prompt-searching system. For a fair comparison, we reuse exactly the same initial sampled prompts from LAIONCOCO-1M for iterative APE because their “instruction-induction” paradigm cannot be applied to VLM optimization settings. The results are shown in Table 8. We find that iterative APE shows inferior performance to our method, presumably because we lever- age more textual feedback for more efficient search. The exact prompt we use is shown below: Hi ChatGPT, generate a single variation of the following template while keeping the semantic meaning: - template Here is my requirement: - Please return a single template starting with ’-’ Comparison of CLIP backbones. To verify that our method scales properly to other CLIP backbones, we test our method on ImageNet using four different CLIP backbones:Method Dataset AvgCaltech ImageNet Aircraft Food Pets Cars SUN UCF DTD EuroSAT Flowers LAIONCOCO-1M 81.4 56.2 17.4 76.5 79.6 51.3 54.9 55.8 43.1 38.6 61.3 56.0 Iterative APE 88.3 58.1 17.0 77.3 85.1 54.8 58.6 57.4 41.2 46.7 65.3 59.0 Ours (P only) 89.0 59.4 17.9 77.8 85.7 55.7 60.4 58.7 43.6 46.7 66.6 60.1 Ours (P+N) 89.1 59.6 18.1 78.3 88.1 56.2 61.0 60.2 44.8 49.0 67.2 61.1 Table 8. Comparing our method with our own version of iterative APE [77]. Optimized using 1-shot training sets, we find that both iterative APE and our methods can effectively improve upon the initial sampled prompts. However, our method achieves better performance within the same computational budget, presumably because we provide explicit textual feedback to ChatGPT, leading to faster convergence. Shot Method Dataset AvgCaltech ImageNet Aircraft Food Pets Cars SUN UCF DTD EuroSAT Flowers 1 shot Ours (P only) 89.0 59.4 17.9 77.8 87.8 55.7 60.4 58.7 43.6 46.7 66.6 60.1 Ours (P+N) 89.1 59.6 18.1 78.3 88.1 56.2 61.0 60.2 44.8 49.0 67.2 61.1 16 shotOurs (P only) 89.3 59.6 17.7 77.9 86.6 56.2 61.0 60.2 44.0 49.0 66.0 60.6 Ours (P+N) 89.5 59.9 18.1 78.3 88.3 56.8 60.8 60.5 44.9 51.4 67.4 61.4 Table 9. Higher-shot performance. We report the 16-shot performance of our method in this table. It is important to note that as the number of shots increases, the role of the natural language prompt diminishes because it will be more effective to tune the visual representations (which requires white-box access to VLMs). GPT version Dataset AvgCaltech ImageNet Aircraft Food Pets Cars SUN UCF DTD EuroSAT Flowers gpt-turbo-3.5-0301 89.1 59.6 18.1 78.3 88.1 56.2 61.0 60.2 44.8 49.0 67.2 61.1 gpt-4-0314 89.1 59.6 17.9 78.5 87.7 56.2 60.3 59.9 45.0 48.0 67.6 60.9 Table 10. ChatGPT versus GPT4. Our approach is equally effective using other versions of ChatGPT. ResNet-50, ResNet-101, ViT-B/32, and ViT-B/16. We com- pare our method with hand-engineered prompts, and a linear probe (linear classification on the visual embeddings). Ta- ble 7 shows the results of the experiment, where we see that our method outperforms the baselines consistently. Thus, our method scales appropriately with larger and more powerful models. Results on higher shots. We additionally test the general- ization ability of our method given more data (16 shots), with results shown in Table 9. We observe that our method gains small but incremental improvements given more data, and using both top-k and bottom-k prompts (P+N) consistently outperforms top-2k prompts (P only). Results using GPT4. We run our approach using the same hyperparameters and initial prompts using GPT4 in Table 10. It shows that our approach is equally effective using other versions of ChatGPT, but interestingly, there is no performance benefit of using GPT4. This may be because our hyperparameters were optimized on ChatGPT, and are suboptimal for GPT4. Cost analysis. We use GPT3.5 which costs $0.0015 per 1000 tokens. In our default setup, we use an average of 500 tokens per API call. We use a total of 500 API calls (50 resets and 10 iterations) for a total of 250,000 tokens per restart, and thus each run costs around 50 cents. Since we use 20 restarts per dataset, the total cost over the suite of 11 datasets is around $100 for each of the three folds.10. T2I Experimental Details In this section, we include implementation details and more qualitative results for T2I generation experiments. Image generation using DALL-E 3. We use the below template to generate images without changing the prompts. Create this exact image without any changes to the prompt: {prompt}. T2I generation (Figure 3). We use DALL-E 3 to expand the query text to a longer prompt for the first image. Next, we send generated image, query text, and current prompt to GPT4-V for prompt optimization. Prompt for DALLE-3 (first round): Create an image that shows {query text}. Prompt for GPT-4V: Do you think this image {generated image} correctly depicts {query text}? If not, briefly explain why and suggest modifications. Then, help me adjust the prompt to make it correct: {prompt}. Please provide a response in a JSON file format containing: (1) \"feedback\" summarizing the key points, and (2) \"new prompt\" with the revised text. Prompt inversion (Figure 4). We use GPT4-V to gen- erate the initial prompt given the query image. Next, we send query image, generated image, and current prompt to GPT4-V for prompt optimization. Prompt for GPT-4V (first round): Generate a detailed text prompt to recreate the attached image {query image} using an image generator. Prompt for GPT4-V: Compare the original image {query image} and generated image {generated image}, analyze their differences, and then propose changes to update the original prompt in-place: {prompt}. Please provide a response in a JSON file format containing: (1) \"feedback\" summarizing the key points, and (2) \"new prompt\" with the revised text. Failure cases. We show some failure cases of our method in Table 13 and Table 14. We note that these queries are es- pecially challenging even for state-of-the-art VLMs because they require complex reasoning abilities. We expect better performance of our framework using stronger generative models in the future. Prompt inversion on natural images. In addition to sampling queries from DiffusionDB [67], we also attempt at prompt inversion with natural images, as shown in Table 15. Human studies. We hire two graphical designers who have one year of experience using AI content creation tools such as Stable Diffusion and Midjourney to manually design the prompts for DALL-E 3. We also hire two volunteers to assign a Likert scale score between the generated image and user query according to Table 16.User Query Init. Image Final Image Final Prompt The unmasked wrestler hits the masked wrestler. Photo of a wrestling ring where an unmasked male wrestler with a muscular physique is in the midst of delivering a powerful blow to a masked male wrestler donning a lucha libre style mask. The spectators in the background are on the edge of their seats, watching the action closely. The person with earrings pays the person without earrings Photo of a person with a short haircut and noticeable earrings in the process of paying a long-haired vendor without earrings at a market stall, with warm lighting. A bird eats a snake Photo of a vast desert landscape under a clear blue sky. In the foreground, a large, powerful eagle with brown feathers and piercing eyes is perched confidently on a tall, green cactus. The eagle tightly clenches a rattlesnake in its strong talons. The snake’s rattle is visible, and it appears to be struggling. The eagle’s beak is wide open, showing its sharp beak, indicating it’s about to consume the snake. A shorter person is covering the eyes of a taller person. A shorter individual reaching up to cover the eyes of a taller person standing in front of them. The shorter person is on their tiptoes, trying to reach the taller person’s eyes. Both of them are smiling and seem to be enjoying the moment. The backdrop is simple and unobtrusive to maintain focus on the subjects. There is less milk than orange juice. A kitchen scene showing a fridge door open. Inside, there’s a clear glass bottle of milk and a larger bottle of orange juice. Both the milk and orange juice bottles are almost full, with a nearly equal amount visible at the top. The fridge shelves are well-lit, and the rest of the fridge contains a variety of food items including fruits and vegetables. The focus is on the milk and orange juice bottles that are prominently displayed on the shelves. Getting a horse wet. A scene depicting a person using a hose to gently spray water on a horse in an open field. The horse appears calm and enjoys the water, with droplets of water glistening on its coat. The person is smiling, dressed in casual outdoor attire. The background features a clear blue sky and a few trees, creating a serene and peaceful setting. The horse is a beautiful chestnut color, and the person is Caucasian with short brown hair. Some are parking in a train. A whimsical scene depicting a train where some of the carriages are designed as parking spaces, with various types of cars parked inside them. The train is moving through a picturesque landscape, with mountains in the background and a clear blue sky overhead. The cars in the train’s parking carriages include a red sports car, a green SUV , and a yellow compact car. The train itself is a classic steam locomotive with a touch of modern design, emitting a puff of steam as it chugs along the tracks. The white wall will soon be painted blue. A white wall in a room, with a paint can and a paintbrush beside it. The can is open and filled with blue paint, ready for use. A painter, a middle-aged Caucasian man wearing a white painter’s outfit and a cap, is dipping the brush into the blue paint, preparing to start painting the wall. The room has a window with daylight coming through, casting a bright ambiance over the scene. Text-to-image generation Table 11. More results of T2I optimization.User Query Init. Image Final Image Final Prompt Create a digital artwork of a stylized, geometric rhinoceros head with a dynamic array of sharp, crystalline facets in a monochromatic palette of black, white, and gray. The design should feature intricate shadows and highlights to produce a three-dimensional illusion, with a focus on accurately representing the creature’s contours and muscle structure. Adjust the composition to show the rhinoceros head from a frontal perspective, ensuring that both the horn and the ears are symmetrically aligned in the center. Emphasize the geometric nature of the facets by making them more pronounced and varied in shape, creating a complex mosaic that captures the interplay of light and shadow. Add a slight glow to the edges of the facets to enhance the three-dimensional effect and the metallic quality of the artwork. Display the rhinoceros head against a pitch-black background, with a light source positioned to cast dramatic, high-contrast illumination that emphasizes its multifaceted texture. Incorporate a subtle reflective sheen on the surface to suggest a sleek, metallic finish, and ensure the rhinoceros’s eye is detailed and expressive, contributing to the overall lifelike appearance of the artwork. A hyper-realistic full slice of an orange with intricate details, including the textured pulp and clearly defined rind, positioned off-center on a reflective gradient surface transitioning from white to dark. The orange’s juicy texture is accentuated by a dynamic splash of juice, with droplets captured mid-air, creating an energetic and lively scene. The lighting is dramatic and contrasting, with a spotlight effect casting a pronounced shadow to one side to enhance the three-dimensional effect and emphasize the vibrant orange color. Include a clear reflection on the surface and a small stem attached to the orange slice to underscore the realism and freshness. Enhance the composition by ensuring the orange slice is angled slightly, with the splash of juice originating from the lower right side, to add a sense of motion and vitality. A medieval knight in full armor stands with a shield, the dark background highlighting his silhouette against a subtle warm glow. His helmet features a visor with a single vertical slit, and his armor includes a chainmail coif beneath a segmented plate gorget and articulated plate gauntlets, with layered plate armor and flared ridged pauldrons. The knight’s shield is centered and bears a detailed, embossed golden fleur-de-lis on a field of weathered steel, surrounded by rivets. The vibrant orange cloak drapes over both shoulders and behind his back, adding a touch of regal color to the composition. His stance is grounded and balanced, with his left arm extended, presenting the shield, and his right hand resting on the pommel of his sword, exuding a calm and noble demeanor. Create a stylized illustration of a dove in flight, with feathers that transition smoothly through a spectrum of colors including red, orange, yellow, green, blue, indigo, and violet. The dove’s plumage should resemble a dynamic, three-dimensional arrangement of vibrant, overlapping feathers, giving a sense of movement and freedom. The style should be a fusion of semi-realistic and digital art, with a focus on vivid colors and a clean, light background that emphasizes the artwork’s lively and spirited nature. Adjust the feather arrangement to be more structured and flame-like, with the feathers at the tips being more elongated and pointed to enhance the sense of elegance and flow. Create an illustration of a stylized, geometric dinosaur with a textured body in two shades of green: a lighter green for the main body and a darker green for the spiky plates along its back. The dinosaur should have a friendly demeanor, with a long, curved tail and a smooth, rounded head featuring two small, circular white eyes with black pupils. It should stand on two legs with small, rounded feet, each with three visible toes. The background should be a flat, light beige color, with a simple, elongated shadow extending to the right of the dinosaur, indicating a soft light source to the left. Generate an image of a cartoon-style polar bear with gleefully closed eyes and a wide, toothy grin, revealing just a hint of its tongue. The bear should look exuberant, standing on its hind legs with arms open wide as if ready for a hug. The bear’s fur should appear extremely soft and fluffy, with a pronounced blush of rosy pink on both cheeks and belly, enhancing its charm. Adorn the bear with a cozy, chunky-knit scarf, vibrant red with prominent, horizontal white stripes, stylishly wrapped around its neck and draping with a dense tassel fringe at the ends. Situate the bear against a gentle pastel pink backdrop, scattered with delicate, small snowflakes, conveying the splendor and coziness of festive winter cheer. An anthropomorphic duck standing confidently with hands on hips, styled as a classic film noir detective. The duck has a calm and cool expression, wearing a tan detective’s fedora hat and a matching double-breasted trench coat, buttoned up, with a broad collar, epaulets, and a belted waist. The character has a white shirt and a patterned tie with a diagonal stripe design underneath. The character has orange webbed feet and a large, prominent beak. The lighting is dramatic, with a strong contrast between light and shadow, creating a focused shadow on the background that mimics the character’s silhouette. The overall color palette is warm with a gentle light source coming from the side, casting the background in a gradient from warm beige to shadows, giving the image a mysterious and dramatic appearance. Prompt inversion Table 12. More results of prompt inversion.User Query Final Image Final Prompt Reason for Failure There are fewer forks than spoons. Photo of a wooden dining table with a collection of spoons and forks laid out. It’s clear upon counting that there are fewer spoons than forks. The spoons are stainless steel and placed on the left while the forks, also stainless steel, are on the right. Sunlight filters from a nearby window, casting soft shadows. Failure to compare the exact number of forks and spoons in an image. The blue balloon is being lifted by the weight. A surreal scene where a blue balloon appears to be lifting a weight. The weight is realistically depicted, with clear markings indicating its heaviness, while the balloon is oversized and vibrant blue, floating in a clear sky. The scene plays with the concept of reverse gravity, where the balloon, typically light and airy, is paradoxically lifting the heavy weight, creating a visually intriguing and thought-provoking image. Failure to incorporate unusual / counterintuitive situations. Seat numbers increasing from right to left. Illustration of a series of seats in a minimalist room. The seats on the right start with the number ’1’ and the numbers increase as we move to the left. The room has a neutral color palette to ensure focus on the seat numbers. Failure to sort the chair numbers in increasing order. Text-to-image generation Table 13. Failure cases of T2I generation. We note that some Winoground queries that involve commonsense reasoning (e.g., mathematical reasoning, counting) are still too challenging even for DALL-E 3. We expect better results with stronger generative models in the future.User Query Final Image Final Prompt Reason for Failure Create an abstract composition with a dynamic array of shattered, angular shapes emanating from a central point towards the edges of the image. Intensify the contrast by incorporating a deep black void at the core, surrounded by a gradient of vivid colors like red, orange, yellow, green, blue, and indigo transitioning from warm to cool tones to represent this burst of shapes. Add a contrasting background with subtle grayscale gradients, smudges, and paint splatters to enhance the sense of explosion and movement. Include sharp, crisp edges on the shapes to give a sense of three-dimensionality and depth. Ensure the overall effect is of a high-contrast, visually impactful piece that combines both geometric and organic elements, with a clear distinction between the vibrant center and the muted, textured periphery. Adjust the composition to have a more chaotic arrangement of shapes with varying sizes and directions, and incorporate a mix of both soft and hard edges to add complexity. Emphasize a more pronounced use of shadows and highlights to give the shapes a more tangible feel and enhance the illusion of depth and volume. Highly challenging abstract details with complex atypical shapes are difficult to describe in detail, even for GPT-4V . Create a collection of highly detailed, anthropomorphic bird knights with meticulously crafted medieval armors and heraldic shields, standing in a 3x3 grid formation against a smooth, gradient background. Each bird should display intricate feather patterns and vibrant colors true to real bird species, with helmets thoughtfully designed to accommodate their beaks and crests. The armor should be complete with ornate shoulder plates, breastplates, gauntlets, and greaves, while the shields are to be kite or tower shield shaped, adorned with elaborate coat of arms featuring mythical creatures. Armaments will include finely wrought swords, lances, and axes. Aim for a high-fidelity 3D rendering style with a sophisticated color palette and dynamic lighting to accentuate the textures and metallic sheen of the armors, ensuring each knight is posed in a stately and dignified manner. Failure to determine the exact number of objects in an image, especially if the values are greater than 10 or are not in a uniform pattern (grid-shaped). Create a 3x3x3 cube arrangement of light grey pumice stones with visible pores and rough texture, each stone equally sized and cube-shaped, on a gradient dark to light gray background with soft focused shadows and a glossy surface reflective of studio lighting. Adjust the lighting to create a more pronounced contrast, highlighting the top edges of the cubes and casting a subtle shadow on the right side, ensuring the image is sharp and high-resolution at a close-up angle to showcase the detail of the stones’ textures, with the topmost center stone slightly brighter as if catching more light. DALLE-3 fails to understand the meaning of a 3x3x3 set of cubes multiple times, and performs poorly on geometry and patterns in 3 dimensions. Create a collage of six images with a cosmic gastronomy theme: top-left depicts an assortment of cookies and chocolates arranged to mimic a galaxy on a space-like background; top-middle features a swirl of soft-serve ice cream in a dark cup, resembling a nebula against a starry sky; top-right displays a stack of golden brown waffles with a dusting of powdered sugar, resembling a celestial body; bottom-left shows a large, detailed moon looming over a twilight horizon; bottom-middle captures various sweets and snacks cascading onto a shadowy surface, evoking a meteor shower; bottom-right presents a cup filled with popcorn and a straw, giving the illusion of a galaxy-themed beverage, set against a backdrop of floating popcorn and sparkling stars. DALLE-3 fails to disambiguate details between panels, eventually confusing GPT-4V as well from the comparison. Prompt Inversion Table 14. Failure cases of prompt inversion. We find that our method produces suboptimal results for challenging query images. These involve images that are too abstract to describe, contain too many objects, require geometric reasoning, or involve multiple panels.User Query Init. Image Final Image Final Prompt A young golden retriever puppy with a soft, fluffy coat and gentle eyes, tenderly nuzzling a small American Shorthair kitten with a curious and attentive expression. Both animals are sitting close together on a sunlit cobblestone path with patches of vibrant green moss, with the puppy’s paw affectionately resting on the kitten, in the warm ambiance of a backyard during the golden hour. The background features a soft bokeh of lush greenery and the warm tones of a wooden fence, evoking a serene garden or park. The scene captures a moment of affection and camaraderie, showcasing the endearing connection between the two different species, with the sunlight casting a gentle glow and creating soft shadows on their fur. A person in a luxurious crimson kimono, embossed with bold indigo floral patterns, stands diminutive at the lower center of a photorealistic Japanese street as dusk settles in. Facing a grand five-tiered pagoda that ascends into the hazy sky, they hold an expansive crimson paper umbrella aloft, masking their upper body and creating an arresting visual anchor. The alley, bathed in the soft glow from the traditional wooden buildings’ lanterns, stretches around them, while the cobblestone path gleams under the ambient light. In the background, life continues as silhouettes of pedestrians engage in subdued conversations or pause to photograph the scene, adding layers of depth and motion to the tranquil tableau. The pagoda, a silhouette against the misty heavens, invites the viewer’s gaze upward, reinforcing the composition’s sense of depth and perspective. Create an image of a juvenile giant panda with a striking black and white fur pattern, perched on a tree branch. The panda’s mouth is agape as if mid-vocalization, and it is raising its left paw in a greeting gesture, showcasing its prominent claws. Its eyes are round and expressive, reflecting a sense of wonder. The background is a soft-focus portrayal of lush greenery, evoking a dense, misty forest atmosphere. The lighting is diffuse, with a subtle emphasis on the panda’s facial features to highlight its endearing and playful demeanor. Prompt inversion Table 15. Prompt inversion for natural images. We show that our framework can also reverse engineer prompts for natural photos. Score Meaning 1 Not Aligned.The generated image shows a significant divergence from the user query. Key elements, attributes, or relationships differ notably, indicating a clear mismatch. 2 Mildly Similar.The generated image shows a basic level of resemblance to the user query. It includes some of the requested elements or themes, but there are significant inaccuracies or omissions, making it only loosely related. 3 Moderately Similar.The generated image is moderately aligned with the user query. Most of the key elements and attributes are present, but there may be some minor inaccuracies or missing details. 4 Highly Similar.The generated image closely aligns with the user query, accurately representing most main objects, attributes, and their relations, with only minor discrepancies. 5 Perfect.The generated image perfectly matches the user query in every aspect. All main objects, attributes, and their relations are exactly as requested, representing an ideal, precise match. Table 16. Likert scale for human evaluation.References [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems , 35:23716–23736, 2022. 1, 3 [2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision , pages 2425– 2433, 2015. 1 [3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Lud- wig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. 1 [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Joyce Zhuang, Juntang an- dLee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiang, and Aditya Ramesh. Improving image generation with better captions. Note on Dalle-3, 2023. 1, 2, 3, 7, 8 [5] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components with random forests. In European Conference on Computer Vision, 2014. 6, 7 [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. 3 [7] Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Clip-tuning: Towards derivative-free prompt learning with a mixture of rewards. arXiv preprint arXiv:2210.12050, 2022. 3 [8] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. 7 [9] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. ACL, 2023. 3, 4 [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 1, 5, 6, 7 [11] Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, and Zhit- ing Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548 , 2022. 3 [12] Shizhe Diao, Xuechun Li, Yong Lin, Zhichao Huang, and Tong Zhang. Black-box prompt learning for pre-trained lan- guage models. arXiv preprint arXiv:2201.08531, 2022. 3 [13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 1 [14] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, and Jianfeng Gao. Vision-language pre-training: Basics, recent advances, and future trends. Foundations and Trends® in Computer Graphics and Vision, 14(3-4):163–352, 2022. 3 [15] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba- tra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017. 1 [16] Tanmay Gupta and Aniruddha Kembhavi. Visual program- ming: Compositional visual reasoning without training. arXiv preprint arXiv:2211.11559, 2022. 3 [17] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification, 2017. 5, 7 [18] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 3 [19] Matthew Honnibal and Ines Montani. spacy 2: Natural lan- guage understanding with bloom embeddings, convolutional neural networks and incremental parsing. To appear, 7(1): 411–420, 2017. 5 [20] Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782, 2022. 3 [21] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019. 1 [22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.arXiv preprint arXiv:2106.09685, 2021. 1 [23] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Ha- jishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. 1 [24] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information process- ing systems, 35:22199–22213, 2022. 1 [25] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia, 2013. 7 [26] Li, Andreeto, Ranzato, and Perona. Caltech 101, 2022. 6, 7[27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip- 2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 1, 3 [28] Shuang Li, Yilun Du, Joshua B Tenenbaum, Antonio Torralba, and Igor Mordatch. Composing ensembles of pre-trained mod- els via iterative consensus. arXiv preprint arXiv:2210.11522, 2022. 3 [29] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. IEEE signal processing magazine, 37(3):50–60, 2020. 1 [30] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz- ing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. 3 [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 1 [32] Zhiqiu Lin, Xinyue Chen, Deepak Pathak, Pengchuan Zhang, and Deva Ramanan. Revisiting the role of language priors in vision-language models. arXiv preprint arXiv:2306.01879, 2023. 9 [33] Zhiqiu Lin, Samuel Yu, Zhiyi Kuang, Deepak Pathak, and Deva Ramana. Multimodality helps unimodality: Cross- modal few-shot learning with multimodal models. arXiv preprint arXiv:2301.06267, 2023. 1, 3, 5, 6 [34] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Eval- uating text-to-visual generation with image-to-text generation. arXiv preprint arXiv:2404.01291, 2024. 9 [35] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi- roaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1–35, 2023. 3 [36] Vivian Liu. Beyond text-to-image: Multimodal prompts to explore generative ai. In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1–6, 2023. 7 [37] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842 , 2023. 3 [38] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learn- ing models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. 1 [39] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, 2013. 5, 7 [40] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54 (6):1–35, 2021. 9 [41] Sachit Menon and Carl V ondrick. Visual classification via description from large language models. ICLR, 2023. 1, 2, 3, 5, 6 [42] Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing instructional prompts to gptk’s language. arXiv preprint arXiv:2109.07830, 2021. 3 [43] Melanie Mitchell, John Holland, and Stephanie Forrest. When will a genetic algorithm outperform hill climbing. Advances in neural information processing systems, 6, 1993. 3 [44] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In In- dian Conference on Computer Vision, Graphics and Image Processing, 2008. 7 [45] Changdae Oh, Hyeji Hwang, Hee-young Lee, YongTaek Lim, Geunyoung Jung, Jiyoung Jung, Hosik Choi, and Kyung- woo Song. Blackvip: Black-box visual prompting for robust transfer learning. In CVPR, 2023. 3, 5, 6, 9 [46] OpenAI. Gpt-4 technical report. 2023. 1, 2, 3, 7, 8 [47] Yassine Ouali, Adrian Bulat, Brais Matinez, and Georgios Tzimiropoulos. Black box few-shot adaptation for vision- language models. In ICCV, 2023. 3, 5, 6 [48] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car- roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.Advances in Neural Information Processing Systems, 35:27730–27744, 2022. 1, 2, 3, 4 [49] Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, and Shu Kong. The neglected tails of vision-language models. arXiv preprint arXiv:2401.12425, 2024. 9 [50] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. InIEEE Conference on Com- puter Vision and Pattern Recognition, 2012. 7 [51] Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based instruction search for prompt- ing large language models. arXiv preprint arXiv:2203.07281, 2022. 3 [52] Sarah Pratt, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating customized prompts for zero- shot image classification. ICCV, 2023. 1, 2, 3 [53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 1, 2, 3, 4, 6, 11 [54] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image genera- tion with clip latents, 2022. 1 [55] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven gen- eration. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 22500–22510, 2023. 2, 9 [56] Stuart J Russell. Artificial intelligence a modern approach. Pearson Education, Inc., 2010. 3, 4[57] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 1, 4, 5 [58] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next gener- ation image-text models. arXiv preprint arXiv:2210.08402, 2022. 1, 3 [59] Sheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jianwei Yang, Pengchuan Zhang, Zhe Gan, Lijuan Wang, Lu Yuan, Ce Liu, et al. K-lite: Learning transferable visual models with external knowledge. Advances in Neural Information Processing Systems, 35:15558–15573, 2022. 3 [60] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weim- ing Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. 3 [61] Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wal- lace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020. 3 [62] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 6, 7 [63] Tianxiang Sun, Zhengfu He, Hong Qian, Yunhua Zhou, Xuan- Jing Huang, and Xipeng Qiu. Bbtv2: Towards a gradient-free future with large language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3916–3930, 2022. 3 [64] Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning for language-model-as-a- service. In International Conference on Machine Learning, pages 20841–20855. PMLR, 2022. 3 [65] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio- linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5238–5248, 2022. 2, 7, 8 [66] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 1 [67] Zijie J Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. Diffusiondb: A large-scale prompt gallery dataset for text-to-image gen- erative models. arXiv preprint arXiv:2210.14896, 2022. 9, 13 [68] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of- thought prompting elicits reasoning in large language mod- els. Advances in Neural Information Processing Systems, 35: 24824–24837, 2022. 1 [69] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo-Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. arXiv preprint arXiv:2109.01903, 2021. https://arxiv.org/abs/2109.01903. 2, 5, 6 [70] Jianxiong Xiao, Krista A. Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring a large collection of scene categories. Int. J. Comput. Vision, 119(1): 3–22, 2016. 7 [71] Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang- gang Wang, Haiyu Li, and Zhilin Yang. Gps: Genetic prompt search for efficient few-shot learning. arXiv preprint arXiv:2210.17041, 2022. 3 [72] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken- maier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descrip- tions. Transactions of the Association for Computational Linguistics, 2:67–78, 2014. 1 [73] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo- jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. 1 [74] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choro- manski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. So- cratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022. 3 [75] Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel Albanie. Can gpt-4 perform neural architecture search? arXiv preprint arXiv:2304.10970, 2023. 3 [76] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. IJCV, 2022. 1, 2, 3, 5, 6 [77] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large lan- guage models are human-level prompt engineers. ICLR, 2023. 2, 3, 11, 12",
            "references": {},
            "meta_data": {
                "arxiv_id": "2309.05950v5",
                "doi": null,
                "is_generated": null,
                "authors": [
                    "Shihong Liu",
                    "Zhiqiu Lin",
                    "Samuel Yu",
                    "Ryan Lee",
                    "Tiffany Ling",
                    "Deepak Pathak",
                    "Deva Ramanan"
                ],
                "author_affiliations": null,
                "language": null,
                "published_date": "2023-09-12T04:03:41Z",
                "venue": null,
                "volume": null,
                "issue": null,
                "pages": null,
                "pdf_url": null,
                "github_url": "",
                "peer_review_status": null,
                "access_type": null,
                "reference_count": null,
                "citation_count": null,
                "h_index_relevance": null
            },
            "llm_extracted_info": {
                "main_contributions": "This paper introduces a truly black-box framework that uses chat-based large language models (LLMs) as optimizers to automatically discover and refine natural-language prompts for vision-language models (VLMs) without access to model weights, embeddings, or logits. Key contributions include: (1) an iterative conversational hill-climbing procedure that provides LLMs with performance feedback (top-k and bottom-k prompts) to produce improved class-agnostic templates; (2) demonstration that conversational feedback (positive + negative examples) supplies an implicit “gradient” that improves search efficiency; (3) empirical results showing the black-box method surpasses human-engineered prompts and outperforms white-box continuous prompting (CoOp) in 1-shot classification across 11 datasets; (4) evidence that discovered natural-language prompts are interpretable and transfer better across CLIP backbones than continuous prompts; (5) extension of the framework to optimize text-to-image generation, prompt inversion, and personalization for a state-of-the-art black-box generative VLM (DALL-E 3).",
                "methodology": "The approach treats an LLM (e.g., ChatGPT/GPT-3.5, with GPT-4 ablations; GPT4-V for multimodal feedback) as a black-box optimizer that iteratively refines text templates. Key elements: (a) build a large initial pool of candidate templates by extracting and masking noun phrases from LAION-COCO captions; (b) evaluate prompts on a few-shot training set using the target VLM as a black box (function F returns accuracy); (c) use stochastic hill-climbing with random restarts, resets, and multi-turn conversational feedback where the LLM is given top-k (positive) and bottom-k (negative) prompts plus scores and asked to propose improved templates; (d) two feedback variants: P-only (top-k) and P+N (top-k and bottom-k); (e) hyperparameters and search strategy: nrestart=20, nreset=50, niter=10, m=100 sampled prompts per restart, k=15 shown to LLM (default GPT sampling temperature=1.0); (f) for T2I and prompt-inversion, use GPT4-V to compare generated images to targets and produce JSON-formatted feedback and revised prompts over a few (e.g., 3) iterations. The method does not access model internals, logits, or embeddings.",
                "experimental_setup": "Few-shot classification: benchmark follows CoOp experimental setup across 11 datasets (ImageNet, Caltech101, DTD, EuroSAT, FGVC-Aircraft, Food101, Oxford Pets, Stanford Cars, SUN397, UCF101, Oxford Flowers), using the same three-fold k-shot train sets and reporting average accuracy across folds. Primary VLM: CLIP (RN50) used as the black-box classifier; baselines include white-box methods (CoOp, WiSE-FT, Cross-Modal Adaptation), recent black-box/privileged-access methods (BlackVIP, LFA, DCLIP), hand-engineered templates, and initial sampled pool (LAIONCOCO-1M). LLMs: ChatGPT (gpt-3.5-turbo-0301) as default, GPT-4 ablations reported; multimodal GPT4-V used for image-feedback tasks. T2I tasks: 100 challenging queries sampled from Winoground for text-to-image and 100 queries from DiffusionDB for prompt inversion; human evaluations conducted with annotators/designers using a 1–5 Likert scale for faithfulness. Implementation details: initial prompt pool ≈2M templates (from noun-phrase masked captions), prompt length constraints, iterative/iterative-non-iterative ablations, cost reporting (~$0.50 per restart run; ~ $100 total for experiments across datasets and folds).",
                "limitations": "Notable limitations reported: (1) cost and energy: the approach requires many LLM API calls (token/input costs and compute), raising monetary and environmental concerns; (2) constrained optimization space: restricting search to natural-language templates can limit achievable gains compared to white-box fine-tuning when more labeled data is available; (3) dependence on LLM and VLM capabilities: failures occur for complex reasoning, precise counting, geometry, or highly abstract/artistic descriptions (GPT4-V and DALL-E 3 limitations); (4) no access to logits/embeddings or weights prevents certain gradient-based improvements and may limit performance in higher-shot regimes; (5) bias and dataset issues: underlying VLMs trained on web-scale noisy data may carry biases that the method does not address; (6) evaluation bottlenecks: T2I evaluation relies on costly human judgments and some automated evaluation strategies are not yet adopted at scale.",
                "future_research_directions": "Promising extensions include: (1) hybrid methods that combine black-box LLM-driven prompt search with lightweight white-box adaptation when partial access is available (e.g., few-token adapters or LoRA on small exposed subcomponents); (2) more efficient search and cost-reduction strategies (e.g., meta-learned prompt priors, lower-token conversational formats, cheaper surrogate models, or reinforcement-learning-based search); (3) improved multimodal feedback signals and automatic metrics for large-scale evaluation of T2I and prompt inversion to reduce human labeling costs; (4) stronger multimodal LLMs or specialized evaluators to better handle counting, geometry, and abstract reasoning to reduce failure modes; (5) investigating fairness, robustness, and bias mitigation in discovered prompts and downstream behavior; (6) extending to other VLM tasks (VQA, retrieval, captioning, multimodal instruction following) and to per-class or compositional prompt generation; (7) studying transferability and generalization across more diverse VLM architectures and multilingual prompts; (8) integrating prompt inversion for user personalization pipelines and interactive design tools with improved human-in-the-loop interfaces.",
                "experimental_code": "# Core implementation (pseudocode + example API usage) for the black-box LLM-driven prompt search and T2I prompt inversion used in the paper.\n\n# Helpers / data preparation\n# 1) Build initial candidate pool by extracting and masking noun phrases from LAION-COCO captions\n# (high-level description; actual repo contains a script that tokenizes captions, extracts NP spans and replaces object nouns with a placeholder <OBJ>)\n\ndef build_initial_pool(coco_captions_path):\n    \"\"\"Produce a large pool (~2M) of class-agnostic candidate templates by masking noun phrases.\n    Steps:\n      - Load LAION/COCO captions\n      - Use an off-the-shelf POS/NP chunker to extract noun phrases\n      - Replace main noun(s) with a placeholder token (e.g., <OBJ>) to form templates\n      - Deduplicate and filter by length / token count\n    Returns: list of textual templates (strings)\n    \"\"\"\n    pass\n\n# 2) Sample m templates from the pool for a single restart\n\ndef sample_templates(pool, m=100):\n    # randomly sample m templates (uniform or weighted sampling)\n    return random.sample(pool, m)\n\n# Black-box evaluation wrapper\n# In the paper, F(prompt_template) is a black-box function that returns classification accuracy (or other metric)\n\ndef evaluate_prompt_on_vlm(template, vlm_api, few_shot_train_set):\n    \"\"\"Evaluate one textual template on the VLM in few-shot classification.\n    Implementation notes:\n      - For each class, fill template with class name(s) to produce text descriptions\n      - Query the VLM (treated as black box) for accuracy on the few-shot train set\n      - Return scalar score (e.g., top-1 accuracy)\n    \"\"\"\n    # In practice, the repo used CLIP (RN50) as the VLM and computed accuracy by calling local CLIP inference.\n    # But the method does not rely on access to logits — treat returned score as the only feedback signal.\n    pass\n\n# Core stochastic hill-climbing loop with conversational feedback\n\ndef hill_climb_with_llm(pool, vlm_api, llm_api, dataset_splits,\n                       nrestart=20, nreset=50, niter=10, m=100, k=15,\n                       feedback_mode='P+N', llm_model='gpt-3.5-turbo-0301', temperature=1.0):\n    \"\"\"High-level algorithm described in the paper.\n    - nrestart: number of random restarts\n    - nreset: after how many consecutive iterations to reset the chain (prevent local optima)\n    - niter: number of iterations per restart\n    - m: number of sampled prompts per restart\n    - k: number of top/bottom prompts shown to the LLM\n    - feedback_mode: 'P-only' or 'P+N'\n    \"\"\"\n    best_global = None\n    for r in range(nrestart):\n        pool_sample = sample_templates(pool, m=m)\n        # Evaluate all sampled templates on the few-shot train set\n        scores = {t: evaluate_prompt_on_vlm(t, vlm_api, dataset_splits['train']) for t in pool_sample}\n        # Sort prompts by accuracy\n        sorted_prompts = sorted(pool_sample, key=lambda t: scores[t], reverse=True)\n        for it in range(niter):\n            # Select top-k and (optionally) bottom-k\n            top_k = sorted_prompts[:k]\n            bottom_k = sorted_prompts[-k:]\n\n            # Prepare conversational feedback payload for LLM\n            prompt_for_llm = format_feedback_for_llm(top_k, bottom_k if feedback_mode=='P+N' else None,\n                                                    scores, instructions=llm_instructions_template)\n\n            # Call LLM to propose revised templates\n            proposals = call_llm_propose_templates(llm_api, model=llm_model, prompt=prompt_for_llm, temperature=temperature)\n\n            # Evaluate proposals on the VLM\n            for p in proposals:\n                scores[p] = evaluate_prompt_on_vlm(p, vlm_api, dataset_splits['train'])\n\n            # Insert proposals into sorted_prompts, keep pool size m (e.g., replace worst)\n            sorted_prompts = update_sorted_prompts(sorted_prompts, proposals, scores, m)\n\n            # Optionally reset chain after nreset iterations\n            if (it+1) % nreset == 0:\n                sorted_prompts = sample_templates(pool, m=m)\n\n        # After niter iterations pick best from this restart and update global best\n        best_this_restart = sorted_prompts[0]\n        if best_global is None or scores[best_this_restart] > scores[best_global]:\n            best_global = best_this_restart\n\n    return best_global\n\n# Example LLM prompt formatting (system + user messages) used to provide conversational feedback\n# Note: the actual repo contains concrete templates used in API calls; below is a close reconstruction consistent with the paper.\n\nllm_instructions_template = (\n    \"You are a prompt engineer for a vision-language model used for image classification. \"\n    \"We will show you a set of top-performing prompt templates and (optionally) a set of low-performing templates with their accuracy scores on a held-out few-shot training set. \"\n    \"Your task: propose up to N improved, class-agnostic natural-language templates that are likely to increase classification accuracy when used with the given VLM. \"\n    \"Constraints: keep templates short (e.g., <= 40 tokens), maintain <OBJ> as the placeholder for the object class token, do not include class names, and be human-interpretable. \"\n    \"If we provide negative examples, explain briefly why the negatives fail and how your proposals fix them. Return proposals as a numbered list.\"\n)\n\ndef format_feedback_for_llm(top_k, bottom_k, scores, instructions):\n    # Build a text block listing top_k with scores and bottom_k with scores, then append instructions and request for proposals.\n    block = instructions + \"\\n\\nTop-%d templates (with scores):\\n\" % len(top_k)\n    for t in top_k:\n        block += f\"- {t}  (acc={scores[t]:.3f})\\n\"\n    if bottom_k is not None:\n        block += \"\\nBottom-%d templates (with scores):\\n\" % len(bottom_k)\n        for t in bottom_k:\n            block += f\"- {t}  (acc={scores[t]:.3f})\\n\"\n    block += \"\\nProduce 5 improved templates and a one-line rationale for each.\"\n    return block\n\n# Example OpenAI API call (pseudocode)\n# response = openai.ChatCompletion.create(model='gpt-3.5-turbo-0301', messages=[{'role':'system','content':SYSTEM}, {'role':'user','content':block}], temperature=1.0, max_tokens=256)\n\n# Handling T2I / prompt inversion with GPT4-V (multimodal)\n# For T2I tasks the repo used GPT4-V to (a) compare generated images to target and (b) propose refined prompts in JSON format.\n\n# Pseudocode for T2I prompt refinement loop\n\ndef refine_t2i_prompt_with_gpt4v(initial_text_prompt, target_image, image_generator_api,\n                                 llm_api, n_iterations=3, model='gpt-4o-mini-vision' /*or gpt-4v*/):\n    prompt_text = initial_text_prompt\n    for it in range(n_iterations):\n        # Generate an image using the current prompt with black-box image generator (DALL-E 3 used in paper)\n        generated_image = image_generator_api.generate(prompt_text)\n\n        # Ask GPT4-V to compare generated_image vs target_image and return JSON: {score, issues:[...], revised_prompt}\n        system = \"You are an image evaluator. Compare two images: target and generated. Output strict JSON with fields 'faithfulness' (0-1), 'issues' (list of strings), and 'revised_prompt' (a concise improved prompt).\" \n        # Provide multimodal inputs: attach target_image and generated_image as two images in the message\n        user_block = \"Target image (A) and Generated image (B) are provided. Compare them and produce the JSON described above. Keep revised_prompt <= 40 tokens.\"\n\n        response = call_gpt4v_with_images(llm_api, model, system, user_block, images=[target_image, generated_image], temperature=1.0)\n        parsed = parse_json_from_response(response)\n        prompt_text = parsed['revised_prompt']\n    return prompt_text\n\n# Example expected JSON output from GPT4-V for T2I loop\n# {\n#   \"faithfulness\": 0.63,\n#   \"issues\": [\"missing glasses\", \"wrong background color\"],\n#   \"revised_prompt\": \"A portrait of <OBJ> wearing round glasses, soft warm background, high detail\"\n# }\n\n# Variant: P-only vs P+N\n# - P-only: only provide top-k prompts to the LLM (positive examples)\n# - P+N: provide both top-k (positive) and bottom-k (negative) prompts to give implicit negative gradient\n\n# Hyperparameter defaults used in experiments (used directly when calling hill_climb_with_llm):\n# nrestart = 20\n# nreset = 50\n# niter = 10\n# m = 100\n# k = 15\n# llm_model = 'gpt-3.5-turbo-0301' (default), ablations with 'gpt-4*' and 'gpt-4v'\n# llm sampling temperature = 1.0\n\n# Notes on practical implementation details in the repo:\n# - The repo provides scripts to evaluate template buckets on CLIP (RN50) using the CoOp experimental splits. Even when CLIP is used locally, the algorithm treats that evaluation output as the only feedback signal.\n# - Candidate templates were length-filtered and deduplicated (max token length constraint, minimal natural-language checks).\n# - Proposals returned by the LLM are parsed (numbers, bullets, or JSON) with robust parsing logic and heuristics to extract the new templates.\n# - Logging code captures per-iteration prompt lists, LLM calls, and scores for analysis and ablations.\n\n# Cost/accounting helper (used in repo to estimate monetary cost of runs)\n# Example: cost_per_restart_run ~ $0.50 (paper reported ~ $0.50 per restart), total experiment across datasets & folds ~ $100\n",
                "experimental_info": "Datasets and experimental protocol:\n- Few-shot classification benchmark follows CoOp setup across 11 datasets:\n  - ImageNet, Caltech101, DTD, EuroSAT, FGVC-Aircraft, Food101, Oxford Pets, Stanford Cars, SUN397, UCF101, Oxford Flowers\n- Use the same 3-fold k-shot train sets as CoOp; report average accuracy across folds.\n- For each dataset run the search on the few-shot training folds and evaluate final templates on held-out validation/test fold(s) as per CoOp benchmarking.\n\nPrimary VLM and access model:\n- Primary vision-language model used as the black-box classifier: CLIP (RN50 backbone) treated as a black box (function F returns accuracy). The repo also reports results and comparisons to other backbones.\n- Methodology is strictly black-box: no access to model weights, embeddings, or logits during LLM-driven search (evaluation only returns scalar performance metric).\n\nLLMs used:\n- Default conversational LLM: ChatGPT (gpt-3.5-turbo-0301) used as the optimizer in the main experiments.\n- Ablations: GPT-4 family models reported in the paper.\n- Multimodal tasks (T2I, prompt inversion, personalization): GPT4-V (GPT-4 with vision) used to compare generated images to targets and produce structured JSON feedback.\n\nInitial prompt pool:\n- Source: LAION-COCO captions (approximate size of candidate templates ~2M after masking noun phrases to create class-agnostic templates).\n- Template format: natural-language templates with a placeholder token (e.g., <OBJ>) for the class name.\n- Filtering: length/token constraints and deduplication applied.\n\nSearch algorithm & hyperparameters:\n- Search strategy: stochastic hill-climbing with multi-turn conversational feedback (iterative).\n- Random restarts: nrestart = 20\n- Pool per restart: m = 100 sampled candidate templates\n- Iterations per restart: niter = 10\n- Reset frequency: nreset = 50 (reset chain to a new sample to escape local optima)\n- Feedback to LLM: show top-k and optionally bottom-k prompts\n  - k = 15 (default)\n  - Two feedback variants: P-only (only show top-k positive examples) and P+N (show top-k positives and bottom-k negatives)\n- LLM sampling temperature: default = 1.0\n- LLM max tokens and other ChatCompletion params selected conservatively (e.g., max_tokens ~= 256) to balance cost.\n\nT2I and prompt inversion specifics:\n- Image comparison and prompt refinement used GPT4-V (multimodal) which receives both target and generated images.\n- GPT4-V outputs JSON containing: faithfulness score, list of issues, and a revised prompt.\n- Iterations for T2I refinement: typically n_iterations = 3\n- Query sets for T2I evaluation:\n  - 100 challenging queries sampled from Winoground for text-to-image.\n  - 100 queries sampled from DiffusionDB for prompt inversion.\n- Human evaluation: annotators/designers rate faithfulness on a Likert scale 1–5 for generated images; used for final T2I and prompt inversion comparisons.\n\nBaselines and comparisons:\n- White-box baselines: CoOp (continuous prompt tuning), WiSE-FT, Cross-Modal Adaptation, etc.\n- Black-box / privileged access baselines: BlackVIP, LFA, DCLIP.\n- Hand-engineered templates and the initial sampled pool (LAIONCOCO-1M) used as simple baselines.\n\nEvaluation and reporting:\n- Primary metric for classification: average accuracy (top-1) across folds per dataset; reported average across all 11 datasets.\n- Report ablations comparing P-only vs P+N feedback, LLM variants (gpt-3.5 vs gpt-4), number of restarts, and iterative vs non-iterative discovery.\n- Transfer experiments: measure how discovered natural-language prompts transfer across CLIP backbones compared to continuous prompts (e.g., CoOp).\n\nPractical notes and limitations (as recorded in the repo):\n- Cost and compute: method requires many LLM API calls; paper reported approximate cost ~ $0.50 per restart run and ~ $100 total across datasets and folds for the experiments in the paper. These are approximate and depend on API pricing and prompt lengths.\n- Black-box constraints: repository implements the search assuming access only to scalar performance feedback during search; no gradients, logits, or embeddings are used in optimization.\n- Failure modes: documented in experiments (sensitivity to LLM quality, difficulty with counting/geometry, and noisy VLM outputs for complex prompts).\n\nReproducibility artifacts included in the repo:\n- Scripts for building and sampling the initial prompt pool from LAION-COCO captions\n- Evaluation scripts to compute per-template accuracy on CoOp few-shot splits using CLIP (RN50) as the evaluator\n- Orchestrator scripts to perform restarts, call the LLM (ChatCompletion) with structured prompts, parse proposals, and log iterations\n- T2I loop scripts to call multimodal LLM (GPT4-V) with target/generated image pairs and parse JSON feedback\n- Hyperparameter configs and recommended defaults (nrestart, nreset, niter, m, k, temperature) used to reproduce the main results\n- Cost-estimation helper to track LLM API calls and token usage during runs\n\nSummary of key numeric hyperparameters (concise):\n- nrestart=20, nreset=50, niter=10, m=100, k=15, LLM temperature=1.0, initial pool size ~2M templates, T2I iterations=3, T2I query sets=100 each (Winoground, DiffusionDB), human Likert scale 1-5 for T2I evaluation."
            }
        }
    ],
    "refinement_rounds": 1,
    "llm_mapping": {
        "generate_hypothesis": "global.anthropic.claude-haiku-4-5-20251001-v1:0",
        "evaluate_novelty_and_significance": "global.anthropic.claude-haiku-4-5-20251001-v1:0",
        "refine_hypothesis": "global.anthropic.claude-haiku-4-5-20251001-v1:0"
    }
}
