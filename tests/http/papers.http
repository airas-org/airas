### Search paper by title

GET http://127.0.0.1:8000/airas/v1/papers/search/title
Content-Type: application/json

{
    "queries": ["Optimizer"]
}


### Retrieve paper content

GET http://127.0.0.1:8000/airas/v1/papers/content
Content-Type: application/json

{
  "research_study_list": [
    {
      "title": "AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix",
      "abstract": null,
      "full_text": null,
      "image_data": null,
      "references": null,
      "meta_data": null,
      "llm_extracted_info": null
    }
  ]
}


### Summarize paper content

GET http://127.0.0.1:8000/airas/v1/papers/summarize
Content-Type: application/json

{
  "research_study_list": [
    {
      "title": "AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix",
      "abstract": "Adaptive optimizers, such as Adam, have achieved remarkable success in deep\nlearning. A key component of these optimizers is the so-called preconditioning\nmatrix, providing enhanced gradient information and regulating the step size of\neach gradient direction. In this paper, we propose a novel approach to\ndesigning the preconditioning matrix by utilizing the gradient difference\nbetween two successive steps as the diagonal elements. These diagonal elements\nare closely related to the Hessian and can be perceived as an approximation of\nthe inner product between the Hessian row vectors and difference of the\nadjacent parameter vectors. Additionally, we introduce an auto-switching\nfunction that enables the preconditioning matrix to switch dynamically between\nStochastic Gradient Descent (SGD) and the adaptive optimizer. Based on these\ntwo techniques, we develop a new optimizer named AGD that enhances the\ngeneralization performance. We evaluate AGD on public datasets of Natural\nLanguage Processing (NLP), Computer Vision (CV), and Recommendation Systems\n(RecSys). Our experimental results demonstrate that AGD outperforms the\nstate-of-the-art (SOTA) optimizers, achieving highly competitive or\nsignificantly better predictive performance. Furthermore, we analyze how AGD is\nable to switch automatically between SGD and the adaptive optimizer and its\nactual effects on various scenarios. The code is available at\nhttps://github.com/intelligent-machine-learning/atorch/tree/main/atorch/optimizers.",
      "full_text": "AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix Yun Yue∗ Ant Group Hangzhou, Zhejiang, China yueyun.yy@antgroup.com Zhiling Ye ∗ Ant Group Hangzhou, Zhejiang, China yezhiling.yzl@antgroup.com Jiadi Jiang ∗ Ant Group Hangzhou, Zhejiang, China jiadi.jjd@antgroup.com Yongchao Liu Ant Group Hangzhou, Zhejiang, China yongchao.ly@antgroup.com Ke Zhang Ant Group Beijing, China yingzi.zk@antgroup.com Abstract Adaptive optimizers, such as Adam, have achieved remarkable success in deep learning. A key component of these optimizers is the so-called preconditioning matrix, providing enhanced gradient information and regulating the step size of each gradient direction. In this paper, we propose a novel approach to designing the preconditioning matrix by utilizing the gradient difference between two successive steps as the diagonal elements. These diagonal elements are closely related to the Hessian and can be perceived as an approximation of the inner product between the Hessian row vectors and difference of the adjacent parameter vectors. Additionally, we introduce an auto-switching function that enables the preconditioning matrix to switch dynamically between Stochastic Gradient Descent (SGD) and the adaptive optimizer. Based on these two techniques, we develop a new optimizer named AGD that enhances the generalization performance. We evaluate AGD on public datasets of Natural Language Processing (NLP), Computer Vision (CV), and Rec- ommendation Systems (RecSys). Our experimental results demonstrate that AGD outperforms the state-of-the-art (SOTA) optimizers, achieving highly competitive or significantly better predictive performance. Furthermore, we analyze how AGD is able to switch automatically between SGD and the adaptive optimizer and its actual effects on various scenarios. The code is available at this link2. 1 Introduction Consider the following empirical risk minimization problems: min w∈Rn f(w) := 1 M MX k=1 ℓ(w; xk), (1) where w ∈ Rn is the parameter vector to optimize, {x1, . . . ,xM } is the training set, and ℓ(w; x) is the loss function measuring the predictive performance of the parameter w on the example x. Since it is expensive to calculate the full batch gradient in each optimization iteration when M is large, the ∗Co-first authors with equal contributions. 2https://github.com/intelligent-machine-learning/atorch/tree/main/atorch/ optimizers 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2312.01658v2  [cs.LG]  9 Dec 2024standard approach is to adopt a mini-batched stochastic gradient, i.e., g(w) = 1 |B| X k∈B ∇ℓ(w; xk), where B ⊂ {1, . . . , M} is the sample set of size |B| ≪M. Obviously, we have Ep(x)[g(w)] = ∇f(w) where p(x) is the distribution of the training data. Equation (1) is usually solved iteratively. Assume wt is already known and let ∆w = wt+1 − wt, we have arg min wt+1∈Rn f(wt+1) = arg min ∆w∈Rn f(∆w + wt) ≈arg min ∆w∈Rn f(wt) + (∆w)T ∇f(wt) + 1 2(∆w)T ∇2f(wt)∆w ≈arg min ∆w∈Rn f(wt) + (∆w)T mt + 1 2αt (∆w)T Bt∆w, (2) where the first approximation is from Taylor expansion, and the second approximation are from mt ≈ ∇f(wt) (mt denotes the weighted average of gradient gt) and αt ≈ (∆w)T Bt∆w (∆w)T ∇2f(wt)∆w (αt denotes the step size). By solving Equation (2), the general update formula is wt+1 = wt − αtB−1 t mt, t ∈ {1, 2, . . . , T}, (3) where Bt is the so-called preconditioning matrix that adjusts updated velocity of variable wt in each direction. The majority of gradient descent algorithms can be succinctly encapsulated by Equation (3), ranging from the conventional second order optimizer, Gauss-Newton method, to the standard first-order optimizer, SGD, via different combinations of Bt and mt. Table 1 summarizes different implementations of popular optimizers. Table 1: Different optimizers by choosing different Bt. Bt Optimizer Bt = H GAUSS -HESSIAN Bt ≈ H BFGS [4, 13, 14, 31], LBFGS [5] Bt ≈ diag(H) ADAHESSIAN [34] Bt = F NATURAL GRADIENT [2] B2 t ≈ Femp SHAMPOO [16] B2 t ≈ diag(Femp) ADAGRAD [12], ADADELTA [35], ADAM [18], ADAM W [21], AMSG RAD [28] B2 t ≈ diag(Var(gt)) ADABELIEF [39] Bt = I SGD [29], M OMENTUM [27] H is the Hessian. F is the Fisher information matrix. Femp is the empirical Fisher information matrix. Intuitively, the closer Bt approxi- mates the Hessian, the faster conver- gence rate the optimizer can achieve in terms of number of iterations, since the Gauss-Hessian method enjoys a quadratic rate, whereas the gradient descent converges linearly under cer- tain conditions (Theorems 1.2.4, 1.2.5 in Nesterov [25]). However, comput- ing the Hessian is computationally ex- pensive for large models. Thus, it is essential to strike a balance between the degree of Hessian approximation and computational efficiency when de- signing the preconditioning matrix. In this paper, we propose the AGD (Auto-switchable optimizer with Gradient Difference of adjacent steps) optimizer based on the idea of efficiently and effectively acquiring the information of the Hessian. The diagonal entries of AGD’s preconditioning matrix are computed as the difference of gradients between two successive iterations, serving as an approximation of the inner product between the Hessian row vectors and difference of parameter vectors. In addition, AGD is equipped with an adaptive switching mechanism that automatically toggles its preconditioning matrix between SGD and the adaptive optimizer, governed by a threshold hyperparameter δ which enables AGD adaptive to various scenarios. Our contributions can be summarized as follows. • We present a novel optimizer called AGD, which efficiently and effectively integrates the infor- mation of the Hessian into the preconditioning matrix and switches dynamically between SGD and the adaptive optimizer. We establish theoretical results of convergence guarantees for both non-convex and convex stochastic settings. • We validate AGD on six public datasets: two from NLP (IWSLT14 [6] and PTB [23]), two from CV (Cifar10 [19] and ImageNet [30]), and the rest two from RecSys (Criteo [11] and Avazu [3]). The experimental results suggest that AGD is on par with or outperforms the SOTA optimizers. • We analyze how AGD is able to switch automatically between SGD and the adaptive optimizer, and assess the effect of hyperparameter δ which controls the auto-switch process in different scenarios. 2Notation We use lowercase letters to denote scalars, boldface lowercase to denote vectors, and uppercase letters to denote matrices. We employ subscripts to denote a sequence of vectors, e.g., x1, . . . ,xt where t ∈ [T] := {1, 2, . . . , T}, and one additional subscript is used for specific entry of a vector, e.g., xt,i denotes i-th element of xt. For any vectors x, y ∈ Rn, we write xT y or x · y for the standard inner product, xy for element-wise multiplication, x/y for element-wise division, √x for element-wise square root, x2 for element-wise square, and max(x, y) for element-wise maximum. Similarly, any operator performed between a vector x ∈ Rn and a scalar c ∈ R, such as max(x, c), is also element-wise. We denote ∥x∥ = ∥x∥2 = p ⟨x, x⟩ for the standard Euclidean norm, ∥x∥1 = P i |xi| for the ℓ1 norm, and ∥x∥∞ = maxi |xi| for the ℓ∞-norm, where xi is the i-th element of x. Let ft(w) be the loss function of the model at t-step where w ∈ Rn. We consider mt as Exponential Moving Averages (EMA) of gt throughout this paper, i.e., mt = β1mt−1 + (1 − β1)gt = (1 − β1) tX i=1 gt−i+1βi−1 1 , t≥ 1, (4) where β1 ∈ [0, 1) is the exponential decay rate. 2 Related work ASGD [36] leverages Taylor expansion to estimate the gradient at the global step in situations where the local worker’s gradient is delayed, by analyzing the relationship between the gradient difference and Hessian. To approximate the Hessian, the authors utilize the diagonal elements of empirical Fisher information due to the high computational and spatial overhead of Hessian. ADABELIEF [39] employs the EMA of the gradient as the predicted gradient and adapts the step size by scaling it with the difference between predicted and observed gradients, which can be considered as the variance of the gradient. Hybrid optimization methods, including ADABOUND [22] and SWATS [17], have been proposed to enhance generalization performance by switching an adaptive optimizer to SGD . ADABOUND utilizes learning rate clipping on ADAM, with upper and lower bounds that are non-increasing and non-decreasing functions, respectively. One can show that it ultimately converges to the learning rate of SGD . Similarly, SWATS also employs the clipping method, but with constant upper and lower bounds. 3 Algorithm 3.1 Details of AGD optimizer Algorithm 1 AGD 1: Input: parameters β1, β2, δ, w1 ∈ Rn, step size αt, initialize m0 = 0, b0 = 0 2: for t = 1 to T do 3: gt = ∇ft(wt) 4: mt ← β1mt−1 + (1 − β1)gt 5: st = ( m1 1−β1 t = 1 mt 1−βt 1 − mt−1 1−βt−1 1 t >1 6: bt ← β2bt−1 + (1 − β2)s2 t 7: wt+1 = wt − αt √ 1−βt 2 1−βt 1 mt max(√bt,δ √ 1−βt 2) 8: end for 4  2  0 2 4 4 3 2 1 0 1 2 3 4 A B Optimal Adam AGD Figure 1: Trajectories of AGD and Adam in the Beale function. Algorithm 1 summarizes our AGD algorithm. The design of AGD comes from two parts: gradient difference and auto switch for faster convergence and better generalization performance across tasks. 3Gradient difference Our motivation stems from how to efficiently and effectively integrate the information of the Hessian into the preconditioning matrix. Let ∆w = wt − wt−1 and ∇if denote the i-th element of ∇f. From Taylor expansion or the mean value theorem, when ∥∆w∥ is small, we have the following approximation, ∇if(wt) − ∇if(wt−1) ≈ ∇∇if(wt) · ∆w. It means the difference of gradients between adjacent steps can be an approximation of the inner product between the Hessian row vectors and the difference of two successive parameter vectors. To illustrate the effectiveness of gradient difference in utilizing Hessian information, we compare the convergence trajectories of AGD and Adam on the Beale function. As shown in Figure 1, we see that AGD converges much faster than Adam; when AGD reaches the optimal point, Adam has only covered about half of the distance. We select the two most representative points on the AGD trajectory in the figure, the maximum and minimum points of ∥∇f∥1/∥diag(H)∥1, to illustrate how AGD accelerates convergence by utilizing Hessian information. At the maximum point (A), where the gradient is relatively large and the curvature is relatively small (∥∇f∥1 = 22.3, ∥diag(H)∥1 = 25.3), the step size of AGD is 1.89 times that of Adam. At the minimum point (B), where the gradient is relatively small and the curvature is relatively large (∥∇f∥1 = 0.2, ∥diag(H)∥1 = 34.8), the step size decreases to prevent it from missing the optimal point during the final convergence phase. To approximate ∇f(wt), we utilize mt/(1 − βt 1) instead of gt, as the former provides an unbiased estimation of ∇f(wt) with lower variance. According to Kingma and Ba [18], we have E h mt 1−βt 1 i ≊ E[gt], where the equality is satisfied if {gt} is stationary. Additionally, assuming {gt} is strictly stationary and Cov(gi, gj) = 0 if i ̸= j for simplicity and β1 ∈ (0, 1), we observe that Var \u0014 mt 1 − βt 1 \u0015 = 1 (1 − βt 1)2 Var \" (1 − β1) tX i=1 βt−i 1 gi # = (1 + βt 1)(1 − β1) (1 − βt 1)(1 + β1)Var[gt] < Var[gt]. Now, we denote st = \u001a m1/(1 − β1) t = 1, mt/(1 − βt 1) − mt−1/(1 − βt−1 1 ) t >1, and design the preconditioning matrix Bt satisfying B2 t = diag(EMA(s1sT 1 , s2sT 2 , ··· , stsT t ))/(1 − βt 2), where β2 represents the parameter of EMA and bias correction is achieved via the denominator. Note that previous research, such as the one discussed in Section 2 by Zheng et al. [36], has acknowledged the correlation between the difference of two adjacent gradients and the Hessian. However, the key difference is that they did not employ this relationship to construct an optimizer. In contrast, our approach presented in this paper leverages this relationship to develop an optimizer, and its effectiveness has been validated in the experiments detailed in Section 4. Auto switch Typically a small value is added to Bt for numerical stability, resulting in Bt + ϵI. However, in this work we propose to replace this withmax(Bt, δI), where we use a different notation δ to emphasize its crucial role in auto-switch mechanism. In contrast to ϵ, δ can be a relatively large (such as 1e-2). If the element of ˆbt := p bt/(1 − βt 2) exceeds δ, AGD (Line 7 of Algorithm 1) takes a confident adaptive step. Otherwise, the update is performed using EMA, i.e., mt, with a constant scale of αt/(1 − βt 1), similar to SGD with momentum. It’s worth noting that, AGD can automatically switch modes on a per-parameter basis as the training progresses. Compared to the commonly used additive method, AGD effectively eliminates the noise generated by ϵ during adaptive updates. In addition, AGD offers an inherent advantage of being able to generalize across different tasks by tuning the value of δ, obviating the need for empirical choices among a plethora of optimizers. 43.2 Comparison with other optimizers Comparison with AdaBound As noted in Section 2, the auto-switch bears similarities to AdaBound [22] in its objective to enhance the generalization performance by switching to SGD using the clipping method. Nonetheless, the auto-switch’s design differs significantly from AdaBound. Rather than relying solely on adaptive optimization in the early stages, AGD has the flexibility to switch seamlessly between stochastic and adaptive methods, as we will demonstrate in Section 4.5. In addition, AGD outperforms AdaBound’s across various tasks, as we will show in Appendix A.2. 10-15 10-13 10-11 10-9 10-7 10-5 δ (or ϵ) 60 65 70 75 80 85 90 95 100Test accuracy (%) AGD, lr=0.001 AGD, lr=0.005 AGD, lr=0.01 AdaBelief, lr=0.001 AdaBelief, lr=0.005 AdaBelief, lr=0.01 Figure 2: Comparison of stability between AGD and AdaBelief relative to the parameterδ (or ϵ) for ResNet32 on Cifar10. AGD shows better stability over a wide range of δ (or ϵ) variations than Ad- aBelief. Comparison with AdaBelief While in princi- ple our design is fundamentally different from that of AdaBelief [39], which approximates gra- dient variance with its preconditioning matrix, we do see some similarities in our final forms. Compared with the denominator of AGD, the denominator of AdaBelief st = gt − mt = β1 1−β1 (mt −mt−1) lacks bias correction for the subtracted terms and includes a multiplication factor of β1 1−β1 . In addition, we observe that AGD exhibits superior stability compared to Ad- aBelief. As shown in Figure 2, when the value of ϵ deviates from 1e-8 by orders of magnitude, the performance of AdaBelief degrades signif- icantly; in contrast, AGD maintains good stabil- ity over a wide range of δ variations. 3.3 Numerical analysis 3  2  1  0 1 2 3 3 2 1 0 1 2 3 Optimal AGD SGD + Momentum Adam AdaBelief AdaHessian (a) f(x, y) 4  2  0 2 4 4 3 2 1 0 1 2 3 4 Optimal AGD SGD + Momentum Adam AdaBelief AdaHessian (b) Beale 6  4  2  0 2 4 6 6 4 2 0 2 4 6 Optimal AGD SGD + Momentum Adam AdaBelief AdaHessian (c) Rosenbrock Figure 3: Trajectories of different optimizers in three test functions, where f(x, y) = (x + y)2 + (x − y)2/10. We also provide animated versions at https://youtu.be/Qv5X3v5YUw0. In this section, we present a comparison between AGD and several SOTA optimiz- ers on three test functions. We use the parameter set- tings from Zhuang et al. [39], where the learning rate is set to 1e-3 for all adaptive optimizers, along with the same default val- ues of ϵ (or δ) (1e-8) and be- tas (β1 = 0.9, β2 = 0.999). For SGD, we set the mo- mentum to 0.9 and the learn- ing rate to 1e-6 to ensure numerical stability. As shown in Figure 3, AGD exhibits promising results by firstly reaching the optimal points in all experiments before other competitors. In addition, we conduct a search of the largest learning rate for each optimizer with respect to the Beale function, and AGD once again stands out as the strongest method. More details on the search process can be found in Appendix A.3. 4 Experiments 4.1 Experiment setup We extensively compared the performance of various optimizers on diverse learning tasks in NLP, CV , and RecSys; we only vary the settings for the optimizers and keep the other settings consistent in this evaluation. To offer a comprehensive analysis, we provide a detailed description of each task and the optimizers’ efficacy in different application domains. NLP: We conduct experiments using Language Modeling (LM) on Penn TreeBank [23] and Neural Machine Translation (NMT) on IWSLT14 German-to-English (De-En) [6] datasets. For the LM task, 5we train 1, 2, and 3-layer LSTM models with a batch size of 20 for 200 epochs. For the NMT task, we implement the Transformer small architecture, and employ the same pre-processing method and settings as AdaHessian [34], including a length penalty of 1.0, beam size of 5, and max tokens of 4096. We train the model for 55 epochs and average the last 5 checkpoints for inference. We maintain consistency in our learning rate scheduler and warm-up steps. Table 2 provides complete details of the experimental setup. Table 2: Experiments setup. Task Dataset Model Train Val/Test Params 1-layer LSTM 5.3M NLP-LM PTB 2-layer LSTM 0.93M 730K/82K 13.6M 3-layer LSTM 24.2M NLP-NMT IWSLT14 De-En Transformer small 153K 7K/7K 36.7M CV Cifar10 ResNet20/ResNet32 50K 10K 0.27M/0.47M ImageNet ResNet18 1.28M 50K 11.69M RecSys Avazu MLP 36.2M 4.2M 151M Criteo DCN 39.4M 6.6M 270M CV: We conduct experiments us- ing ResNet20 and ResNet32 on the Cifar10 [ 19] dataset, and ResNet18 on the ImageNet [30] dataset, as detailed in Table 2. It is worth noting that the number of param- eters of ResNet18 is significantly larger than that of ResNet20/32, stemming from inconsistencies in ResNet’s naming conventions. Within the ResNet architecture, the consistency in filter sizes, feature maps, and blocks is maintained only within specific datasets. Originally proposed for ImageNet, ResNet18 is more complex compared to ResNet20 and ResNet32, which were tailored for the less demanding Cifar10 dataset. Our training process involves 160 epochs with a learning rate decay at epochs 80 and 120 by a factor of 10 for Cifar10, and 90 epochs with a learning rate decay every 30 epochs by a factor of 10 for ImageNet. The batch size for both datasets is set to 256. RecSys: We conduct experiments on two widely used datasets, Avazu [3] and Criteo [11], which contain logs of display ads. The goal is to predict the Click Through Rate (CTR). We use the samples from the first nine days of Avazu for training and the remaining samples for testing. We employ the Multilayer Perceptron (MLP) structure (a fundamental architecture used in most deep CTR models). The model maps each categorical feature into a 16-dimensional embedding vector, followed by four fully connected layers of dimensions 64, 32, 16, and 1, respectively. For Criteo, we use the first 6/7 of all samples as the training set and last 1/7 as the test set. We adopt the Deep & Cross Network (DCN) [32] with an embedding size of 8, along with two deep layers of size 64 and two cross layers. Detailed summary of the specifications can be found in Table 2. For both datasets, we train them for one epoch using a batch size of 512. Optimizers to compare include SGD [29], Adam [18], AdamW [21], AdaBelief [39] and AdaHessian [34]. To determine each optimizer’s hyperparameters, we adopt the parameters suggested in the literature of AdaHessian and AdaBelief when the experimental settings are identical. Otherwise, we perform hyperparameter searches for optimal settings. A detailed description of this process can be found in Appendix A.1. For our NLP and CV experiments, we utilize GPUs with the PyTorch framework [26], while our RecSys experiments are conducted with three parameter servers and five workers in the TensorFlow framework [1]. To ensure the reliability of our results, we execute each experiment five times with different random seeds and calculate statistical results. 4.2 NLP We report the perplexity (PPL, lower is better) and case-insensitive BiLingual Evaluation Understudy (BLEU, higher is better) score on test set for LM and NMT tasks, respectively. The results are shown in Table 3. For the LM task on PTB, AGD achieves the lowest PPL in all 1,2,3-layer LSTM experiments, as demonstrated in Figure 4. For the NMT task on IWSLT14, AGD is on par with AdaBelief, but outperforms the other optimizers. 4.3 CV Table 4 reports the top-1 accuracy for different optimizers when trained on Cifar10 and ImageNet. It is remarkable that AGD outperforms other optimizers on both Cifar10 and ImageNet. The test accuracy ([µ ± σ]) curves of different optimizers for ResNet20/32 on Cifar10 and ResNet18 on 6Table 3: Test PPL and BLEU score for LM and NMT tasks. † is reported in AdaHessian [34]. Dataset PTB IWSLT14 Metric PPL, lower is better BLEU, higher is better Model 1-layer LSTM 2-layer LSTM 3-layer LSTM Transformer SGD 85.36 ± .34 (−4.13) 67.26 ± .17 (−1.42) 63.68 ± .17 (−2.79) 28.57 ± .15†(+7.37) Adam 84.50 ± .16 (−3.27) 67.01 ± .11 (−1.17) 64.45 ± .26 (−3.56) 32.93 ± .26 (+3.01) AdamW 88.16 ± .19 (−6.93) 95.25 ± 1.33 (−29.41) 102.61 ± 1.13 (−41.72) 35.82 ± .06 (+0.12) AdaBelief 84.40 ± .21 (−3.17) 66.69 ± .23 (−0.85) 61.34 ± .11 (−0.45) 35.93 ± .08 (+0.01) AdaHessian 88.62 ± .15 (−7.39) 73.37 ± .22 (−7.53) 69.51 ± .19 (−8.62) 35.79 ± .06†(+0.15) AGD 81.23 ± .17 65 .84 ± .18 60 .89 ± .09 35 .94 ± .11 0 25 50 75 100 125 150 175 200 Epoch 80 85 90 95 100 105T est PPL SGD Adam AdamW AdaBelief AdaHessian AGD (a) 1-layer 0 25 50 75 100 125 150 175 200 Epoch 65 70 75 80 85 90 95 100T est PPL SGD Adam AdamW AdaBelief AdaHessian AGD (b) 2-layer 0 25 50 75 100 125 150 175 200 Epoch 60 65 70 75 80 85 90 95 100 105T est PPL SGD Adam AdamW AdaBelief AdaHessian AGD (c) 3-layer Figure 4: Test PPL ([µ ± σ]) on Penn Treebank for 1,2,3-layer LSTM. 0 20 40 60 80 100 120 140 160 Epoch 0.4 0.5 0.6 0.7 0.8 0.9T est accuracy 120 140 160 0.915 0.920 0.925 SGD Adam AdamW AdaBelief AdaHessian AGD (a) ResNet20 on Cifar10 0 20 40 60 80 100 120 140 160 Epoch 0.3 0.4 0.5 0.6 0.7 0.8 0.9T est accuracy 120 140 160 0.925 0.930 SGD Adam AdamW AdaBelief AdaHessian AGD (b) ResNet32 on Cifar10 0 20 40 60 80 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7T est accuracy 60 80 0.68 0.70 SGD Adam AdamW AdaHessian AdaBelief AGD (c) ResNet18 on ImageNet Figure 5: Test accuracy ([µ ± σ]) of different optimizers for ResNet20/32 on Cifar10 and ResNet18 on ImageNet. ImageNet are illustrated in Figure 5. Notice that the numbers of SGD and AdaHessian on ImageNet are lower than the numbers reported in original papers [7, 34], which were run only once (we average multiple trials here). AdaHessian can achieve 70.08% top-1 accuracy in Yao et al. [34] while we report 69.57 ± 0.12%. Due to the limited training details provided in Yao et al. [34], it is difficult for us to explain the discrepancy. However, regardless of which result of AdaHessian is taken, AGD outperforms AdaHessian significantly. Our reported top-1 accuracy of SGD is 69.94 ± 0.10%, slightly lower than 70.23% reported in Chen et al. [7]. We find that the differences in training epochs, learning rate scheduler and weight decay rate are the main reasons. We also run the experiment using the same configuration as in Chen et al. [7], and AGD can achieve 70.45% accuracy at lr = 4e-4 and δ = 1e-5, which is still better than the 70.23% result reported in Chen et al. [7]. We also report the accuracy of AGD for ResNet18 on Cifar10 for comparing with the SOTA results 3, which is listed in Appendix A.5. Here we clarify again the ResNet naming confusion. The test accuracy of ResNet18 on Cifar10 training with AGD is above 95%, while ResNet32 is about 93% since ResNet18 is much more complex than ResNet32. 4.4 RecSys To evaluate the accuracy of CTR estimation, we have adopted the Area Under the receiver-operator Curve (AUC) as our evaluation criterion, which is widely recognized as a reliable measure [ 15]. As stated in Cheng et al. [10], Wang et al. [32], Ling et al. [20], Zhu et al. [38], even an absolute 3https://paperswithcode.com/sota/stochastic-optimization-on-cifar-10-resnet-18 7Table 4: Top-1 accuracy for different optimizers when trained on Cifar10 and ImageNet. Dataset Cifar10 ImageNet Model ResNet20 ResNet32 ResNet18 SGD 92.14 ± .14 (+0.21) 93.10 ± .07 (+0.02) 69.94 ± .10 (+0.41) Adam 90.46 ± .20 (+1.89) 91.54 ± .12 (+1.58) 64.03 ± .16 (+6.32) AdamW 92.12 ± .14 (+0.23) 92.72 ± .01 (+0.40) 69.11 ± .17 (+1.24) AdaBelief 92.19 ± .15 (+0.16) 92.90 ± .13 (+0.22) 70.20 ± .03 (+0.15) AdaHessian 92.27 ± .27 (+0.08) 92.91 ± .14 (+0.21) 69.57 ± .12 (+0.78) AGD 92.35 ± .24 93 .12 ± .18 70 .35 ± .17 Table 5: Test AUC for different optimizers when trained on Avazu and Criteo. Dataset Avazu Criteo Model MLP DCN SGD 0.7463 ± .0005 (+1.7‰) 0.7296 ± .0067 (+72.7‰) Adam 0.7458 ± .0010 (+2.2‰) 0.8023 ± .0002 (+0.0‰) AdaBelief 0.7467 ± .0009 (+1.3‰) 0.8022 ± .0002 (+0.1‰) AdaHessian 0.7434 ± .0006 (+4.6‰) 0.8004 ± .0005 (+1.9‰) AGD 0.7480 ± .0008 0 .8023 ± .0004 improvement of 1‰ in AUC can be considered practically significant given the difficulty of improving CTR prediction. Our experimental results in Table 5 indicate that AGD can achieve highly competitive or significantly better performance when compared to other optimizers. In particular, on the Avazu task, AGD outperforms all other optimizers by more than 1‰. On the Criteo task, AGD performs better than SGD and AdaHessian, and achieves comparable performance to Adam and AdaBelief. 4.5 The effect of δ 0 10 20 30 40 50 60 70 80 90 Epoch 0 20 40 60 80 100Ratio (%) 86.95 82.2 80.93 80.57 22.68 5.03 2.63 1.58 1.07 0.91 [0, 1e-8) [1e-8, 1e-5) [1e-5, 1e-4) [1e-4, + ) (a) ResNet18 on ImageNet 0 10 20 30 40 50 Epoch 0 20 40 60 80 100Ratio (%) 0.0 0.05 0.21 0.29 0.32 0.34 0.34 0.35 0.35 0.35 0.35 [0, 1e-14) [1e-14, 1e-6) [1e-6, 1e-5) [1e-5,+ )  (b) Transformer on IWSLT14 0 25 50 75 100 125 150 175 200 Epoch 0 20 40 60 80 100Ratio (%) 97.05 97.06 97.01 96.97 96.39 95.27 95.12 [0, 1e-6) [1e-6, 1e-5) [1e-5, 1e-4) [1e-4,+ )  (c) 2-layer LSTM on PTB 0 10000 20000 30000 40000 50000 60000 70000 Global Step 0 20 40 60 80 100Ratio (%) 100.0 84.03 71.52 60.16 49.51 39.09 29.09 20.15 14.7 [0, 1e-10) [1e-10, 1e-8) [1e-8, 1e-6) [1e-6, 1e-4) [1e-4, + )  (d) DCN on Criteo Figure 6: The distribution of ˆbt on different epochs/steps. The colored area denotes the ratio of ˆbt in the corresponding interval. The values of δ for ResNet18 on ImageNet, Transformer on IWSLT14, 2-layer LSTM on PTB, and DCN on Criteo are 1e-5, 1e-14, 1e-5 and 1e-8, respectively. In this section, we aim to provide a comprehensive analysis of the impact of δ on the training process by precisely determining the percentage of ˆbt that Algorithm 1 truncates. To this end, Figure 6 shows the distribution of ˆbt across various tasks under the optimal configuration that we have identified. The black dot on the figure provides the precise percentage of ˆbt that δ truncates during the training process. Notably, a lower percentage indicates a higher degree of SGD-like updates compared to adaptive steps, which can be adjusted through δ. As SGD with momentum generally outperforms adaptive optimizers on CNN tasks [ 34, 39], we confirm this observation as illustrated in Figure 6a: AGD behaves more like SGD during the initial stages of training (before the first learning rate decay at the 30th epoch) and switches to adaptive optimization for fine-tuning. Figure 6b indicates that the parameters taking adaptive updates are dominant, as expected because adaptive optimizers such as AdamW are preferred in transformers. Figure 6c demonstrates that most parameters update stochastically, which explains why AGD has a similar curve to SGD in Figure 4b before the 100th epoch. The proportion of parameters taking adaptive updates grows from 3% to 5% afterward, resulting in a better PPL in the fine-tuning stage. Concerning Figure 6d, the model of the RecSys task trains for only one epoch, and AGD gradually switches to adaptive updates for a better fit to the data. 4.6 Computational cost We train a Transformer small model for IWSLT14 on a single NVIDIA P100 GPU. AGD is comparable to the widely used AdamW optimizer, while significantly outperforms AdaHessian in terms of memory footprint and training speed. As a result, AGD can be a drop-in replacement for AdamW with similar computation cost and better generalization performance. 8Table 6: Computational cost for Transformer small. Optimizer Memory Time per Epoch Relative time to AdamW SGD 5119 MB 230 s 0.88× AdamW 5413 MB 260 s 1.00× AdaHessian 8943 MB 750 s 2.88× AGD 5409 MB 278 s 1.07× 5 Theoretical analysis Using the framework developed in Reddi et al. [28], Yang et al. [33], Chen et al. [9], Zhou et al. [37], we have the following theorems that provide the convergence in non-convex and convex settings. Particularly, we use β1,t to replace β1, where β1,t is non-increasing with respect to t. Theorem 1. (Convergence in non-convex settings) Suppose that the following assumptions are satisfied: 1. f is differential and lower bounded, i.e., f(w∗) > −∞ where w∗ is an optimal solution. f is also L-smooth, i.e., ∀u, v ∈ Rn, we have f(u) ≤ f(v) + ⟨∇f(v), u − v⟩ + L 2 ∥u − v∥2. 2. At step t, the algorithm can access a bounded noisy gradient and the true gradient is bounded, i.e., ∥gt∥∞ ≤ G∞, ∥∇f(wt)∥∞ ≤ G∞, ∀t ∈ [T]. Without loss of generality, we assume G∞ ≥ δ. 3. The noisy gradient is unbiased and the noise is independent, i.e., gt = ∇f(wt) + ζt, E[ζt] = 0 and ζi is independent of ζj if i ̸= j. 4. αt = α/ √ t, β1,t is non-increasing satisfying β1,t ≤ β1 ∈ [0, 1), β2 ∈ [0, 1) and bt,i ≤ bt+1,i ∀i ∈ [n]. Then Algorithm 1 yields min t∈[T] E[∥∇f(wt)∥2] < C3 1√ T − √ 2 + C4 log T√ T − √ 2 + C5 PT t=1 ˆαt(β1,t − β1,t+1)√ T − √ 2 , (5) where C3, C4 and C5 are defined as follows: C3 = G∞ α(1 − β1)2(1 − β2)2 \u0012 f(w1) − f(w∗) + nG2 ∞α (1 − β1)8δ2 (δ + 8Lα) + αβ1nG2 ∞ (1 − β1)3δ \u0013 , C4 = 15LnG3 ∞α 2(1 − β2)2(1 − β1)10δ2 , C 5 = nG3 ∞ α(1 − β1)5(1 − β2)2δ . The proof of Theorem 1 is presented in Appendix B. There are two important points that should be noted: Firstly, in assumption 2, we can employ the gradient norm clipping technique to ensure the upper bound of the gradients. Secondly, in assumption 4, bt,i ≤ bt+1,i ∀i ∈ [n], which is necessary for the validity of Theorems 1 and 2, may not always hold. To address this issue, we can implement the AMSGrad condition [28] by setting bt+1 = max(bt+1, bt). However, this may lead to a potential decrease in the algorithm’s performance in practice. The more detailed analysis is provided in Appendix A.4. From Theorem 1, we have the following corollaries. Corollary 1. Suppose β1,t = β1/ √ t, we have min t∈[T] E[∥∇f(wt)∥2] < C3 1√ T − √ 2 + C4 log T√ T − √ 2 + C5α 1 − β1 log T + 1√ T − √ 2 , where C3, C4 and C5 are the same with Theorem 1. The proof of Corollary 1 can be found in Appendix C. Corollary 2. Suppose β1,t = β1, ∀t ∈ [T], we have min t∈[T] E[∥∇f(wt)∥2] < C3 1√ T − √ 2 + C4 log T√ T − √ 2 , where C3 and C4 are the same with Theorem 1. 9Corollaries 1 and 2 imply the convergence (to the stationary point) rate for AGD is O(log T/ √ T) in non-convex settings. Theorem 2. (Convergence in convex settings) Let{wt} be the sequence obtained by AGD (Algorithm 1), αt = α/ √ t, β1,t is non-increasing satisfying β1,t ≤ β1 ∈ [0, 1), β2 ∈ [0, 1), bt,i ≤ bt+1,i ∀i ∈ [n] and ∥gt∥∞ ≤ G∞, ∀t ∈ [T]. Suppose ft(w) is convex for all t ∈ [T], w∗ is an optimal solution of PT t=1 ft(w), i.e., w∗ = arg minw∈Rn PT t=1 ft(w) and there exists the constant D∞ such that maxt∈[T] ∥wt − w∗∥∞ ≤ D∞. Then we have the following bound on the regret TX t=1 (ft(wt) − ft(w∗)) < 1 1 − β1   C1 √ T + TX t=1 β1,t 2ˆαt nD2 ∞ + C2 √ T ! , where C1 and C2 are defined as follows: C1 = n(2G∞ + δ)D2 ∞ 2α√1 − β2(1 − β1)2 , C 2 = nαG2 ∞ (1 − β1)3 \u0012 1 + 1 δ√1 − β2 \u0013 . The proof of Theorem 2 is given in Appendix D. To ensure that the condition maxt∈[T] ∥wt − w∗∥∞ ≤ D∞ holds, we can assume that the domain W ⊆Rn is bounded and project the sequence {wt} onto W by setting wt+1 = ΠW \u0012 wt − αt √ 1−βt 2 1−βt 1 mt max(√bt,δ √ 1−βt 2) \u0013 . From Theorem 2, we have the following corollary. Corollary 3. Suppose β1,t = β1/t, we have TX t=1 (ft(wt) − ft(w∗)) < 1 1 − β1 \u0012 C1 √ T + nD2 ∞β1 α√1 − β2 √ T + C2 √ T \u0013 , where C1 and C2 are the same with Theorem 2. The proof of Corollary 3 is given in Appendix E. Corollary 3 implies the regret is O( √ T) and can achieve the convergence rate O(1/ √ T) in convex settings. 6 Conclusion In this paper, we introduce a novel optimizer, AGD, which incorporates the Hessian information into the preconditioning matrix and allows seamless switching between SGD and the adaptive optimizer. We provide theoretical convergence rate proofs for both non-convex and convex stochastic settings and conduct extensive empirical evaluations on various real-world datasets. The results demonstrate that AGD outperforms other optimizers in most cases, resulting in significant performance improvements. Additionally, we analyze the mechanism that enables AGD to automatically switch between stochastic and adaptive optimization and investigate the impact of the hyperparameter δ for this process. Acknowledgement We thank Yin Lou for his meticulous review of our manuscript and for offering numerous valuable suggestions. References [1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning. In Kimberly Keeton and Timothy Roscoe, editors, 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016, pages 265–283. USENIX Association, 2016. [2] Shun-ichi Amari. Natural gradient works efficiently in learning. Neural Comput., 10(2):251–276, 1998. doi: 10.1162/089976698300017746. 10[3] Avazu. Avazu click-through rate prediction. https://www.kaggle.com/c/avazu-ctr-prediction/ data, 2015. [4] Charles G. Broyden. The convergence of a class of double-rank minimization algorithms. Journal of the Institute of Mathematics and Its Applications, 6:6–90, 1970. doi: 10 .1093/imamat/6.1.76. [5] Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for bound constrained optimization. SIAM J. Sci. Comput., 16(5):1190–1208, 1995. doi: 10 .1137/0916069. [6] Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa Bentivogli, and Marcello Federico. Report on the 11th IWSLT evaluation campaign. In Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign, pages 2–17, Lake Tahoe, California, December 4-5 2014. [7] Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. Closing the generalization gap of adaptive gradient methods in training deep neural networks. pages 3239–3247, 07 2020. doi: 10 .24963/ijcai.2020/448. [8] Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. Closing the generalization gap of adaptive gradient methods in training deep neural networks. In Christian Bessiere, editor, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, pages 3267–3275. ijcai.org, 2020. doi: 10 .24963/ijcai.2020/452. [9] Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of A class of adam-type algorithms for non-convex optimization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [10] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. Wide & deep learning for recommender systems. In Alexandros Karatzoglou, Balázs Hidasi, Domonkos Tikk, Oren Sar Shalom, Haggai Roitman, Bracha Shapira, and Lior Rokach, editors, Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, DLRS@RecSys 2016, Boston, MA, USA, September 15, 2016 , pages 7–10. ACM, 2016. doi: 10 .1145/ 2988450.2988454. [11] Criteo. Criteo display ad challenge. http://labs.criteo.com/2014/02/kaggle-display- advertising-challenge-dataset, 2014. [12] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2011. doi: 10 .5555/ 1953048.2021068. [13] R. Fletcher. A new approach to variable metric algorithms. Comput. J., 13(3):317–322, 1970. doi: 10.1093/comjnl/13.3.317. [14] Donald Goldfarb. A family of variable metric updates derived by variational means. Mathematics of Computation, 24(109):23–26, 1970. doi: 10 .1090/S0025-5718-1970-0258249-6. [15] Thore Graepel, Joaquin Quiñonero Candela, Thomas Borchert, and Ralf Herbrich. Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft’s bing search engine. In Johannes Fürnkranz and Thorsten Joachims, editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pages 13–20. Omnipress, 2010. [16] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 1837–1845. PMLR, 2018. [17] Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from adam to SGD. CoRR, abs/1712.07628, 2017. [18] Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations, ICLR ’15, San Diego, CA, USA, 2015. [19] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). http://www.cs.toronto.edu/~kriz/cifar.html, 2009. [20] Xiaoliang Ling, Weiwei Deng, Chen Gu, Hucheng Zhou, Cui Li, and Feng Sun. Model ensemble for click prediction in bing search ads. In Rick Barrett, Rick Cummings, Eugene Agichtein, and Evgeniy Gabrilovich, editors, Proceedings of the 26th International Conference on World Wide Web Companion, Perth, Australia, April 3-7, 2017, pages 689–698. ACM, 2017. doi: 10 .1145/3041021.3054192. 11[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [22] Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic bound of learning rate. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [23] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn treebank, June 1993. [24] Thomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier, Benjamin Charlier, Mathieu Dagréou, Tom Dupré la Tour, Ghislain Durif, Cassio F. Dantas, Quentin Klopfenstein, Johan Larsson, En Lai, Tanguy Lefort, Benoit Malézieux, Badr Moufad, Binh T. Nguyen, Alain Rako- tomamonjy, Zaccharie Ramzi, Joseph Salmon, and Samuel Vaiter. Benchopt: Reproducible, efficient and collaborative optimization benchmarks. 2022. [25] Yurii E. Nesterov. Introductory Lectures on Convex Optimization - A Basic Course, volume 87 of Applied Optimization. Springer, 2004. ISBN 978-1-4613-4691-3. doi: 10 .1007/978-1-4419-8853-9. URL https://doi.org/10.1007/978-1-4419-8853-9 . [26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 8024–8035, 2019. [27] Boris T. Polyak. Some methods of speeding up the convergence of iteration methods.USSR Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964. doi: 10 .1016/0041-5553(64)90137-5. [28] Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. InProceedings of the 6th International Conference on Learning Representations, ICLR ’18, Vancouver, BC, Canada, 2018. OpenReview.net. [29] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400–407, 1951. [30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. doi: 10 .1007/s11263-015-0816-y. [31] David F. Shanno. Conditioning of quasi-newton methods for function minimization. Mathematics of Computation, 24(111):647–656, 1970. doi: 10 .1090/S0025-5718-1970-0274029-X. [32] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. Deep & cross network for ad click predictions. In Proceedings of the ADKDD’17, Halifax, NS, Canada, August 13 - 17, 2017, pages 12:1–12:7. ACM, 2017. doi: 10.1145/3124749.3124754. [33] Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization. CoRR, abs/1604.03257v2, 2016. [34] Zhewei Yao, Amir Gholami, Sheng Shen, Kurt Keutzer, and Michael W. Mahoney. ADAHESSIAN: an adaptive second order optimizer for machine learning. CoRR, abs/2006.00719, 2020. [35] Matthew D. Zeiler. Adadelta: An adaptive learning rate method. CoRR, abs/1212.5701, 2012. [36] Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhiming Ma, and Tie-Yan Liu. Asyn- chronous stochastic gradient descent with delay compensation. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 4120–4129. PMLR, 2017. [37] Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. On the convergence of adaptive gradient methods for nonconvex optimization. CoRR, abs/1808.05671, 2018. 12[38] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. Open benchmarking for click-through rate prediction. In Gianluca Demartini, Guido Zuccon, J. Shane Culpepper, Zi Huang, and Hanghang Tong, editors, CIKM ’21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021, pages 2759–2769. ACM, 2021. doi: 10.1145/3459637.3482486. URL https://doi.org/10.1145/3459637.3482486. [39] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C. Tatikonda, Nicha C. Dvornek, Xenophon Pa- pademetris, and James S. Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan- Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 13Appendix A Details of experiments A.1 Configuration of optimizers In this section, we provide a thorough description of the hyperparameters used for different optimizers across var- ious tasks. For optimizers other than AGD, we adopt the recommended parameters for the identical experimental setup as indicated in the literature of AdaHessian [34] and AdaBelief [39]. In cases where these recommendations are not accessible, we perform a hyperparameter search to determine the optimal hyperparameters. NLP • SGD/Adam/AdamW: For the NMT task, we report the results of SGD from AdaHessian [ 34], and search learning rate among {5e-5, 1e-4, 5e-4, 1e-3} and epsilon in {1e-12, 1e-10, 1e-8, 1e-6, 1e-4} for Adam/AdamW, and for both optimizers the optimal learning rate/epsilon is 5e-4/1e-12. For the LM task, we follow the settings from AdaBelief [39], setting learning rate to 30 for SGD and 0.001 for Adam/AdamW while epsilon to 1e-12 for Adam/AdamW when training 1-layer LSTM. For 2-layer LSTM, we conduct a similar search with learning rate among {1e-4, 5e-4, 1e-3, 1e-2} and epsilon within {1e-12, 1e-10, 1e-8, 1e-6, 1e-4}, and the best parameters are learning rate = 1e-2 and epsilon = 1e-8/1e-4 for Adam/AdamW. For 3-layer LSTM, learning rate = 1e-2 and epsilon = 1e-8 are used for Adam/AdamW. • AdaBelief: For the NMT task we use the recommended configuration from the latest implementation4 for transformer. We search learning rate in {5e-5, 1e-4, 5e-4, 1e-3} and epsilon in {1e-16, 1e-14, 1e-12, 1e-10, 1e-8}, and the best configuration is to set learning rate as 5e-4 and epsilon as 1e-16. We adopt the same LSTM experimental setup for the LM task and reuse the optimal settings provided by AdaBelief [39], except for 2-layer LSTM, where we search for the optimial learning rate in {1e-4, 5e-4, 1e-3, 1e-2} and epsilon in {1e-16, 1e-14, 1e-12, 1e-10, 1e-8}. However, the best configuration is identical to the recommended. • AdaHessian: For the NMT task, we adopt the same experimental setup as in the official implementation.5 For LM task, we search the learning rate among {1e-3, 1e-2, 0.1, 1} and hessian power among {0.5, 1, 2}. We finally select 0.1 for learning rate and 0.5 for hessian power for 1-layer LSTM, and 1.0 for learning rate and 0.5 for for hessian power for 2,3-layer LSTM. Note that AdaHessian appears to overfit when using learning rate 1.0. Accordingly, we also try to decay its learning rate at the 50th/90th epoch, but it achieves a similar PPL. • AGD: For the NMT task, we search learning rate among {5e-5, 1e-4, 5e-4, 1e-3} and δ among {1e-16, 1e-14, 1e-12, 1e-10, 1e-8}. We report the best result with learning rate 5e-5 and δ as 1e-14 for AGD. For the LM task, we search learning rate among {1e-4, 5e-4, 1e-3, 5e-3, 1e-2} and δ from 1e-16 to 1e-4, and the best settings for learning rate (δ) is 5e-4 (1e-10) and 1e-3 (1e-5) for 1-layer LSTM (2,3-layer LSTM). The weight decay is set to 1e-4 (1.2e-6) for all optimizers in the NMT (LM) task. For adaptive optimizers, we set (β1, β2) to (0.9, 0.98) in the NMT task and (0.9, 0.999) in the LM task. For the LM task, the general dropout rate is set to 0.4. CV • SGD/Adam/AdamW: We adopt the same experimental setup in AdaHessian [34]. For SGD, the initial learning rate is 0.1 and the momentum is set to 0.9. For Adam, the initial learning rate is set to 0.001 and the epsilon is set to 1e-8. For AdamW, the initial learning rate is set to 0.005 and the epsilon is set to 1e-8. • AdaBelief: We explore the best learning rate for ResNet20/32 on Cifar10 and ResNet18 on ImageNet, respectively. Finally, the initial learning rate is set to be 0.01 for ResNet20 on Cifar10 and 0.005 for ResNet32/ResNet18 on Cifar10/ImageNet. The epsilon is set to 1e-8. • AdaHessian: We use the recommended configuration as much as possible from AdaHessian [ 34]. The Hessian power is set to 1. The initial learning rate is 0.15 when training on both Cifar10 and ImageNet, as recommended in Yao et al. [34]. The epsilon is set to 1e-4. • AGD: We conduct a grid search of δ and the learning rate. The choice of δ is among {1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2} and the search range for the learning rate is from 1e-4 to 1e-2. Finally, we choose the learning rate to 0.007 and δ to 1e-2 for Cifar10 task, and learning rate to 0.0004 and δ to 1e-5 for ImageNet task. The weight decay for all optimizers is set to 0.0005 on Cifar10 and 0.0001 on ImageNet. β1 = 0.9 and β2 = 0.999 are for all adaptive optimizers. 4https://github.com/juntang-zhuang/Adabelief-Optimizer 5https://github.com/amirgholami/adahessian 14RecSys Note that we implement the optimizers for training on our internal distributed environment. • SGD: We search for the learning rate among {1e-4, 1e-3, 1e-2, 0.1, 1} and choose the best results (0.1 for the Avazu task and 1e-3 for the Criteo task). • Adam/AdaBelief: We search the learning rate among {1e-5, 1e-4, 1e-3, 1e-2} and the epsilon among {1e-16, 1e-14, 1e-12, 1e-10, 1e-8, 1e-6}. For the Avazu task, the best learning rate/epsilon is 1e-4/1e-8 for Adam and 1e-4/1e-16 for AdaBelief. For the Criteo task, the best learning rate/epsilon is 1e-3/1e-8 for Adam and 1e-3/1e-16 for AdaBelief. • AdaHessian: We search the learning rate among {1e-5, 1e-4, 1e-3, 1e-2} and the epsilon among {1e-16, 1e-14, 1e-12, 1e-10, 1e-8, 1e-6}. The best learning rate/epsilon is 1e-4/1e-8 for the Avazu task and 1e-3/1e-6 for the Criteo task. The block size and the Hessian power are set to 1. • AGD: We search the learning rate among {1e-5, 1e-4, 1e-3} and δ among {1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2}. The best learning rate/δ is 1e-4/1e-4 for the Avazu task and 1e-4/1e-8 for the Criteo task. β1 = 0.9 and β2 = 0.999 are for all adaptive optimizers. A.2 AGD vs. AdaBound Table 7: The performance of AGD and AdaBound across different tasks. Optimizer 3-Layer LSTM, test PPL (lower is better) ResNet18 on ImageNet, Top-1 accuracy AdaBound 63.60 [39] 68.13 (100 epochs) [8] AGD 60.89 (better) 70.19 (90 epochs, still better) A.3 Numerical Experiments In our numerical experiments, we employ the same learning rate across optimizers. While a larger learning rate could accelerate convergence, as shown in Figures 7a and 7b, we also note that it could lead to unstable training. To investigate the largest learning rate that each optimizer could handle for the Beale function, we perform a search across the range of {1e-5, 1e-4, ..., 1, 10}. The optimization trajectories are displayed in Figure 7c, and AGD demonstrates slightly superior performance. Nonetheless, we argue that the learning rate selection outlined in Section 3.3 presents a more appropriate representation. 4  3  2  1  0 1 2 3 4 4 3 2 1 0 1 2 3 4 Optimal Adam, lr=1 Adam, lr=1e-1 Adam, lr=1e-2 Adam, lr=1e-3 Adam, lr=1e-4 (a) Adam 4  3  2  1  0 1 2 3 4 4 3 2 1 0 1 2 3 4 Optimal AGD, lr=1 AGD, lr=1e-1 AGD, lr=1e-2 AGD, lr=1e-3 AGD, lr=1e-4 (b) AGD 4  3  2  1  0 1 2 3 4 4 3 2 1 0 1 2 3 4 Optimal SGDM, lr=1e-5 Adam, lr=1 AdaBelief AGD, lr=1 AdaHessian, lr=1e-1 (c) Comparison Figure 7: Optimization trajectories on Beale function using various learning rates. A.4 AGD with AMSGrad condition Algorithm 2 summarizes the AGD optimizer with AMSGrad condition. The AMSGrad condition is usually used to ensure the convergence. We empirically show that adding AMSGrad condition to AGD will slightly degenerate AGD’s performance, as listed in Table 8. For AGD with AMSGrad condition, we search for the best learning rate among {0.001, 0.003, 0.005, 0.007, 0.01} and best δ within {1e-2, 1e-4, 1e-6, 1e-8}.We find the best learning rate is 0.007 and best δ is 1e-2, which are the same as AGD without AMSGrad condition. A.5 AGD for ResNet18 on Cifar10 Since the SOTA accuracy6 for ResNet18 on Cifar10 is 95.55% using SGD optimizer with a cosine learning rate schedule, we also test the performance of AGD for ResNet18 on Cifar10. We find AGD can achieve95.79% 6https://paperswithcode.com/sota/stochastic-optimization-on-cifar-10-resnet-18 15Algorithm 2 AGD with AMSGrad condition 1: Input: parameters β1, β2, δ, w1 ∈ Rn, step size αt, initialize m0 = 0, b0 = 0 2: for t = 1 to T do 3: gt = ∇ft(wt) 4: mt ← β1mt−1 + (1 − β1)gt 5: st = \u001a m1/(1 − β1) t = 1 mt/(1 − βt 1) − mt−1/(1 − βt−1 1 ) t >1 6: bt ← β2bt−1 + (1 − β2)s2 t 7: bt ← max(bt, bt−1) // AMSGrad condtion 8: wt+1 = wt − αt √ 1−βt 2 1−βt 1 mt max(√bt,δ √ 1−βt 2) 9: end for Table 8: Top-1 accuracy for AGD with and without AMSGrad condition when trained with ResNet20 on Cifar10. Optimizer AGD AGD + AMSGrad Accuracy 92.35 ±.24 92.25 ± 0.11 accuracy at lr = 0.001 and δ = 1e-2 when using the same training configuration as the experiment of SGD in Moreau et al. [24], which exceeds the current SOTA result. A.6 Robustness to hyperparameters We test the performance of AGD and Adam with respect to δ (or ϵ) and learning rate. The experiments are performed with ResNet20 on Cifar10 and the results are shown in Figure 8. Compared to Adam, AGD shows better robustness to the change of hyperparameters. 0 20 40 60 80 100 120 140 160 Epoch 0.3 0.4 0.5 0.6 0.7 0.8 0.9T est accuracy lr=0.0001 lr=0.0005 lr=0.0007 lr=0.001 lr=0.005 lr=0.007 lr=0.01 (a) AGD 0 20 40 60 80 100 120 140 160 Epoch 0.4 0.5 0.6 0.7 0.8 0.9T est accuracy lr=0.0001 lr=0.0005 lr=0.0007 lr=0.001 lr=0.005 lr=0.007 lr=0.01 (b) Adam 0 20 40 60 80 100 120 140 160 Epoch 0.3 0.4 0.5 0.6 0.7 0.8 0.9T est accuracy =1e-2 =1e-3 =1e-4 =1e-5 =1e-6 =1e-7 =1e-8 (c) AGD 0 20 40 60 80 100 120 140 160 Epoch 0.4 0.5 0.6 0.7 0.8 0.9T est accuracy =1e-2 =1e-3 =1e-4 =1e-5 =1e-6 =1e-7 =1e-8  (d) Adam Figure 8: Test accuracy of ResNet20 on Cifar10, trained with AGD and Adam using different δ (or ϵ) and learning rate. For (a) and (b), we choose learning rate as 0.007 and 0.001, respectively. For (c) and (d), we set δ (or ϵ) to be 1e-2 and 1e-8, respectively. 16B Proof of Theorem 1 Proof. Denote ˆαt = αt √ 1−βt 2 1−βt 1,t and vt = max(√bt, δ p 1 − βt 2), we have the following lemmas. Lemma 1. For the parameter settings and assumptions in Theorem 1 or Theorem 2, we have ˆαt > ˆαt+1, t∈ [T], where ˆαt = αt √ 1−βt 2 1−βt 1,t . Proof. Since β1,t is non-increasing with respect to t, we have βt+1 1,t+1 ≤ βt 1,t+1 ≤ βt 1,t. Hence, βt 1,t is non- increasing and 1 1−βt 1,t is non-increasing. Thus, we only need to prove ϕ(t) =αt p 1 − βt 2 is decreasing. Since the proof is trivial when β2 = 0, we only need to consider the case where β2 ∈ (0, 1). Taking the derivative of ϕ(t), we have ϕ′(t) =−α 2 t−3 2 (1 − βt 2) 1 2 − α 2 t−1 2 (1 − βt 2)−1 2 βt 2 loge β2 = −α 2 t−3 2 (1 − βt 2)−1 2 (1 − βt 2 + tβt 2 loge β2 | {z } ψ(t) ). Since ψ′(t) =−βt 2 loge β2 + βt 2 loge β2 + tβt 2(loge β2)2 = tβt 2(loge β2)2, we have ψ′(t) > 0 when t >0 and ψ′(t) = 0when t = 0. Combining ψ(0) = 0, we get ψ(t) > 0 when t >0. Thus, we have ϕ′(t) < 0, which completes the proof. Lemma 2. For the parameter settings and assumptions in Theorem 1 or Theorem 2, we have ∥√vt∥2 < n(2G∞ + δ) (1 − β1)2 , t∈ [T], where vt = max(√bt, δ p 1 − βt 2). Proof. ∥mt∥∞ =∥ tX i=1 (1 − β1,t−i+1)gt−i+1 i−1Y j=1 β1,t−j+1∥∞ ≤ ∥ tX i=1 gt−i+1βi−1 1 ∥∞ ≤ G∞ 1 − β1 , ∥st∥∞ ≤    ∥m1∥∞ 1−β1,t ≤ G∞ (1−β1)2 < 2G∞ (1−β1)2 t = 1, ∥mt∥∞ 1−βt 1,t + ∥mt−1∥∞ 1−βt−1 1,t ≤ 2G∞ (1−β1)2 t >1, ∥bt∥∞ =∥(1 − β2) tX i=1 s2 t−i+1βi−1 2 ∥∞ ≤ 4G2 ∞ (1 − β1)4 , ∥√vt∥2 = nX i=1 vt,i < n(∥ √ bt∥∞ + δ) ≤ n(2G∞ + δ) (1 − β1)2 . By assumptions 2, 4, Lemma 1 and Lemma 2, ∀t ∈ [T], we have ∥mt∥∞ ≤ G∞ 1 − β1 , ∥vt∥∞ ≤ 2G∞ (1 − β1)2 , ˆαt vt,i > ˆαt+1 vt+1,i , ∀i ∈ [n]. (6) Following Yang et al. [33], Chen et al. [9], Zhou et al. [37], we define an auxiliary sequence{ut}: ∀t ≥ 2, ut = wt + β1,t 1 − β1,t (wt − wt−1) = 1 1 − β1,t wt − β1,t 1 − β1,t wt−1. (7) 17Hence, we have ut+1 − ut = \u0012 1 1 − β1,t+1 − 1 1 − β1,t \u0013 wt+1 − \u0012 β1,t+1 1 − β1,t+1 − β1,t 1 − β1,t \u0013 wt + 1 1 − β1,t (wt+1 − wt) − β1,t 1 − β1,t (wt − wt−1) = \u0012 1 1 − β1,t+1 − 1 1 − β1,t \u0013 (wt − ˆαt mt vt ) − \u0012 β1,t+1 1 − β1,t+1 − β1,t 1 − β1,t \u0013 wt − ˆαt 1 − β1,t mt vt + β1,t ˆαt−1 1 − β1,t mt−1 vt−1 = \u0012 1 1 − β1,t − 1 1 − β1,t+1 \u0013 ˆαt mt vt − ˆαt 1 − β1,t \u0012 β1,t mt−1 vt + (1− β1,t) gt vt \u0013 + β1,t ˆαt−1 1 − β1,t mt−1 vt−1 = \u0012 1 1 − β1,t − 1 1 − β1,t+1 \u0013 ˆαt mt vt + β1,t 1 − β1,t \u0012 ˆαt−1 vt−1 − ˆαt vt \u0013 mt−1 − ˆαt gt vt . (8) By assumption 1 and Equation (8), we have f(ut+1) ≤f(ut) +⟨∇f(ut), ut+1 − ut⟩ + L 2 ∥ut+1 − ut∥2 =f(ut) +⟨∇f(wt), ut+1 − ut⟩ + ⟨∇f(ut) − ∇f(wt), ut+1 − ut⟩ + L 2 ∥ut+1 − ut∥2 =f(ut) + \u001c ∇f(wt), \u0012 1 1 − β1,t − 1 1 − β1,t+1 \u0013 ˆαt mt vt \u001d + β1,t 1 − β1,t \u001c ∇f(wt), \u0012 ˆαt−1 vt−1 − ˆαt vt \u0013 mt−1 \u001d − ˆαt \u001c ∇f(wt), gt vt \u001d + ⟨∇f(ut) − ∇f(wt), ut+1 − ut⟩ + L 2 ∥ut+1 − ut∥2. (9) Rearranging Equation (9) and taking expectation both sides, by assumption 3 and Equation (6), we get (1 − β1)2 ˆαt 2G∞ E[∥∇f(wt)∥2] ≤ˆαtE \u0014\u001c ∇f(wt), ∇f(wt) vt \u001d\u0015 ≤E[f(ut) − f(ut+1)] +E \u0014\u001c ∇f(wt), \u0012 1 1 − β1,t − 1 1 − β1,t+1 \u0013 ˆαt mt vt \u001d\u0015 | {z } P1 + β1,t 1 − β1,t E \u0014\u001c ∇f(wt), \u0012 ˆαt−1 vt−1 − ˆαt vt \u0013 mt−1 \u001d\u0015 | {z } P2 + E[⟨∇f(ut) − ∇f(wt), ut+1 − ut⟩]| {z } P3 +L 2 E \u0002 ∥ut+1 − ut∥2\u0003 | {z } P4 . (10) To further bound Equation (10), we need the following lemma. Lemma 3. For the sequence {ut} defined as Equation (7), ∀t ≥ 2, we have ∥ut+1 − ut∥ ≤ √nG∞ δ \u0012 ˆαtβ1,t (1 − β1)3 + ˆαt−1β1,t (1 − β1)2 + ˆαt \u0013 , ∥ut+1 − ut∥2 ≤ 3nG2 ∞ δ2 \u0012 ˆα2 t β2 1,t (1 − β1)6 + ˆα2 t−1β2 1,t (1 − β1)4 + ˆα2 t \u0013 . Proof. Since ∀t ∈ [T], ∀i ∈ [n], 1/(1 − β1,t) ≥ 1/(1 − β1,t+1), vt ≥ δ√1 − β2, ˆαt−1/vt−1,i > ˆαt/vt,i. By Equation (8), we have ∥ut+1 − ut∥ ≤ˆαt √nG∞ (1 − β1)δ√1 − β2 \u0012 β1,t − β1,t+1 (1 − β1,t)(1 − β1,t+1) \u0013 + β1,t 1 − β1,t ˆαt−1 √nG∞ (1 − β1)δ√1 − β2 + ˆαt √nG∞ δ√1 − β2 ≤ √nG∞ δ√1 − β2 \u0012 ˆαtβ1,t (1 − β1)3 + ˆαt−1β1,t (1 − β1)2 + ˆαt \u0013 , ∥ut+1 − ut∥2 ≤ 3nG2 ∞ δ2(1 − β2) \u0012 ˆα2 t β2 1,t (1 − β1)6 + ˆα2 t−1β2 1,t (1 − β1)4 + ˆα2 t \u0013 , where the last inequality follows from Cauchy-Schwartz inequality. This completes the proof. 18Now we bound P1, P2, P3 and P4 of Equation (10), separately. By assumptions 1, 2, Equation (6) and Lemma 3, we have P1 ≤ ˆαt \u0012 1 1 − β1,t − 1 1 − β1,t+1 \u0013 E \u0014 ∥∇f(wt)∥∥mt vt ∥ \u0015 ≤ ˆαt \u0012 β1,t − β1,t+1 (1 − β1,t)(1 − β1,t+1) \u0013 nG2 ∞ δ√1 − β2(1 − β1) ≤ nG2 ∞ (1 − β1)3δ√1 − β2 ˆαt(β1,t − β1,t+1), P2 = E \" nX i=1 ∇if(wt)mt−1,i( ˆαt−1 vt−1,i − ˆαt vt,i ) # ≤ G2 ∞ 1 − β1 nX i=1 ( ˆαt−1 vt−1,i − ˆαt vt,i ), P3 ≤ E[∥∇f(ut) − ∇f(wt)∥∥ut+1 − ut∥] ≤ LE[∥ut − wt∥∥ut+1 − ut∥] = Lˆαt−1 β1,t 1 − β1,t E \u0014 ∥mt−1 vt−1 ∥∥ut+1 − ut∥ \u0015 ≤ LnG2 ∞ (1 − β1)2δ2(1 − β2) \u0012 ˆαt−1 ˆαtβ2 1,t (1 − β1)3 + ˆα2 t−1β2 1,t (1 − β1)2 + ˆαt−1 ˆαtβ1,t \u0013 < LnG2 ∞ (1 − β1)2δ2(1 − β2) \u0012 ˆα2 t−1 (1 − β1)3 + ˆα2 t−1 (1 − β1)2 + ˆα2 t−1 \u0013 < 3LnG2 ∞ (1 − β1)5δ2(1 − β2) ˆα2 t−1, P4 ≤ 3nG2 ∞ δ2(1 − β2) \u0012 ˆα2 t β2 1,t (1 − β1)6 + ˆα2 t−1β2 1,t (1 − β1)4 + ˆα2 t \u0013 < 3nG2 ∞ δ2(1 − β2) \u0012 ˆα2 t (1 − β1)6 + ˆα2 t−1 (1 − β1)4 + ˆα2 t \u0013 < 9nG2 ∞ (1 − β1)6δ2(1 − β2) ˆα2 t−1. (11) Replacing P1, P2, P3 and P4 of Equation (10) with Equation (11) and telescoping Equation (10) for t = 2to T, we have TX t=2 (1 − β1)2 ˆαt 2G∞ E \u0002 ∥∇f(wt)∥2\u0003 < E[f(u2) − f(uT+1)] + nG2 ∞ (1 − β1)3δ√1 − β2 TX t=2 ˆαt(β1,t − β1,t+1) + β1G2 ∞ (1 − β1)2 nX i=1 \u0012 ˆα1 v1,i − ˆαT vT,i \u0013 + 3LnG2 ∞ (1 − β1)5δ2(1 − β2) TX t=2 ˆα2 t−1 + 9LnG2 ∞ 2(1 − β1)6δ2(1 − β2) TX t=2 ˆα2 t−1 <E[f(u2)] − f(w∗) + nG2 ∞ (1 − β1)3δ√1 − β2 TX t=1 ˆαt(β1,t − β1,t+1) + αβ1nG2 ∞ (1 − β1)3δ + 15LnG2 ∞ 2(1 − β1)6δ2(1 − β2) TX t=1 ˆα2 t . (12) Since TX t=2 ˆαt = TX t=2 α√ t p 1 − βt 2 1 − βt 1,t ≥ α p 1 − β2 TX t=2 1√ t =α p 1 − β2 \u0012Z 3 2 1√ 2 ds + ··· + Z T T−1 1√ T ds \u0013 > α p 1 − β2 Z T 2 1√sds =2α p 1 − β2 \u0010√ T − √ 2 \u0011 , TX t=1 ˆα2 t = TX t=1 α2 t 1 − βt 2 (1 − β1,t)2 ≤ α2 (1 − β1)2 TX t=1 1 t = α2 (1 − β1)2 \u0012 1 + Z 3 2 1 2ds + ··· + Z T T−1 1 T ds \u0013 < α2 (1 − β1)2 \u0012 1 + Z T 2 1 s − 1ds \u0013 = α2 (1 − β1)2 (log(T − 1) + 1)< α2 (1 − β1)2 (log T + 1), E[f(u2)] ≤f(w1) +E[⟨∇f(w1), u2 − w1⟩] +L 2 E \u0002 ∥u2 − w1∥2\u0003 =f(w1) − ˆα1 1 − β1,2 E \u0014\u001c ∇f(w1), m1 v1 \u001d\u0015 + Lˆα2 1 2(1 − β1,2)2 E \u0014 ∥m1 v1 ∥2 \u0015 ≤f(w1) +α√1 − β2 (1 − β1)2 E \u0014 ∥∇f(w1)∥∥m1 v1 ∥ \u0015 + Lα2(1 − β2) 2(1 − β1)4 E \u0014 ∥m1 v1 ∥2 \u0015 ≤f(w1) + αnG2 ∞ (1 − β1)2δ + Lα2nG2 ∞ 2(1 − β1)4δ2 ≤ f(w1) + nG2 ∞α 2(1 − β1)4δ2 (2δ + Lα), (13) 19substituting Equation (13) into Equation (12), we finish the proof. C Proof of Corollary 1 Proof. Since β1,t = β1/ √ t, we have TX t=1 ˆαt(β1,t − β1,t+1) ≤ TX t=1 ˆαtβ1,t = TX t=1 α√ t p 1 − βt 2 1 − βt 1,t β1,t < α 1 − β1 TX t=1 1 t < α 1 − β1 (log T + 1). (14) Substituting Equation (14) into Equation (5), we have min t∈[T] E \u0002 ∥∇f(wt)∥2\u0003 < G∞ α(1 − β1)2(1 − β2)2 \u0012 f(w1) − f(w∗) + nG2 ∞α (1 − β1)8δ2 (2δ + 8Lα) + αβ1nG2 ∞ (1 − β1)3δ \u0013 1√ T − √ 2 + nG3 ∞ (1 − β2)2(1 − β1)10δ2 \u001215 2 Lα + δ \u0013 log T√ T − √ 2 . This completes the proof. D Proof of Theorem 2 Proof. Denote ˆαt = αt √ 1−βt 2 1−βt 1,t and vt = max(√bt, δ p 1 − βt 2), then ∥√vt(wt+1 − w∗)∥2 = ∥√vt(wt − ˆαt mt vt − w∗)∥2 = ∥√vt(wt − w∗)∥2 − 2ˆαt ⟨wt − w∗, mt⟩ + ˆα2 t ∥ mt √vt ∥2 = ∥√vt(wt − w∗)∥2 + ˆα2 t ∥ mt √vt ∥2 − 2ˆαtβ1,t ⟨wt − w∗, mt−1⟩ −2ˆαt(1 − β1,t) ⟨wt − w∗, gt⟩. (15) Rearranging Equation (15), we have ⟨wt − w∗, gt⟩ = 1 1 − β1,t \u0014 1 2ˆαt (∥√vt(wt − w∗)∥2 − ∥√vt(wt+1 − w∗)∥2) − β1,t ⟨wt − w∗, mt−1⟩ + ˆαt 2 ∥ mt √vt ∥2 \u0015 ≤ 1 1 − β1 \u0014 1 2ˆαt (∥√vt(wt − w∗)∥2 − ∥√vt(wt+1 − w∗)∥2) +β1,t 2ˆαt ∥wt − w∗∥2 + β1,t ˆαt 2 ∥mt−1∥2 + ˆαt 2 ∥ mt √vt ∥2 \u0015 , where the first inequality follows from Cauchy-Schwartz inequality and ab ≤ 1 2 (a2 + b2). Hence, the regret TX t=1 (ft(wt) − ft(w∗)) ≤ TX t=1 ⟨wt − w∗, gt⟩ ≤ 1 1 − β1 TX t=1 \u0014 1 2ˆαt (∥√vt(wt − w∗)∥2 − ∥√vt(wt+1 − w∗)∥2) +β1,t 2ˆαt ∥wt − w∗∥2 + β1,t ˆαt 2 ∥mt−1∥2 + ˆαt 2 ∥ mt √vt ∥2 \u0015 , (16) where the first inequality follows from the convexity of ft(w). For further bounding Equation (16), we need the following lemma. Lemma 4. For the parameter settings and conditions assumed in Theorem 2, we have TX t=1 ˆαt∥mt∥2 < 2nαG2 ∞ (1 − β1)3 √ T . 20Proof. From Equation (4), we have ˆαt∥mt∥2 = ˆαt∥ tX i=1 (1 − β1,t−i+1)gt−i+1 i−1Y j=1 β1,t−j+1∥2 ≤ ˆαt∥ tX i=1 gt−i+1βi−1 1 ∥2 = ˆαt nX j=1  tX i=1 gt−i+1,jβi−1 1 !2 ≤ ˆαt nX j=1  tX i=1 g2 t−i+1,jβi−1 1 ! tX i=1 βi−1 1 ! < α√ t 1 1 − β1,t nG2 ∞ (1 − β1)2 ≤ nαG2 ∞ (1 − β1)3 1√ t, where the second inequality follows from Cauchy-Schwartz inequality. Therefore, TX t=1 ˆαt∥mt∥2 < nαG2 ∞ (1 − β1)3 TX t=1 1√ t < 2nαG2 ∞ √ T (1 − β1)3 , where the last inequality follows from TX t=1 1√ t = 1 + Z 3 2 1√ 2 ds + ··· + Z T T−1 1√ T ds < 1 + Z 3 2 1√s − 1ds + ··· + Z T T−1 1√s − 1ds = 1 + Z T 2 1√s − 1ds = 2 √ T − 1 − 1 < 2 √ T . This completes the proof. Now we return to the proof of Theorem 2. Let ˆα0 := ˆα1. By Lemma 1, Lemma 2, Lemma 4, Equation (16) and the third inequality of Equation (6), we have TX t=1 (ft(wt) − ft(w∗)) ≤ 1 1 − β1 \u0014 1 2ˆα1 ∥√v1(w1 − w∗)∥2 + TX t=2 \u0012 1 2ˆαt ∥√vt(wt − w∗)∥2 − 1 2ˆαt−1 ∥√vt−1(wt − w∗)∥2 \u0013 + TX t=1 β1,t 2ˆαt ∥wt − w∗∥2 + TX t=1 \u0012 ˆαt−1 2 ∥mt−1∥2 + ˆαt 2 ∥ mt √vt ∥2 \u0013\u0015 ≤ 1 1 − β1 \u0014D2 ∞ 2ˆα1 ∥√v1∥2 + TX t=2 D2 ∞   ∥√vt∥2 2ˆαt − ∥√vt−1∥2 2ˆαt−1 ! + TX t=1 β1,t 2ˆαt nD2 ∞ + TX t=1 ˆαt \u00121 2 + 1 2δ√1 − β2 \u0013 ∥mt∥2 \u0015 = 1 1 − β1 \u0014 D2 ∞ ∥√vT ∥2 2αT + TX t=1 β1,t 2ˆαt nD2 ∞ + TX t=1 ˆαt \u00121 2 + 1 2δ√1 − β2 \u0013 ∥mt∥2 \u0015 < 1 1 − β1 \u0014 n(2G∞ + δ)D2 ∞ 2α√1 − β2(1 − β1)2 √ T + TX t=1 β1,t 2ˆαt nD2 ∞ + nαG2 ∞ (1 − β1)3 \u0012 1 + 1 δ√1 − β2 \u0013√ T \u0015 . This completes the proof. E Proof of Corollary 3 Proof. Since β1,t = β1/t, we have TX t=1 β1,t 2ˆαt = TX t=1 (1 − βt 1,t) √ tβ1,t 2α p 1 − βt 2 < TX t=1 √ tβ1,t 2α√1 − β2 = β1 2α√1 − β2 TX t=1 1√ t < β1 α√1 − β2 √ T . This completes the proof. 21",
      "image_data": null,
      "references": null,
      "meta_data": {
        "arxiv_id": "2312.01658v2",
        "doi": null,
        "is_generated": null,
        "authors": [
          "Yun Yue",
          "Zhiling Ye",
          "Jiadi Jiang",
          "Yongchao Liu",
          "Ke Zhang"
        ],
        "author_affiliations": null,
        "language": null,
        "published_date": "2023-12-04T06:20:14Z",
        "venue": null,
        "volume": null,
        "issue": null,
        "pages": null,
        "pdf_url": "https://arxiv.org/pdf/2312.01658v2.pdf",
        "github_url": null,
        "peer_review_status": null,
        "access_type": null,
        "reference_count": null,
        "citation_count": null,
        "h_index_relevance": null
      },
      "llm_extracted_info": null
    }
  ]
}
